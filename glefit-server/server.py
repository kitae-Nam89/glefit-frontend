# -*- coding: utf-8 -*-
import os, re, json, time, sqlite3, unicodedata
from dotenv import load_dotenv
from flask import Flask, request, jsonify
from flask_cors import CORS

# [ADD] Auth imports
from datetime import datetime, timedelta
from functools import wraps
import jwt
from passlib.hash import pbkdf2_sha256 as bcrypt


from openai import OpenAI
import yaml
from collections import defaultdict
# --- [LOCAL SPELLCHECK] begin ---
import os, re
from symspellpy import SymSpell, Verbosity
import uuid

_HANGUL_RE = re.compile(r"[ê°€-í£]+")
DATA_DIR = "data"
KO_WORDS_PATH = os.path.join(DATA_DIR, "ko_words.txt")   # "ë‹¨ì–´<TAB>ë¹ˆë„"
WHITE_PATH    = os.path.join(DATA_DIR, "whitelist.txt")  # ì¤„ë‹¹ 1ê°œ ë‹¨ì–´

_sym = None
_whitelist = set()

def _init_symspell():
    import os
    os.makedirs(DATA_DIR, exist_ok=True)
    global _sym, _whitelist
    _sym = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)

    if os.path.exists(KO_WORDS_PATH):
        _sym.load_dictionary(KO_WORDS_PATH, term_index=0, count_index=1,
                             separator="\t", encoding="utf-8")

    custom_path = os.path.join(DATA_DIR, "custom_words.txt")
    if os.path.exists(custom_path):
        _sym.load_dictionary(custom_path, term_index=0, count_index=1,
                             separator="\t", encoding="utf-8")

    _whitelist = set()
    if os.path.exists(WHITE_PATH):
        for enc in ("utf-8","utf-8-sig","cp949","euc-kr"):
            try:
                with open(WHITE_PATH,"r",encoding=enc) as f:
                    _whitelist = {ln.strip() for ln in f if ln.strip()}
                break
            except UnicodeDecodeError:
                continue

    print(f"ğŸ”¤ SymSpell ready: words={len(_sym.words)}, whitelist={len(_whitelist)}")

def _token_spans_ko(text: str):
    """í•œê¸€ ì—°ì†êµ¬ê°„ì„ í† í°ìœ¼ë¡œ ë½‘ì•„ (start, end, token)"""
    for m in _HANGUL_RE.finditer(text or ""):
        yield m.start(), m.end(), m.group()
# --- [LOCAL SPELLCHECK] end ---


# ================== ê¸°ë³¸ ì„¤ì • ==================
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    raise RuntimeError("OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤ (.env í™•ì¸)")

client = OpenAI(api_key=OPENAI_API_KEY, timeout=30)

MODEL = "gpt-4o"
import os, shutil

# ë””í´íŠ¸ëŠ” ë¡œì»¬ íŒŒì¼, í™˜ê²½ë³€ìˆ˜(DB_PATH)ë¡œ ë®ì–´ì“°ê¸°
DB_PATH = os.getenv("DB_PATH", "rewrite.db")
print(f"[DB] Using DB_PATH={DB_PATH}")  # â† ë¶€íŒ… ë¡œê·¸ í™•ì¸ìš©

# (ì„ íƒ) ë§ˆì´ê·¸ë ˆì´ì…˜ ë„ìš°ë¯¸: /app/rewrite.db â†’ /data/rewrite.db ë¡œ ìµœì´ˆ 1íšŒ ë³µì‚¬
try:
    app_db = os.path.join(os.path.dirname(__file__), "rewrite.db")
    if os.getenv("DB_PATH") and os.getenv("DB_PATH") != "rewrite.db":
        os.makedirs(os.path.dirname(os.getenv("DB_PATH")), exist_ok=True)
        if os.path.exists(app_db) and not os.path.exists(os.getenv("DB_PATH")):
            shutil.copy2(app_db, os.getenv("DB_PATH"))
except Exception:
    pass
MAX_CHARS_PER_CHUNK = 4000

# [ADD] JWT config
JWT_SECRET = os.getenv("JWT_SECRET", "CHANGE_ME_TO_RANDOM_SECRET")
JWT_ALG = "HS256"
# [ADD] ensure users table
def ensure_users_table():
    conn = sqlite3.connect(DB_PATH)
    cur = conn.cursor()
    cur.execute("""
    CREATE TABLE IF NOT EXISTS users (
        id INTEGER PRIMARY KEY AUTOINCREMENT,
        username TEXT UNIQUE NOT NULL,
        password_hash TEXT NOT NULL,
        is_active INTEGER DEFAULT 0,
        paid_until TEXT,
        role TEXT DEFAULT 'user',
        notes TEXT
    )
    """)
    conn.commit()
    conn.close()

ensure_users_table()

def migrate_users_table():
    conn = sqlite3.connect(DB_PATH)
    cur = conn.cursor()

    # ë¡œê·¸ì¸/ì„¸ì…˜ ê´€ë¦¬ ë° í¸ì˜ í•„ë“œ ì¶”ê°€ (ì´ë¯¸ ìˆìœ¼ë©´ ë„˜ì–´ê°)
    try: cur.execute("ALTER TABLE users ADD COLUMN token_version INTEGER DEFAULT 0")
    except Exception: pass
    try: cur.execute("ALTER TABLE users ADD COLUMN last_jti TEXT")
    except Exception: pass
    try: cur.execute("ALTER TABLE users ADD COLUMN allow_concurrent INTEGER DEFAULT 0")
    except Exception: pass
    try: cur.execute("ALTER TABLE users ADD COLUMN site_url TEXT")
    except Exception: pass
    try: cur.execute("ALTER TABLE users ADD COLUMN created_at TEXT")
    except Exception: pass

    # â˜… ì‹ ê·œ: ê²Œì‹œê¸€ ì‘ì„± ì •ì§€ í”Œë˜ê·¸
    try: cur.execute("ALTER TABLE users ADD COLUMN posting_blocked INTEGER DEFAULT 0")
    except Exception: pass
    # (ì„ íƒ) ì¸ë±ìŠ¤
    try: cur.execute("CREATE INDEX IF NOT EXISTS idx_users_posting_blocked ON users(posting_blocked)")
    except Exception: pass

    conn.commit()
    conn.close()


# === (BOOT SEED) ì„œë²„ ê¸°ë™ ì‹œ ê´€ë¦¬ì ê³„ì • ë³´ì¥ ===
def _boot_seed_admin():
    try:
        import sqlite3, os
        from datetime import datetime, timedelta
        # passlib í•´ì‹œë¥¼ ì „ì—­ì—ì„œ: from passlib.hash import pbkdf2_sha256 as bcrypt

        admin_user = normalize_username(os.getenv("ADMIN_USER") or "")
        admin_pass = (os.getenv("ADMIN_PASS") or "").strip()
        admin_days = int(os.getenv("ADMIN_DAYS") or 0)
        if not admin_user or not admin_pass or admin_days <= 0:
            return

        conn = sqlite3.connect(DB_PATH)
        cur = conn.cursor()

        # ìµœì†Œ ìŠ¤í‚¤ë§ˆ ë³´ì¥ (í…Œì´ë¸”/ì¸ë±ìŠ¤ë§Œ)
        cur.executescript("""
        CREATE TABLE IF NOT EXISTS users (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            username TEXT UNIQUE NOT NULL,
            password_hash TEXT NOT NULL,
            is_active INTEGER DEFAULT 1,
            paid_until TEXT,
            role TEXT DEFAULT 'user',
            notes TEXT
        );
        CREATE UNIQUE INDEX IF NOT EXISTS idx_users_username ON users(username);
        """)

        paid_until = (datetime.utcnow() + timedelta(days=admin_days)).isoformat()
        cur.execute("""
        INSERT INTO users (username, password_hash, is_active, paid_until, role, notes)
        VALUES (?, ?, 1, ?, 'admin', 'seed admin')
        ON CONFLICT(username) DO UPDATE SET
          password_hash = excluded.password_hash,
          is_active     = 1,
          paid_until    = excluded.paid_until,
          role          = 'admin',
          notes         = 'seed admin'
        """, (admin_user, bcrypt.hash(admin_pass), paid_until))

        conn.commit()
        conn.close()
        print(f"[boot-seed] admin ready: {admin_user} (until {paid_until})")

    except Exception as e:
        print("[boot-seed] skip/error:", e)

# [ADD] ìš´ì˜/í†µê³„/ìºì‹œìš© í…Œì´ë¸”
def migrate_ops_tables():
    conn = sqlite3.connect(DB_PATH)
    cur = conn.cursor()
    cur.execute("""
    CREATE TABLE IF NOT EXISTS usage_logs (
        id INTEGER PRIMARY KEY AUTOINCREMENT,
        username TEXT,
        action TEXT,           -- verify|policy|dedup_intra|dedup_inter|spell_local|login
        files_count INTEGER,
        created_at TEXT
    )""")
    cur.execute("""
    CREATE TABLE IF NOT EXISTS error_logs (
        id INTEGER PRIMARY KEY AUTOINCREMENT,
        username TEXT,
        path TEXT,
        status INTEGER,
        message TEXT,
        created_at TEXT
    )""")
    cur.execute("""
    CREATE TABLE IF NOT EXISTS agreements (
        username TEXT PRIMARY KEY,
        agreed_at TEXT
    )""")

    cur.execute("""
    CREATE TABLE IF NOT EXISTS visit_logs (
        id INTEGER PRIMARY KEY AUTOINCREMENT,
        username TEXT,            -- ë¡œê·¸ì¸ ì „/í›„ ëª¨ë‘ ê¸°ë¡(ì—†ìœ¼ë©´ ë¹ˆë¬¸ì)
        path TEXT,                -- í”„ë¡ íŠ¸ê°€ ë³´ë‚´ëŠ” í˜„ì¬ ê²½ë¡œ/ìŠ¤í¬ë¦° ëª…
        ip TEXT,
        user_agent TEXT,
        created_at TEXT
    )""")

    # --- [ADD] board posts & limits ---
    cur.execute("""
    CREATE TABLE IF NOT EXISTS board_posts (
        id TEXT PRIMARY KEY,
        username TEXT,
        text TEXT,
        pinned INTEGER DEFAULT 0,
        ts INTEGER
    )
    """)
    cur.execute("""
    CREATE TABLE IF NOT EXISTS board_limits (
        username TEXT PRIMARY KEY,
        daily_limit INTEGER DEFAULT 2
    )
    """)


    conn.commit()
    conn.close()

# === INSERT near line ~217 (after migrate_ops_tables, before "ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤í–‰") ===
def migrate_board_tables():
    conn = sqlite3.connect(DB_PATH); cur = conn.cursor()
    cur.execute("""
    CREATE TABLE IF NOT EXISTS board_posts (
        id TEXT PRIMARY KEY,
        username TEXT,
        text TEXT,
        pinned INTEGER DEFAULT 0,
        ts INTEGER
    )""")
    cur.execute("""
    CREATE TABLE IF NOT EXISTS board_limits (
        username TEXT PRIMARY KEY,
        daily_limit INTEGER DEFAULT 2
    )""")

    # --- NEW: hidden ì»¬ëŸ¼(ì†Œí”„íŠ¸ ì‚­ì œ/ìˆ¨ê¹€) ë³´ê°• ---
    cur.execute("PRAGMA table_info(board_posts)")
    cols = [r[1] for r in cur.fetchall()]
    if "hidden" not in cols:
        cur.execute("ALTER TABLE board_posts ADD COLUMN hidden INTEGER DEFAULT 0")

    conn.commit(); conn.close()



def log_visit(username, path):
    try:
        ip = request.headers.get("CF-Connecting-IP") or request.remote_addr or ""
        ua = request.headers.get("User-Agent") or ""
        conn = sqlite3.connect(DB_PATH); cur = conn.cursor()
        # ë°©ë¬¸ ì‹œê°„ë„ í•œêµ­ ì‹œê°„(KST, UTC+9) ê¸°ì¤€
        now_kst = datetime.utcnow() + timedelta(hours=9)
        cur.execute(
            "INSERT INTO visit_logs (username, path, ip, user_agent, created_at) VALUES (?,?,?,?,?)",
            (username or "", path or "", ip[:120], ua[:300], now_kst.isoformat())
        )
        conn.commit(); conn.close()
    except Exception:
        pass

# ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤í–‰
migrate_users_table()
migrate_ops_tables()
migrate_board_tables()
_boot_seed_admin()

# [ADD] user helpers & guards
def _get_user(username: str):
    conn = sqlite3.connect(DB_PATH)
    cur = conn.cursor()
    cur.execute("SELECT id, username, password_hash, is_active, paid_until, role FROM users WHERE username = ?", (username,))
    row = cur.fetchone()
    conn.close()
    if not row:
        return None
    return {
        "id": row[0], "username": row[1], "password_hash": row[2],
        "is_active": bool(row[3]),
        "paid_until": row[4],
        "role": row[5] or "user",
    }

# === [ADD] Non-admin text size limit ===============================
MAX_TEXT_BYTES_NON_ADMIN = 100 * 1024  # 100KB

def _utf8_len(s: str) -> int:
    if not s:
        return 0
    return len(s.encode("utf-8", "ignore"))

def _is_admin_current() -> bool:
    try:
        uname = _username_from_req()
        u = _get_user(uname) if uname else None
        return (u or {}).get("role") == "admin"
    except Exception:
        return False

def enforce_size_limit_or_400(texts_or_iterable):
    """
    - ê´€ë¦¬ìë©´ ë¬´ì œí•œ í†µê³¼
    - ë¹„ê´€ë¦¬ì(ì¼ë°˜/ì²´í—˜íŒ)ëŠ” 'í•­ëª©ë‹¹' 100KB ì´ˆê³¼ ì‹œ 400 ë°˜í™˜
    - texts_or_iterable: str | list[str] | list[{'text':str}, ...]
    """
    if _is_admin_current():
        return  # admin unlimited

    # normalize
    if isinstance(texts_or_iterable, str):
        payloads = [texts_or_iterable]
    else:
        payloads = []
        for item in (texts_or_iterable or []):
            if isinstance(item, str):
                payloads.append(item)
            elif isinstance(item, dict) and "text" in item:
                payloads.append(item.get("text") or "")
            else:
                payloads.append(str(item or ""))

    for idx, s in enumerate(payloads):
        if _utf8_len(s) > MAX_TEXT_BYTES_NON_ADMIN:
            return jsonify({
                "ok": False,
                "error": "TEXT_TOO_LARGE",
                "message": f"ì¼ë°˜/ì²´í—˜íŒì€ í•­ëª©ë‹¹ 100KB(102400 bytes)ê¹Œì§€ë§Œ í—ˆìš©ë©ë‹ˆë‹¤. (#{idx+1})",
                "limit_bytes": MAX_TEXT_BYTES_NON_ADMIN
            }), 400
    return
# ===================================================================

def _remaining_days(paid_until: str) -> int:
    if not paid_until:
        return 0
    try:
        # í•œêµ­ ì‹œê°„(KST, UTC+9) ê¸°ì¤€ ë‚¨ì€ ì¼ìˆ˜ ê³„ì‚°
        now_kst = datetime.utcnow() + timedelta(hours=9)
        delta = datetime.fromisoformat(paid_until) - now_kst
        return max(0, delta.days)
    except Exception:
        return 0

# [ADD] ì‚¬ìš©ëŸ‰/ì—ëŸ¬ ë¡œê¹… í—¬í¼
def log_usage(username, action, files_count=0):
    try:
        conn = sqlite3.connect(DB_PATH); cur = conn.cursor()
        # created_at ì„ í•œêµ­ ì‹œê°„(KST, UTC+9) ê¸°ì¤€ ë¡œì»¬ ì‹œê°ìœ¼ë¡œ ì €ì¥
        now_kst = datetime.utcnow() + timedelta(hours=9)
        cur.execute("INSERT INTO usage_logs (username, action, files_count, created_at) VALUES (?,?,?,?)",
                    (username or "", action or "", int(files_count or 0), now_kst.isoformat()))
        conn.commit(); conn.close()
    except Exception:
        pass

def log_error(username, path, status, message):
    try:
        conn = sqlite3.connect(DB_PATH); cur = conn.cursor()
        # created_at ë„ í•œêµ­ ì‹œê°„(KST, UTC+9) ê¸°ì¤€
        now_kst = datetime.utcnow() + timedelta(hours=9)
        cur.execute("INSERT INTO error_logs (username, path, status, message, created_at) VALUES (?,?,?,?,?)",
                    (username or "", path or "", int(status or 0), str(message)[:500], now_kst.isoformat()))
        conn.commit(); conn.close()
    except Exception:
        pass

def _username_from_req():
    try:
        auth = request.headers.get("Authorization","")
        tok = auth.split(" ",1)[1]
        data = jwt.decode(tok, JWT_SECRET, algorithms=[JWT_ALG])
        return data.get("sub") or ""
    except Exception:
        return ""

def _is_paid_and_active(u: dict) -> bool:
    if not u or not u.get("is_active"):
        return False
    try:
        pu = u.get("paid_until")
        if not pu:
            return False
        # í•œêµ­ ì‹œê°„(KST, UTC+9) ê¸°ì¤€ìœ¼ë¡œ ë§Œë£Œ ì—¬ë¶€ íŒë‹¨
        now_kst = datetime.utcnow() + timedelta(hours=9)
        return now_kst <= datetime.fromisoformat(pu)
    except Exception:
        return False

def _origin_allowed(user_site_url: str) -> bool:
    if not user_site_url:
        return True  # ì£¼ì†Œ ì œí•œ ì•ˆ ê±´ ê³„ì •
    origin = (request.headers.get("Origin") or "").lower()
    referer = (request.headers.get("Referer") or "").lower()
    target = user_site_url.lower()
    return (target in origin) or (target in referer)

def require_user(fn):
    @wraps(fn)
    def wrapper(*args, **kwargs):
        if request.method == "OPTIONS":
            return "", 200

        auth = request.headers.get("Authorization","").strip()
        if not auth.startswith("Bearer "):
            return jsonify({"error":"Unauthorized"}), 401
        token = auth.split(" ",1)[1]

        try:
            data = jwt.decode(token, JWT_SECRET, algorithms=[JWT_ALG])
        except Exception:
            return jsonify({"error":"Invalid token"}), 401

        username = data.get("sub","")
        tok_ver  = int(data.get("ver", 0) or 0)
        tok_jti  = data.get("jti","") or ""

                # DBì—ì„œ ìµœì‹  ìƒíƒœ(í™œì„±/ê¸°ê°„/ì‚¬ì´íŠ¸/ë²„ì „/JTI/ë™ì‹œì ‘ì†) í™•ì¸
        conn = sqlite3.connect(DB_PATH)
        cur  = conn.cursor()
        cur.execute(
            "SELECT is_active, paid_until, site_url, token_version, last_jti, allow_concurrent "
            "FROM users WHERE username=?",
            (username,)
        )
        row = cur.fetchone()
        conn.close()

        if not row:
            return jsonify({"error":"Unauthorized"}), 401

        is_active, paid_until, site_url, db_ver, db_jti, allow_concurrent = (
            row[0], row[1], (row[2] or ""), int(row[3] or 0), (row[4] or ""), bool(row[5])
        )

        # ê²°ì œ/ê¸°ê°„/í™œì„± ì²´í¬ (í•œêµ­ ì‹œê°„ KST ê¸°ì¤€)
        try:
            now_kst = datetime.utcnow() + timedelta(hours=9)
            if not is_active or not paid_until or now_kst > datetime.fromisoformat(paid_until):
                return jsonify({"error":"Payment required or expired"}), 402
        except Exception:
            return jsonify({"error":"Payment required or expired"}), 402

        # ë‹¨ì¼ ë¡œê·¸ì¸ ê°•ì œ: ë™ì‹œì ‘ì† í—ˆìš©ì´ ì•„ë‹ ë•Œë§Œ ë²„ì „/JTI ê²€ì¦
        if not allow_concurrent:
            if tok_ver != db_ver or (db_jti and tok_jti != db_jti):
                return jsonify({
                    "error": "Session invalidated. Please log in again.",
                    "code": "SESSION"
                }), 401

        # 2025-11-26: ë„ë©”ì¸ ì œí•œ(site_url ê¸°ë°˜)ì€ ì¼ë‹¨ ë¹„í™œì„±í™”
        # - ë„ë©”ì¸ ì œì–´ëŠ” CORS(ALLOWED_ORIGINS)ì—ì„œë§Œ ì²˜ë¦¬
        # - glefit.kr / www.glefit.kr / vercel / localhost ëª¨ë‘ ê°™ì€ API ì‚¬ìš©

        return fn(*args, **kwargs)
    return wrapper

def require_admin(fn):
    @wraps(fn)
    def wrapper(*args, **kwargs):
        auth = request.headers.get("Authorization","").strip()
        if not auth.startswith("Bearer "):
            return jsonify({"error":"Unauthorized"}), 401
        token = auth.split(" ",1)[1]

        try:
            data = jwt.decode(token, JWT_SECRET, algorithms=[JWT_ALG])
        except Exception:
            return jsonify({"error":"Invalid token"}), 401

        username = data.get("sub","")
        tok_ver  = int(data.get("ver", 0) or 0)
        tok_jti  = data.get("jti","") or ""

        conn = sqlite3.connect(DB_PATH)
        cur  = conn.cursor()
        cur.execute(
            "SELECT role, is_active, paid_until, token_version, last_jti, allow_concurrent "
            "FROM users WHERE username=?",
            (username,)
        )
        row = cur.fetchone()
        conn.close()

        if not row:
            return jsonify({"error":"Unauthorized"}), 401

        role, is_active, paid_until, db_ver, db_jti, allow_concurrent = (
            (row[0] or "user"), row[1], row[2], int(row[3] or 0), (row[4] or ""), bool(row[5])
        )

        # ê²°ì œ/ê¸°ê°„/í™œì„± ì²´í¬ (í•œêµ­ ì‹œê°„ KST ê¸°ì¤€)
        try:
            now_kst = datetime.utcnow() + timedelta(hours=9)
            if not is_active or not paid_until or now_kst > datetime.fromisoformat(paid_until):
                return jsonify({"error":"Payment required or expired"}), 402
        except Exception:
            return jsonify({"error":"Payment required or expired"}), 402

        # ë‹¨ì¼ ë¡œê·¸ì¸ ê°•ì œ â€” í—ˆìš© ê³„ì •ì€ ì˜ˆì™¸
        if not allow_concurrent:
            if tok_ver != db_ver or (db_jti and tok_jti != db_jti):
                return jsonify({"error":"Session invalidated. Please log in again.", "code":"SESSION"}), 401

        if role != "admin":
            return jsonify({"error":"Admin only"}), 403

        return fn(*args, **kwargs)
    return wrapper


RULE_PATH = os.getenv("RULE_PATH", "kr-medhealth.yaml")
RULES = {}   # {"rules":[{...}], ...}
RULES_INDEX = {}  # {rule_id: rule_obj}

# === INSERT @ line 467 (right after require_admin ends) ===
def verify_board_auth(payload):
    """
    payload: {"username": "...", "password": "..."}
    - users í…Œì´ë¸”ì˜ ìê²© í™•ì¸
    - ë°˜í™˜: (ok, username, is_admin)
    """
    try:
        u = (payload.get("username") or "").strip()
        p = payload.get("password") or ""
        user = _get_user(u)
        if not user or not bcrypt.verify(p, user["password_hash"]):
            return False, "", False
        # ê²°ì œ/í™œì„± ìœ íš¨
        if not _is_paid_and_active(user):
            return False, "", False
        is_admin = (user.get("role") == "admin")
        return True, user["username"], is_admin
    except Exception:
        return False, "", False

# ================== ìœ í‹¸ ==================

# === username ê·œì¹™: ì†Œë¬¸ì ì˜ë¬¸/ìˆ«ì/._- 3~32ì ===
USERNAME_RE = re.compile(r"^[a-z0-9._-]{3,32}$")

def normalize_username(u: str) -> str:
    # ê³µë°± ì œê±° + ì†Œë¬¸ì í†µì¼
    return (u or "").strip().lower()

def validate_username(u: str):
    if not USERNAME_RE.match(u or ""):
        raise ValueError("username must be 3-32 chars: a-z, 0-9, dot, underscore, hyphen only")

def search_db(sentence):
    try:
        conn = sqlite3.connect(DB_PATH)
        cur = conn.cursor()
        cur.execute("SELECT alternative FROM sentences WHERE original = ?", (sentence,))
        rows = cur.fetchall()
        conn.close()
        return [r[0] for r in rows] if rows else []
    except Exception as e:
        print("DB ê²€ìƒ‰ ì˜¤ë¥˜:", e)
        return []

def find_all(text, substring, start_from=0):
    out = []
    i = start_from
    L = len(substring)
    if not substring:
        return out
    while True:
        i = text.find(substring, i)
        if i == -1:
            break
        out.append(i)
        i += L
    return out

def chunk_text_with_offsets(text, max_chars=MAX_CHARS_PER_CHUNK):
    chunks = []
    n = len(text)
    i = 0
    while i < n:
        end = min(i + max_chars, n)
        if end < n:
            cut = text.rfind("\n", i, end)
            if cut == -1 or cut <= i + int(max_chars * 0.3):
                cut = end
        else:
            cut = end
        chunks.append((i, text[i:cut]))
        i = cut
    return chunks

_json_array_pattern = re.compile(r"\[\s*{.*?}\s*\]", re.DOTALL)
def extract_json_array(s):
    if not s:
        return []
    s = re.sub(r"^```json\s*|\s*```$", "", s.strip(), flags=re.IGNORECASE | re.DOTALL)
    m = _json_array_pattern.search(s)
    if not m:
        try:
            j = json.loads(s)
            return j if isinstance(j, list) else []
        except:
            return []
    try:
        return json.loads(m.group(0))
    except Exception as e:
        print("JSON íŒŒì‹± ì‹¤íŒ¨:", e)
        return []

def basic_kr_sentence_split(text):
    parts = re.split(r"([\.?!]+|\n+)", text)
    out, buf = [], ""
    for p in parts:
        if p is None:
            continue
        buf += p
        if p.endswith((".", "?", "!", "\n")) or p.strip() == "":
            s = buf.strip()
            if s:
                out.append(s)
            buf = ""
    if buf.strip():
        out.append(buf.strip())
    return out

def add_context(text, start, length, window=8):
    before = text[max(0, start - window): start]
    after  = text[start + length: start + length + window]
    return before, after

def gpt_call(model, messages):
    return client.chat.completions.create(model=model, messages=messages)

# === [NEW] ë¬¸ì„œ ìŠ¤íƒ€ì¼/ìœ í˜• ë¶„ì„ (ì •ë³´í˜•/í›„ê¸°í˜•/í”„ë¡œëª¨ì…˜) ===
# 1) ì •ë³´í˜•/í›„ê¸°í˜•/í”„ë¡œëª¨ì…˜ í‚¤ì›Œë“œ íŒíŠ¸
_INFO_HINTS = [
    "ê¸°ë³¸ ì •ë³´", "íŠ¹ì§•", "ì¥ì ", "ë‹¨ì ", "êµ¬ì„±", "ê°€ê²©", "ìŠ¤í™",
    "ì´ìš© ë°©ë²•", "ì‹ ì²­ ë°©ë²•", "ìì£¼ ë¬»ëŠ” ì§ˆë¬¸", "FAQ",
]

_REVIEW_HINTS = [
    "ì§ì ‘ ì‚¬ìš©í•´ ë³´ë‹ˆ", "ì¨ë³´ë‹ˆ", "ê²½í—˜ë‹´", "í›„ê¸°", "ëŠë‚€ ì ",
    "ì†”ì§ í›„ê¸°", "ê°œì¸ì ì¸ ìƒê°", "ì‚¬ìš©ê¸°", "ì²´í—˜ê¸°",
]

_CTA_HINTS = [
    "ì§€ê¸ˆ ë°”ë¡œ", "ì§€ê¸ˆ ì‹ ì²­", "ì§€ê¸ˆ ë¬¸ì˜", "ìƒë‹´ì´ í•„ìš”í•˜ì‹œë©´",
    "ì•„ë˜ ë²ˆí˜¸ë¡œ", "ì—°ë½ ì£¼ì‹œë©´", "ë¬¸ì˜ ì£¼ì‹œë©´", "ë„ì™€ë“œë¦¬ê² ìŠµë‹ˆë‹¤",
    "ì§„í–‰í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤", "ì•ˆë‚´í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤",
]

_FIRST_PERSON = ["ì €ëŠ”", "ì œê°€", "ì œ ì…ì¥ì—ì„œëŠ”", "ì œ ìƒê°ì—ëŠ”"]
_OPINION_ADJ = ["ì¢‹ì•˜ë˜", "ë§Œì¡±ìŠ¤ëŸ¬ìš´", "ì•„ì‰¬ìš´", "ê´œì°®ì€", "ì¸ìƒì ì¸"]

# ì‹œê°„/í†µê³„ í‘œí˜„ (ì¡°ê¸ˆ ë” ì •ë³´í˜• ëŠë‚Œ)
_TIME_EXPR_RX = re.compile(r"(ë…„|ì›”|ì¼|ì£¼|ì‹œê°„|ë¶„)\s*ë™ì•ˆ|ìµœê·¼\s*\d+\s*(ë…„|ê°œì›”)")
_PHONE_RX = re.compile(r"\d{2,4}-\d{3,4}-\d{4}")
# ë¬¸ì„œ ìŠ¤íƒ€ì¼ ë¶„ì„ì—ì„œ ì‚¬ìš©í•˜ëŠ” ì „í™”ë²ˆí˜¸ íŒ¨í„´ alias
_DOC_PHONE_RX = _PHONE_RX

# ì¸ê¸°/ìœ ëª…/ê°€ì„±ë¹„ í‘œí˜„
_POPULAR_EXPR_RX = re.compile(
    r"(ì¸ê¸°(?:ê°€|ë¥¼)?\s*(ë§ì´|ë†’ê²Œ|ì¢‹ê²Œ)?|ìœ ëª…í•˜|ë§ì´\s*(ì°¾ëŠ”|ì„ íƒí•˜ëŠ”)|ê°€ì„±ë¹„\s*ê°€?\s*ì¢‹)"
)

# ë¶ˆíŠ¹ì • ë‹¤ìˆ˜/ì§‘ë‹¨ í‘œí˜„
_GENERIC_PEOPLE_RX = re.compile(
    r"(ë§ì€|ëŒ€ë¶€ë¶„|ì—¬ëŸ¬|ìˆ˜ë§ì€)\s*(ë¶„ë“¤|ì‚¬ëŒë“¤|ê³ ê°|ì´ìš©ì|í•™ìƒ|ìˆ˜í—˜ìƒ)"
)

# 'ê°€ëŠ¥í•©ë‹ˆë‹¤/ê°€ëŠ¥í•´ìš”' ì•ˆë‚´í˜• í‘œí˜„
_POSSIBLE_EXPR_RX = re.compile(r"(ê°€ëŠ¥í•©ë‹ˆë‹¤|ê°€ëŠ¥í•´ìš”)")

# ê°•í•œ í–‰ë™ ìœ ë„í˜• í‘œí˜„(ê´‘ê³ í†¤)
_STRONG_CTA_RX = re.compile(
    r"(ë†“ì¹˜ì§€\s*ë§ˆì„¸ìš”|ì„œë‘ë¥´ì„¸ìš”|ì„œë‘˜ëŸ¬\s*ì£¼ì„¸ìš”|ì§€ê¸ˆ\s*ë°”ë¡œ\s*ì‹ ì²­|ì§€ê¸ˆ\s*ë°”ë¡œ\s*ì˜ˆì•½)"
)

# ë¬¸ì¥ ë§ˆë¬´ë¦¬/ì¶”ìƒ í‘œí˜„/ì ‘ì†ì‚¬ íŒ¨í„´ìš©
_MECH_END_RX = re.compile(
    r"(ë„ì™€ë“œë¦¬ê² ìŠµë‹ˆë‹¤|ì•ˆë‚´í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤|ì§„í–‰í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤|"
    r"í™•ì¸í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤|ì ìš©í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤|ì…ë‹ˆë‹¤|í•©ë‹ˆë‹¤|ë©ë‹ˆë‹¤)$"
)

_VAGUE_TREND_RX = re.compile(
    r"(ë§ì´|ì ì |ê³„ì†|ê¾¸ì¤€íˆ)\s*(ëŠ˜ì–´ë‚˜|ì¦ê°€í•˜|ì¤„ì–´ë“¤|ê°ì†Œí•˜|ë†’ì•„ì§€|ë‚®ì•„ì§€)[^0-9%]{0,15}(ìˆìŠµë‹ˆë‹¤|ìˆì–´ìš”|ìˆìŠµë‹ˆë‹¤\.)?"
)

_GENERIC_MANY_PEOPLE_RX = re.compile(
    r"(ë§ì€|ë‹¤ì–‘í•œ)\s*(ë¶„ë“¤|ì‚¬ëŒë“¤|ì´ìš©ì|ê³ ê°).{0,15}(ì´ìš©í•˜ê³  ìˆìŠµë‹ˆë‹¤|ì°¾ê³  ìˆìŠµë‹ˆë‹¤|ì‚¬ìš©í•˜ê³  ìˆìŠµë‹ˆë‹¤)"
)

_CONNECTIVES = [
    "ë˜í•œ", "ê·¸ë¦¬ê³ ", "ê·¸ëŸ¬ë¯€ë¡œ", "ê·¸ë˜ì„œ", "ë”°ë¼ì„œ",
    "í•˜ì§€ë§Œ", "ê·¸ëŸ¬ë‚˜", "í•œí¸", "ê²Œë‹¤ê°€", "ì´ì™€ í•¨ê»˜",
    "ë¿ë§Œ ì•„ë‹ˆë¼", "ë¬´ì—‡ë³´ë‹¤", "ë¨¼ì €", "ë‹¤ìŒìœ¼ë¡œ", "ì¢…í•©í•˜ë©´",
]

_HELP_PHRASES = [
    "ë„ì™€ë“œë¦¬ê² ìŠµë‹ˆë‹¤", "ì•ˆë‚´í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤", "ì•ˆë‚´í•´ ë“œë¦¬ê² ìŠµë‹ˆë‹¤",
    "ì§„í–‰í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤", "ì§„í–‰í•´ ë“œë¦¬ê² ìŠµë‹ˆë‹¤",
    "í™•ì¸í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤", "í™•ì¸í•´ ë“œë¦¬ê² ìŠµë‹ˆë‹¤",
    "ì ìš©í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤", "ì ìš©í•´ ë“œë¦¬ê² ìŠµë‹ˆë‹¤",
]


def _count_hits(text: str, hints: list[str]) -> int:
    c = 0
    for h in hints:
        if h in text:
            c += 1
    return c


def analyze_doc_style(text: str):
    """
    ë¬¸ì„œ ì „ì²´ì˜ ë¬¸ì¥ ê¸¸ì´/ì–´íˆ¬/ì ‘ì†ì‚¬/ì•ˆë‚´ ë©˜íŠ¸/ì¶”ì„¸ í‘œí˜„ ë“±ì„ ê¸°ë°˜ìœ¼ë¡œ
    'ë¬¸ì²´/ì„œìˆ í˜•' ê´€ë ¨ ì¡°ì–¸ ë¬¸êµ¬ë§Œ ë§Œë“¤ì–´ ì£¼ëŠ” í—¬í¼.
    ì ìˆ˜ëŠ” ì•ˆ ì“°ê³ , ì¡°ì–¸(advice)ë§Œ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    # ê¸°ë³¸ í† í°/ë¬¸ì¥ ìˆ˜
    norm = kr_norm(text)
    tokens = re.findall(r"[ê°€-í£A-Za-z0-9]+", norm)
    n_tok = max(1, len(tokens))

    sents = basic_kr_sentence_split(text)
    sent_texts = [s.strip() for s in sents if s.strip()]
    n_sent = max(1, len(sent_texts))

    # ë¬¸ì¥ ê¸¸ì´(ë¬¸ì ìˆ˜) í†µê³„
    sent_lens = [len(s) for s in sent_texts]
    avg_len = sum(sent_lens) / len(sent_lens) if sent_lens else 0
    long_sent_count = sum(1 for L in sent_lens if L >= 50)

    # ê¸°ì¡´ í‚¤ì›Œë“œ ì¹´ìš´íŠ¸(ì •ë³´/í›„ê¸°/CTA) â€“ íƒ€ì… ë¶„ë¥˜ëŠ” ì•ˆ ì“°ì§€ë§Œ
    # ì¡°ì–¸ ë¬¸êµ¬ì— ì°¸ê³ ìš©ìœ¼ë¡œë§Œ ì‚¬ìš©
    info_hits = _count_hits(text, _INFO_HINTS)
    review_hits = _count_hits(text, _REVIEW_HINTS)
    cta_hits = _count_hits(text, _CTA_HINTS)

    fp_hits = _count_hits(text, _FIRST_PERSON)
    opinion_hits = _count_hits(text, _OPINION_ADJ)
    time_expr_hits = len(_TIME_EXPR_RX.findall(text))
    phone_hits = len(_PHONE_RX.findall(text))

    # ì–´íˆ¬: ê³µì†í•œ ì„¤ëª…ì²´ / ëŠë‚Œí‘œ ë¹„ìœ¨
    polite_ends = ("ìŠµë‹ˆë‹¤", "í•©ë‹ˆë‹¤", "ë©ë‹ˆë‹¤", "ë˜ì–´ìš”", "í•´ìš”")
    polite_sent = 0
    exclam_sent = 0

    # ê¸°ê³„ì ì¸ ë¬¸ì¥ ë íŒ¨í„´(ì…ë‹ˆë‹¤/í•©ë‹ˆë‹¤/ë“œë¦¬ê² ìŠµë‹ˆë‹¤...)
    mech_ends = [
        "ì…ë‹ˆë‹¤", "í•©ë‹ˆë‹¤", "ë©ë‹ˆë‹¤", "ë“œë¦¬ê² ìŠµë‹ˆë‹¤", "í•´ ë“œë¦¬ê² ìŠµë‹ˆë‹¤",
        "í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤", "ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤", "í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤",
    ]
    mech_counts = {m: 0 for m in mech_ends}

    # ì ‘ì†ì‚¬/ë„ì…ë¶€
    connective_hits = 0
    connective_start_hits = 0

    for s in sent_texts:
        st = s.strip()
        # ë¬¸ì¥ ëì˜ ë§ˆì¹¨í‘œ/ì‰¼í‘œ/ëŠë‚Œí‘œ ë“±ì€ ì œê±°í•˜ê³  ì–´ë¯¸ë§Œ ë¹„êµ
        core = st.rstrip(" .,!?â€¦~")

        if any(core.endswith(e) for e in polite_ends):
            polite_sent += 1
        if "!" in st:
            exclam_sent += 1

        for m in mech_ends:
            if core.endswith(m):
                mech_counts[m] += 1
                break

        # ì ‘ì†ì‚¬: ë¬¸ì¥ ì•ìª½ì— ì˜¤ëŠ” ê²½ìš°/ì „ì²´ ë“±ì¥ íšŸìˆ˜ ëª¨ë‘ ì²´í¬
        for w in _CONNECTIVES:
            if w in st:
                connective_hits += st.count(w)
            if st.startswith(w):
                connective_start_hits += 1
                break

    polite_ratio = polite_sent / n_sent if n_sent else 0
    exclam_ratio = exclam_sent / n_sent if n_sent else 0

    # ê·¼ê±° ì—†ëŠ” 'ëŠ˜ì–´ë‚˜ê³  ìˆìŠµë‹ˆë‹¤ / ë§ì•„ì§€ê³  ìˆìŠµë‹ˆë‹¤' ì‹ í‘œí˜„
    vague_trend_hits = len(_VAGUE_TREND_RX.findall(text))
    generic_many_hits = len(_GENERIC_MANY_RX.findall(text))
    trend_hits = vague_trend_hits + generic_many_hits
    has_number = bool(re.search(r"\d", norm)) or ("í¼ì„¼íŠ¸" in norm) or ("ë°°" in norm)

    # 'ë„ì™€ë“œë¦¬ê² ìŠµë‹ˆë‹¤/ì•ˆë‚´í•´ ë“œë¦¬ê² ìŠµë‹ˆë‹¤' ì—°ì† ì•ˆë‚´ ë©˜íŠ¸
    help_hits = _count_hits(text, _HELP_PHRASES)

    # ì¸ê¸°/ìœ ëª…/ê°€ì„±ë¹„, ë¶ˆíŠ¹ì • ë‹¤ìˆ˜, 'ê°€ëŠ¥í•©ë‹ˆë‹¤', ê°•í•œ CTA íŒ¨í„´ ì¹´ìš´íŠ¸
    popular_hits = len(_POPULAR_EXPR_RX.findall(text))
    generic_people_hits = len(_GENERIC_PEOPLE_RX.findall(text))
    possible_hits = len(_POSSIBLE_EXPR_RX.findall(text))
    strong_cta_hits = len(_STRONG_CTA_RX.findall(text))

    # ì§€ë°°ì ì¸ ê¸°ê³„ì  ì–´ë¯¸ ë¹„ìœ¨
    dominant_mech = max(mech_counts.values()) if mech_counts else 0
    mech_ratio = dominant_mech / n_sent if n_sent else 0

    advice: list[str] = []

    # 1) ë¬¸ì¥ì´ ë„ˆë¬´ ê¸¸ê±°ë‚˜ ê¸´ ë¬¸ì¥ì´ ì—°ì†ë˜ëŠ” ê²½ìš°
    if avg_len >= 45 or long_sent_count >= max(2, n_sent // 3):
        advice.append(
            "ë¬¸ì¥ì˜ í‰ê·  ê¸¸ì´ê°€ ê¸¸ê²Œ ë‚˜ì˜µë‹ˆë‹¤. í•œ ë¬¸ì¥ ì•ˆì— ì—¬ëŸ¬ ì •ë³´ë¥¼ ë„£ì§€ ë§ê³  "
            "2~3ê°œì˜ ì§§ì€ ë¬¸ì¥ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ë¦¬ë“¬ì„ ë§Œë“¤ì–´ ì£¼ì„¸ìš”."
        )

    # 2) ê³µì†ì²´ ì–´ë¯¸ê°€ ì§€ë‚˜ì¹˜ê²Œ ë§ì€ ê²½ìš°
    if polite_ratio >= 0.7 and n_sent >= 5:
        advice.append(
            "ìŠµë‹ˆë‹¤/í•©ë‹ˆë‹¤/ë¼ìš” ê°™ì€ ê³µì†ì²´ ì–´ë¯¸ ë¹„ìœ¨ì´ ë†’ìŠµë‹ˆë‹¤. "
            "ì„¤ëª…í˜• ë¬¸ì¥ ì‚¬ì´ì— '-ë‹¤', '-ì¸ í¸ì…ë‹ˆë‹¤', '-í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤'ì²˜ëŸ¼ "
            "ì–´ë¯¸ ë³€í™”ë¥¼ ì„ì–´ ì£¼ë©´ ê¸°ê³„ì ì¸ íŒ¨í„´ì„ ì¤„ì´ëŠ” ë° ë„ì›€ì´ ë©ë‹ˆë‹¤."
        )

    # 3) ëŠë‚Œí‘œ ê³¼ë‹¤ ì‚¬ìš©
    if exclam_ratio >= 0.25:
        advice.append(
            "ëŠë‚Œí‘œê°€ ë§ì´ ì‚¬ìš©ë˜ì—ˆìŠµë‹ˆë‹¤. ì¤‘ìš”í•œ í•œë‘ ë¬¸ì¥ì—ë§Œ ëŠë‚Œí‘œë¥¼ ë‚¨ê¸°ê³  "
            "ë‚˜ë¨¸ì§€ëŠ” ë§ˆì¹¨í‘œë¡œ ì •ë¦¬í•˜ë©´ ê²€ìƒ‰ì—”ì§„ì´ ê³¼í•œ ê´‘ê³  í†¤ìœ¼ë¡œ ë³´ì§€ ì•ŠìŠµë‹ˆë‹¤."
        )

    # 4) CTA/ë¬¸ì˜ í‘œí˜„ì´ ë§ì„ ë•Œ
    if cta_hits >= 3 or phone_hits >= 1:
        advice.append(
            "ìƒë‹´/ë¬¸ì˜/ì˜ˆì•½/ì „í™”ë²ˆí˜¸ ê°™ì€ í‘œí˜„ì´ ì—¬ëŸ¬ ë²ˆ ë“±ì¥í•©ë‹ˆë‹¤. "
            "ë³¸ë¬¸ì—ì„œëŠ” ì •ë³´Â·ê²½í—˜ ì¤‘ì‹¬ìœ¼ë¡œ ì“°ê³ , ë§ˆì§€ë§‰ ë‹¨ë½ì—ë§Œ ê°„ë‹¨íˆ "
            "ë¬¸ì˜Â·ì—°ë½ì²˜ë¥¼ ì •ë¦¬í•˜ëŠ” í¸ì´ ì•ˆì „í•©ë‹ˆë‹¤."
        )

    # 5) ì •ë³´ í‚¤ì›Œë“œì™€ í›„ê¸° í‚¤ì›Œë“œê°€ ëª¨ë‘ ë§ì€ ê²½ìš°
    if info_hits >= 5 and review_hits >= 5:
        advice.append(
            "ì„¤ëª…í˜• ë¬¸ì¥ê³¼ í›„ê¸°/ê²½í—˜ ë¬¸ì¥ì´ ì„ì—¬ ìˆìŠµë‹ˆë‹¤. "
            "ì •ë³´ ì•ˆë‚´ ë¶€ë¶„ê³¼ ì‹¤ì œ ì‚¬ìš© í›„ê¸° ë¶€ë¶„ì„ ì†Œì œëª©ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ì •ë¦¬í•˜ë©´ "
            "ê°€ë…ì„±ì´ ì¢‹ì•„ì§€ê³  ê²€ìƒ‰ì—”ì§„ë„ ì˜ë„ë¥¼ ë” ëª…í™•íˆ íŒŒì•…í•©ë‹ˆë‹¤."
        )

    # 6) 'ì…ë‹ˆë‹¤/í•©ë‹ˆë‹¤/ë“œë¦¬ê² ìŠµë‹ˆë‹¤' í•œ ê°€ì§€ ì–´ë¯¸ë¡œë§Œ ëë‚˜ëŠ” íŒ¨í„´
    if mech_ratio >= 0.6 and n_sent >= 4:
        advice.append(
            "ì—¬ëŸ¬ ë¬¸ì¥ì´ ê°™ì€ ì–´ë¯¸(ì˜ˆ: '~ì…ë‹ˆë‹¤', '~í•©ë‹ˆë‹¤', '~ë“œë¦¬ê² ìŠµë‹ˆë‹¤')ë¡œë§Œ ëë‚©ë‹ˆë‹¤. "
            "ë‹¨ë½ë³„ë¡œ ì–´ë¯¸ë¥¼ ì¡°ê¸ˆì”© ë°”ê¾¸ê³ , ë¬¸ì¥ ê¸¸ì´ë„ ì„ì–´ ì£¼ë©´ ì‚¬ëŒì´ ì§ì ‘ ì“´ ëŠë‚Œì´ ë” ê°•í•´ì§‘ë‹ˆë‹¤."
        )

    # 7) 'ë„ì™€ë“œë¦¬ê² ìŠµë‹ˆë‹¤/ì•ˆë‚´í•´ ë“œë¦¬ê² ìŠµë‹ˆë‹¤' ì•ˆë‚´ ë©˜íŠ¸ê°€ ì—°ì†ë˜ëŠ” ê²½ìš°
    if help_hits >= 3:
        advice.append(
            "ë„ì™€ë“œë¦¬ê² ìŠµë‹ˆë‹¤/ì•ˆë‚´í•´ ë“œë¦¬ê² ìŠµë‹ˆë‹¤ ê°™ì€ ì•ˆë‚´ ë¬¸ì¥ì´ ì—°ì†ìœ¼ë¡œ ë“±ì¥í•©ë‹ˆë‹¤. "
            "ì¤‘ê°„ì—ëŠ” ì‹¤ì œ ì •ë³´Â·ì˜ˆì‹œÂ·ê²½í—˜ì„ ë„£ê³ , ë§ˆì§€ë§‰ ë‹¨ë½ì—ë§Œ ì•ˆë‚´ ë¬¸ì¥ì„ ëª¨ì•„ì„œ ì •ë¦¬í•˜ëŠ” í¸ì´ ì¢‹ìŠµë‹ˆë‹¤."
        )

    # 8) ë˜í•œ/ê·¸ë¦¬ê³ /ê·¸ëŸ¬ë‚˜/ë”°ë¼ì„œ ë“± ì ‘ì†ì‚¬ ë°˜ë³µ
    if connective_start_hits >= 3 or connective_hits >= max(5, n_sent):
        advice.append(
            "ë˜í•œ/ê·¸ë¦¬ê³ /ê·¸ëŸ¬ë‚˜/ë”°ë¼ì„œ ê°™ì€ ì ‘ì†ì‚¬ê°€ ìì£¼ ë°˜ë³µë©ë‹ˆë‹¤. "
            "ëª¨ë“  ë¬¸ì¥ì„ ì ‘ì†ì‚¬ë¡œ ì‹œì‘í•˜ê¸°ë³´ë‹¤ëŠ”, í•µì‹¬ í‚¤ì›Œë“œë¥¼ ë¬¸ì¥ ì•ì— ë‘ê³  "
            "ë¬¸ë‹¨ ì‚¬ì´ ì—°ê²°ì´ í•„ìš”í•  ë•Œë§Œ ì ‘ì†ì‚¬ë¥¼ ì‚¬ìš©í•˜ëŠ” í¸ì´ ìì—°ìŠ¤ëŸ½ìŠµë‹ˆë‹¤."
        )

    # 9) 'ë§ì´ ëŠ˜ì–´ë‚˜ê³  ìˆìŠµë‹ˆë‹¤' ì‹ ì¶”ì„¸ í‘œí˜„ì¸ë° ìˆ«ì/ê¸°ì¤€ì´ ì—†ëŠ” ê²½ìš°
    if trend_hits >= 1 and not has_number:
        advice.append(
            "â€˜ë§ì´ ëŠ˜ì–´ë‚˜ê³  ìˆìŠµë‹ˆë‹¤â€™, â€˜ìˆ˜ìš”ê°€ ë†’ì•„ì§€ê³  ìˆìŠµë‹ˆë‹¤â€™ ê°™ì€ ì¶”ì„¸ í‘œí˜„ì´ ë‚˜ì˜¤ì§€ë§Œ "
            "êµ¬ì²´ì ì¸ ìˆ«ìë‚˜ ê¸°ê°„, ê¸°ì¤€ì´ ì—†ìŠµë‹ˆë‹¤. ìµœê·¼ 1~2ë…„ ê¸°ì¤€ì˜ í†µê³„Â·ê²€ìƒ‰ëŸ‰Â·ë¬¸ì˜ ê±´ìˆ˜ ë“± "
            "ê°„ë‹¨í•œ ê·¼ê±°ë¥¼ í•¨ê»˜ ì ì–´ ì£¼ë©´ ì„¤ë“ë ¥ê³¼ ì‹ ë¢°ë„ê°€ ì˜¬ë¼ê°‘ë‹ˆë‹¤."
        )

    # 10) ì¸ê¸°/ìœ ëª…/ê°€ì„±ë¹„ í‘œí˜„ì¸ë° ê¸°ì¤€ì´ ì—†ëŠ” ê²½ìš°
    if popular_hits >= 1 and not has_number:
        advice.append(
            "â€˜ì¸ê¸°â€™, â€˜ìœ ëª…â€™, â€˜ë§ì´ ì°¾ëŠ”â€™, â€˜ê°€ì„±ë¹„ê°€ ì¢‹ë‹¤â€™ ê°™ì€ í‘œí˜„ì´ ë‚˜ì˜¤ì§€ë§Œ "
            "ê¸°ê°„Â·ìˆ«ìÂ·ë¹„êµ ëŒ€ìƒ ë“±ì˜ ê¸°ì¤€ì´ ì—†ìŠµë‹ˆë‹¤. ìµœê·¼ 3~6ê°œì›” ê¸°ì¤€ì˜ ì´ìš©ì ìˆ˜ë‚˜ "
            "ë¹„êµ ëŒ€ìƒ(ë™ê¸‰ ì œí’ˆ, ì£¼ë³€ ìƒê¶Œ ë“±)ì„ í•¨ê»˜ ì ì–´ ì£¼ë©´ ì‹ ë¢°ë„ê°€ ë†’ì•„ì§‘ë‹ˆë‹¤."
        )

    # 11) 'ë§ì€ ë¶„ë“¤/ëŒ€ë¶€ë¶„ ì‚¬ëŒë“¤' ê°™ì€ ë¶ˆíŠ¹ì • ë‹¤ìˆ˜ í‘œí˜„
    if generic_people_hits >= 1:
        advice.append(
            "â€˜ë§ì€ ë¶„ë“¤â€™, â€˜ëŒ€ë¶€ë¶„ ì‚¬ëŒë“¤â€™, â€˜ì—¬ëŸ¬ ê³ ê°â€™ì²˜ëŸ¼ ëŒ€ìƒì„ ë„“ê²Œ í‘œí˜„í•œ ë¬¸ì¥ì´ ìˆìŠµë‹ˆë‹¤. "
            "â€˜ìµœê·¼ êµ¬ë§¤ìâ€™, â€˜ê¸°ì¡´ ì´ìš© ê³ ê°â€™, â€˜ë¬¸ì˜ ì£¼ì‹  ë¶„ë“¤â€™ì²˜ëŸ¼ ëŒ€ìƒì„ ì¡°ê¸ˆ ë” êµ¬ì²´ì ìœ¼ë¡œ "
            "ë‚˜ëˆ„ì–´ ì ì–´ ì£¼ë©´ ì„¤ë“ë ¥ì´ ì˜¬ë¼ê°‘ë‹ˆë‹¤."
        )

    # 12) 'ì§€ê¸ˆ ë°”ë¡œ ~í•˜ì„¸ìš”', 'ë†“ì¹˜ì§€ ë§ˆì„¸ìš”' ë“± ê°•í•œ í–‰ë™ ìœ ë„ í‘œí˜„
    if strong_cta_hits >= 2:
        advice.append(
            "â€˜ì§€ê¸ˆ ë°”ë¡œ ì‹ ì²­í•˜ì„¸ìš”â€™, â€˜ë†“ì¹˜ì§€ ë§ˆì„¸ìš”â€™, â€˜ì„œë‘ë¥´ì„¸ìš”â€™ ê°™ì€ ê°•í•œ í–‰ë™ ìœ ë„ ë¬¸ì¥ì´ "
            "ì—¬ëŸ¬ ë²ˆ ë“±ì¥í•©ë‹ˆë‹¤. ì •ë³´í˜•Â·í›„ê¸°í˜• ê¸€ì—ì„œëŠ” ê²°ë¡  ë¶€ë¶„ì— í•œë‘ ë²ˆë§Œ ì‚¬ìš©í•˜ê³ , "
            "ë³¸ë¬¸ì€ ì •ë³´Â·ê²½í—˜ ì¤‘ì‹¬ ì–´ì¡°ë¥¼ ìœ ì§€í•˜ëŠ” í¸ì´ ìì—°ìŠ¤ëŸ½ìŠµë‹ˆë‹¤."
        )

    # 13) '~ ê°€ëŠ¥í•©ë‹ˆë‹¤' ì•ˆë‚´í˜• í‘œí˜„ì´ ë°˜ë³µë˜ëŠ” ê²½ìš°
    if possible_hits >= 3:
        advice.append(
            "â€˜ì˜ˆì•½ ê°€ëŠ¥í•©ë‹ˆë‹¤â€™, â€˜ìƒë‹´ ê°€ëŠ¥í•©ë‹ˆë‹¤â€™ì²˜ëŸ¼ '~ ê°€ëŠ¥í•©ë‹ˆë‹¤' ì•ˆë‚´ ë¬¸ì¥ì´ ì—¬ëŸ¬ ë²ˆ ë°˜ë³µë©ë‹ˆë‹¤. "
            "ì•ˆë‚´ ì •ë³´ëŠ” ë§ˆì§€ë§‰ ë‹¨ë½ì— í•œ ë²ˆì— ëª¨ì•„ì„œ ì •ë¦¬í•˜ê³ , ë³¸ë¬¸ì—ì„œëŠ” ì‹¤ì œ ë‚´ìš©ê³¼ ì‚¬ë¡€ ìœ„ì£¼ë¡œ "
            "ì¨ ì£¼ë©´ ê³µì§€ë¬¸ ëŠë‚Œì„ ì¤„ì´ê³  ì½ê¸° í¸í•œ ê¸€ì´ ë©ë‹ˆë‹¤."
        )

    return {
        "ok": True,
        "type": "general",
        "mixed": None,
        # ê¸°ì¡´ í•„ë“œ í˜¸í™˜ìš© â€“ ì ìˆ˜ëŠ” ì˜ë¯¸ ì—†ìœ¼ë‹ˆ 0ìœ¼ë¡œ ê³ ì •
        "scores": {"info": 0.0, "review": 0.0, "promo": 0.0},
        "features": {
            "tokens": n_tok,
            "sentences": n_sent,
            "avg_sentence_len": round(avg_len, 1),
            "long_sentence_count": long_sent_count,
            "info_hits": info_hits,
            "review_hits": review_hits,
            "cta_hits": cta_hits,
            "first_person_hits": fp_hits,
            "opinion_hits": opinion_hits,
            "time_expr_hits": time_expr_hits,
            "phone_hits": phone_hits,
            "polite_ratio": round(polite_ratio, 3),
            "exclam_ratio": round(exclam_ratio, 3),
            "repeated_ending_types": repeated_ending_types,
            "connective_sentence_ratio": round(connective_sentence_ratio, 3),
            "vague_trend_sentences": vague_trend_sent,
            "popular_hits": popular_hits,
            "generic_people_hits": generic_people_hits,
            "possible_hits": possible_hits,
            "strong_cta_hits": strong_cta_hits,
        },
        "advice": advice,
    }

    # ê¸°ë³¸ í† í°/ë¬¸ì¥ ìˆ˜
    norm = kr_norm(text)
    tokens = re.findall(r"[ê°€-í£A-Za-z0-9]+", norm)
    n_tok = max(1, len(tokens))

    sents = basic_kr_sentence_split(text)
    sent_texts = [s.strip() for s in sents if s.strip()]
    n_sent = max(1, len(sent_texts))

    # ë¬¸ì¥ ê¸¸ì´(ë¬¸ì ìˆ˜) í†µê³„
    sent_lens = [len(s) for s in sent_texts]
    avg_len = sum(sent_lens) / len(sent_lens) if sent_lens else 0
    long_sent_count = sum(1 for L in sent_lens if L >= 50)

    # ê¸°ì¡´ í‚¤ì›Œë“œ ì¹´ìš´íŠ¸(ì •ë³´/í›„ê¸°/CTA)
    info_hits = _count_hits(text, _INFO_HINTS)
    review_hits = _count_hits(text, _REVIEW_HINTS)
    cta_hits = _count_hits(text, _CTA_HINTS)

    fp_hits = _count_hits(text, _FIRST_PERSON)
    opinion_hits = _count_hits(text, _OPINION_ADJ)
    time_expr_hits = len(_TIME_EXPR_RX.findall(text))
    phone_hits = len(_PHONE_RX.findall(text))

    # ì–´íˆ¬: ê³µì†í•œ ì„¤ëª…ì²´ / ëŠë‚Œí‘œ ë¹„ìœ¨
    polite_ends = ("ìŠµë‹ˆë‹¤", "í•©ë‹ˆë‹¤", "ë©ë‹ˆë‹¤", "ë˜ì–´ìš”", "í•´ìš”")
    polite_sent = 0
    exclam_sent = 0
    for s in sent_texts:
        st = s.strip()
        core = st.rstrip(" .,!?â€¦~")
        if any(core.endswith(e) for e in polite_ends):
            polite_sent += 1
        if "!" in st:
            exclam_sent += 1

    polite_ratio = polite_sent / n_sent
    exclam_ratio = exclam_sent / n_sent

    # ===== â‘  ê°™ì€ ì–´ë¯¸/ê°™ì€ êµ¬ì¡° ë°˜ë³µ (ì˜ˆ: ì…ë‹ˆë‹¤/í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤) =====
    ending_counts: dict[str, int] = {}
    im_ends = 0      # 'ì…ë‹ˆë‹¤'ë¡œ ëë‚˜ëŠ” ë¬¸ì¥ ìˆ˜
    hae_ends = 0     # '~í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤' ë¬¸ì¥ ìˆ˜

    for s in sent_texts:
        core = s.strip().rstrip(" .,!?â€¦~")
        if not core:
            continue
        if core.endswith("ì…ë‹ˆë‹¤"):
            im_ends += 1
        if core.endswith("í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤") or core.endswith("í•´ ë“œë¦¬ê² ìŠµë‹ˆë‹¤"):
            hae_ends += 1

        # ë’¤ì—ì„œ 6ê¸€ì ì •ë„ë¥¼ ì–´ë¯¸ íŒ¨í„´ìœ¼ë¡œ ë³¸ë‹¤
        ending = core[-6:] if len(core) >= 6 else core
        if len(ending) >= 3:
            ending_counts[ending] = ending_counts.get(ending, 0) + 1

    repetitive_endings = [e for e, c in ending_counts.items() if c >= 3]
    repeated_ending_types = len(repetitive_endings)

    # ===== â‘¡ ì ‘ì†ì‚¬ë¡œ ì‹œì‘í•˜ëŠ” ë¬¸ì¥ ë¹„ìœ¨ (ë˜í•œ/ê·¸ë¦¬ê³ /ê·¸ëŸ¬ë¯€ë¡œ ë“±) =====
    connective_start = 0
    connective_total = 0
    for s in sent_texts:
        st = s.lstrip()
        for cword in _CONNECTIVES:
            if cword in st:
                connective_total += st.count(cword)
            if st.startswith(cword):
                connective_start += 1
                break
    connective_sentence_ratio = connective_start / n_sent

    # ===== â‘¢ 'ë§ì´ ëŠ˜ì–´ë‚˜ê³  ìˆìŠµë‹ˆë‹¤' ë¥˜: í†µê³„ ì—†ëŠ” ì¶”ì„¸ í‘œí˜„ =====
    vague_trend_sent = 0
    for s in sent_texts:
        if _VAGUE_TREND_RX.search(s):
            # ê°™ì€ ë¬¸ì¥ ì•ˆì— êµ¬ì²´ ìˆ˜ì¹˜ê°€ ì—†ìœ¼ë©´ ë¬¸ì œë¡œ ë³¸ë‹¤
            if not re.search(
                r"\d|í¼ì„¼íŠ¸|%|ëª…|ê°œ|ê±´|íšŒ|ì›|ì‹œê°„|ë¶„|ì¼|ì£¼|ê°œì›”|ë‹¬|ë…„", s
            ):
                vague_trend_sent += 1

    # â”€â”€ ì—¬ê¸°ì„œë¶€í„° 'ë„ì™€ì£¼ëŠ” ì¡°ì–¸'ë§Œ ë§Œë“ ë‹¤ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    advice: list[str] = []

    # (ê¸°ì¡´) 1) ë¬¸ì¥ì´ ë„ˆë¬´ ê¸¸ê±°ë‚˜ ê¸´ ë¬¸ì¥ì´ ì—°ì†ë˜ëŠ” ê²½ìš°
    if avg_len >= 45 or long_sent_count >= max(2, n_sent // 3):
        advice.append(
            "ë¬¸ì¥ì˜ í‰ê·  ê¸¸ì´ê°€ ê¸¸ê²Œ ë‚˜ì˜µë‹ˆë‹¤. í•œ ë¬¸ì¥ ì•ˆì— ì—¬ëŸ¬ ì •ë³´ë¥¼ ë„£ì§€ ë§ê³  "
            "2~3ê°œì˜ ì§§ì€ ë¬¸ì¥ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ë¦¬ë“¬ì„ ë§Œë“¤ì–´ ì£¼ì„¸ìš”."
        )

    # (ê¸°ì¡´) 2) ê³µì†ì²´ ì–´ë¯¸ê°€ ì§€ë‚˜ì¹˜ê²Œ ë§ì€ ê²½ìš°
    if polite_ratio >= 0.7 and n_sent >= 5:
        advice.append(
            "ìŠµë‹ˆë‹¤/í•©ë‹ˆë‹¤/ë¼ìš” ê°™ì€ ê³µì†ì²´ ì–´ë¯¸ ë¹„ìœ¨ì´ ë†’ìŠµë‹ˆë‹¤. "
            "ì„¤ëª…í˜• ë¬¸ì¥ ì‚¬ì´ì— '-ë‹¤', '-ì¸ í¸ì…ë‹ˆë‹¤', '-í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤'ì²˜ëŸ¼ "
            "ì–´ë¯¸ ë³€í™”ë¥¼ ì„ì–´ ì£¼ë©´ ê¸°ê³„ì ì¸ íŒ¨í„´ì„ ì¤„ì´ëŠ” ë° ë„ì›€ì´ ë©ë‹ˆë‹¤."
        )

    # (ê¸°ì¡´) 3) ëŠë‚Œí‘œ ê³¼ë‹¤ ì‚¬ìš©
    if exclam_ratio >= 0.25:
        advice.append(
            "ëŠë‚Œí‘œê°€ ë§ì´ ì‚¬ìš©ë˜ì—ˆìŠµë‹ˆë‹¤. ì¤‘ìš”í•œ í•œë‘ ë¬¸ì¥ì—ë§Œ ëŠë‚Œí‘œë¥¼ ë‚¨ê¸°ê³  "
            "ë‚˜ë¨¸ì§€ëŠ” ë§ˆì¹¨í‘œë¡œ ì •ë¦¬í•˜ë©´ ê²€ìƒ‰ì—”ì§„ì´ ê³¼í•œ ê´‘ê³  í†¤ìœ¼ë¡œ ë³´ì§€ ì•ŠìŠµë‹ˆë‹¤."
        )

    # (ê¸°ì¡´) 4) CTA/ë¬¸ì˜ í‘œí˜„ì´ ë§ì„ ë•Œ
    if cta_hits >= 3 or phone_hits >= 1:
        advice.append(
            "ìƒë‹´/ë¬¸ì˜/ì˜ˆì•½/ì „í™”ë²ˆí˜¸ ê°™ì€ í‘œí˜„ì´ ì—¬ëŸ¬ ë²ˆ ë“±ì¥í•©ë‹ˆë‹¤. "
            "ë³¸ë¬¸ì—ì„œëŠ” ì •ë³´Â·ê²½í—˜ ì¤‘ì‹¬ìœ¼ë¡œ ì“°ê³ , ë§ˆì§€ë§‰ ë‹¨ë½ì—ë§Œ ê°„ë‹¨íˆ "
            "ë¬¸ì˜Â·ì—°ë½ì²˜ë¥¼ ì •ë¦¬í•˜ëŠ” í¸ì´ ì•ˆì „í•©ë‹ˆë‹¤."
        )

    # (ê¸°ì¡´) 5) ì •ë³´ í‚¤ì›Œë“œì™€ í›„ê¸° í‚¤ì›Œë“œê°€ ëª¨ë‘ ë§ì€ ê²½ìš°
    if info_hits >= 5 and review_hits >= 5:
        advice.append(
            "ì„¤ëª…í˜• ë¬¸ì¥ê³¼ í›„ê¸°/ê²½í—˜ ë¬¸ì¥ì´ ì„ì—¬ ìˆìŠµë‹ˆë‹¤. "
            "ì •ë³´ ì•ˆë‚´ ë¶€ë¶„ê³¼ ì‹¤ì œ ì‚¬ìš© í›„ê¸° ë¶€ë¶„ì„ ì†Œì œëª©ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ì •ë¦¬í•˜ë©´ "
            "ê°€ë…ì„±ì´ ì¢‹ì•„ì§€ê³  ê²€ìƒ‰ì—”ì§„ë„ ì˜ë„ë¥¼ ë” ëª…í™•íˆ íŒŒì•…í•©ë‹ˆë‹¤."
        )

    # (í™•ì¥ 1ë²ˆ) êµ¬ì²´ì  ìˆ˜ì¹˜ ì—†ì´ 'ë§ì´ ëŠ˜ì–´ë‚˜ê³  ìˆìŠµë‹ˆë‹¤'ë¥˜ í‘œí˜„
    if vague_trend_sent >= 1:
        advice.append(
            "â€˜ë§ì´ ëŠ˜ì–´ë‚˜ê³  ìˆìŠµë‹ˆë‹¤â€™ì²˜ëŸ¼ ì¦ê°€/ê°ì†Œë¥¼ ë§í•˜ì§€ë§Œ ìˆ«ìë‚˜ ê¸°ê°„ì´ ì—†ëŠ” "
            "í‘œí˜„ì´ ë³´ì…ë‹ˆë‹¤. ëª‡ %ì¸ì§€, ì–´ëŠ ê¸°ê°„ ë™ì•ˆ ì–¼ë§ˆë‚˜ ëŠ˜ì—ˆëŠ”ì§€ ë“± "
            "êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ë‚˜ ì‚¬ë¡€ë¥¼ í•œ ì¤„ë§Œ ë” ë¶™ì—¬ ì£¼ì„¸ìš”."
        )

    # (í™•ì¥ 4ë²ˆ) ê°™ì€ ì–´ë¯¸ ë°˜ë³µ â€“ íŠ¹íˆ 'ì…ë‹ˆë‹¤.'ì™€ 'í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤.'
    if hae_ends >= 3:
        advice.append(
            "ì—¬ëŸ¬ ë¬¸ì¥ì´ ëª¨ë‘ '~í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤.'ë¡œ ëë‚©ë‹ˆë‹¤. "
            "ì¼ë¶€ ë¬¸ì¥ì€ 'ë„ì™€ë“œë¦½ë‹ˆë‹¤', 'ì§€ì›í•©ë‹ˆë‹¤', 'ì§„í–‰í•©ë‹ˆë‹¤'ì²˜ëŸ¼ "
            "ì–´ë¯¸ì™€ í‘œí˜„ì„ ì¡°ê¸ˆì”© ë°”ê¿” ì£¼ë©´ ë” ì‚¬ëŒ ì†ì„ íƒ„ ëŠë‚Œì´ ë‚©ë‹ˆë‹¤."
        )
    elif im_ends >= 3 and im_ends >= n_sent * 0.6:
        advice.append(
            "ë¬¸ì¥ì˜ ìƒë‹¹ìˆ˜ê°€ 'ì…ë‹ˆë‹¤.'ë¡œ ëë‚©ë‹ˆë‹¤. "
            "ì¤‘ê°„ì¤‘ê°„ '-ì¸ í¸ì…ë‹ˆë‹¤', '-í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤', '-ë¡œ ì´ì–´ì§‘ë‹ˆë‹¤'ì²˜ëŸ¼ "
            "ì–´ë¯¸ë¥¼ ë‹¤ì–‘í•˜ê²Œ ì„ì–´ ì£¼ë©´ ê¸°ê³„ì ì¸ ë¦¬ë“¬ì„ ì¤„ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        )
    elif repeated_ending_types >= 1 and n_sent >= 4:
        advice.append(
            "ì—¬ëŸ¬ ë¬¸ì¥ì´ ê±°ì˜ ê°™ì€ ì–´ë¯¸ë¡œë§Œ ëë‚˜ëŠ” íŒ¨í„´ì´ ë³´ì…ë‹ˆë‹¤. "
            "ë¬¸ì¥ ë ì–´ë¯¸ë¥¼ 2~3ê°€ì§€ ì •ë„ë¡œ ë‚˜ëˆ„ì–´ ì“°ë©´ ìì—°ìŠ¤ëŸ¬ìš´ ì„œìˆ í˜•ì— ê°€ê¹ìŠµë‹ˆë‹¤."
        )

    # (ìƒˆ ê·œì¹™) ì ‘ì†ì‚¬ë¡œ ì‹œì‘í•˜ëŠ” ë¬¸ì¥ ë¹„ìœ¨ â€“ ë˜í•œ/ê·¸ë¦¬ê³ /ê·¸ëŸ¬ë¯€ë¡œ/ë”°ë¼ì„œâ€¦
    if connective_start >= 3 and connective_sentence_ratio >= 0.35:
        advice.append(
            "ì—¬ëŸ¬ ë¬¸ì¥ì´ 'ë˜í•œ/ê·¸ë¦¬ê³ /ê·¸ëŸ¬ë¯€ë¡œ/ë”°ë¼ì„œ' ê°™ì€ ì ‘ì†ì‚¬ë¡œ ì‹œì‘í•©ë‹ˆë‹¤. "
            "ì ‘ì†ì‚¬ëŠ” ë¬¸ë‹¨ ì „í™˜ì—ë§Œ ê°€ë³ê²Œ ì“°ê³ , ë‚˜ë¨¸ì§€ ë¬¸ì¥ì€ í‚¤ì›Œë“œë‚˜ ì£¼ì–´ë¡œ "
            "ë°”ë¡œ ì‹œì‘í•˜ë©´ AI íŒ¨í„´ ëŠë‚Œì„ ì¤„ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        )

    return {
        "ok": True,
        "type": "general",
        "mixed": None,
        "scores": {"info": 0.0, "review": 0.0, "promo": 0.0},
        "features": {
            "tokens": n_tok,
            "sentences": n_sent,
            "avg_sentence_len": round(avg_len, 1),
            "long_sentence_count": long_sent_count,
            "info_hits": info_hits,
            "review_hits": review_hits,
            "cta_hits": cta_hits,
            "first_person_hits": fp_hits,
            "opinion_hits": opinion_hits,
            "time_expr_hits": time_expr_hits,
            "phone_hits": phone_hits,
            "polite_ratio": round(polite_ratio, 3),
            "exclam_ratio": round(exclam_ratio, 3),
            "repeated_ending_types": repeated_ending_types,
            "connective_sentence_ratio": round(connective_sentence_ratio, 3),
            "vague_trend_sentences": vague_trend_sent,
        },
        "advice": advice,
    }

# --- ê³µë°±/ë¬¸ì ì •ê·œí™” & ê³µë°±ë¬´ì‹œ í‚¤ì›Œë“œìš© ---
def kr_norm(s: str) -> str:
    s = unicodedata.normalize("NFKC", s or "")
    s = s.replace("%", "í¼ì„¼íŠ¸")
    s = re.sub(r"\s+", " ", s)
    return s

def spacing_agnostic_regex(kw: str) -> str:
    parts = list(kw)
    return r"\s*".join(map(re.escape, parts))

# --- [ADD] í•„ìˆ˜ í‚¤ì›Œë“œ ìœ„ì¹˜ ì°¾ê¸°(ê³µë°± ë¬´ì‹œ + ëŒ€ì†Œë¬¸ì/ì „ê° í˜¸í™˜) ---
def _looks_like_regex(kw: str) -> bool:
    # (), [], {}, +, ?, *, |, . ê°™ì€ ë©”íƒ€ë¬¸ìê°€ ìˆì„ ë•Œë§Œ ì •ê·œì‹ìœ¼ë¡œ ì·¨ê¸‰
    return bool(re.search(r"[.^$*+?{}\[\]|()\\]", kw or ""))

def _find_positions_ko(hay: str, kw: str):
    hits = []
    if not hay or not kw:
        return hits

    if _looks_like_regex(kw):
        rx = re.compile(kw, re.IGNORECASE)
    else:
        # ì¼ë°˜ ë¬¸ìì—´ â†’ ê³µë°±/ê¸°í˜¸ ë¬´ì‹œ íŒ¨í„´ìœ¼ë¡œ ë³€í™˜
        rx = re.compile(spacing_agnostic_regex(kr_norm(kw)), re.IGNORECASE)

    for m in rx.finditer(hay):
        hits.append({"start": m.start(), "end": m.end()})
    return hits


def _guide_keyword_windows(text: str, template: str,
                           window_size: int = 80,
                           min_core_hits: int = 2,
                           max_terms: int = 5):
    """
    í…œí”Œë¦¿ ë¬¸ì¥ì—ì„œ í•µì‹¬ ë‹¨ì–´ë¥¼ ëª‡ ê°œ ë½‘ì•„ì„œ(core_terms),
    ì›ë¬¸ì—ì„œ window_size ê¸€ì ì•ˆì— ì„œë¡œ ë‹¤ë¥¸ í•µì‹¬ ë‹¨ì–´ê°€
    min_core_hits ê°œ ì´ìƒ ê°™ì´ ë“±ì¥í•˜ëŠ” êµ¬ê°„ì„ í›„ë³´ë¡œ ì°¾ëŠ”ë‹¤.

    ë°˜í™˜: [{start, end, sentence, score, core_hits, core_terms}, ...]
    """
    text = text or ""
    template = (template or "").strip()
    if not text or not template:
        return []

    window_size = max(30, int(window_size or 80))
    min_core_hits = max(1, int(min_core_hits or 2))

    # í…œí”Œë¦¿ì—ì„œ í•µì‹¬ ë‹¨ì–´ ì¶”ì¶œ (ì´ë¯¸ server.py ì— core_terms í•¨ìˆ˜ ìˆìŒ)
    terms = core_terms(template, max_terms=max_terms)
    if not terms:
        return []

    # ê° í•µì‹¬ ë‹¨ì–´ì˜ ì¶œí˜„ ìœ„ì¹˜ ìˆ˜ì§‘
    hits = []
    for t in terms:
        for h in _find_positions_ko(text, t):
            hits.append((h["start"], t))
    if not hits:
        return []

    hits.sort(key=lambda x: x[0])

    # ìŠ¬ë¼ì´ë”© ìœˆë„ìš°ë¡œ "ì„œë¡œ ë‹¤ë¥¸ í•µì‹¬ì–´" ê°œìˆ˜ ì„¸ê¸°
    from collections import defaultdict
    left = 0
    counts = defaultdict(int)
    distinct = set()
    n = len(hits)
    candidates = []
    seen_spans = set()

    for right in range(n):
        pos_r, term_r = hits[right]
        counts[term_r] += 1
        distinct.add(term_r)

        # í˜„ì¬ ìœˆë„ìš° í­ì´ window_size ë¥¼ ë„˜ìœ¼ë©´ ì™¼ìª½ ì¤„ì´ê¸°
        while left <= right and pos_r - hits[left][0] > window_size:
            pos_l, term_l = hits[left]
            counts[term_l] -= 1
            if counts[term_l] <= 0:
                distinct.discard(term_l)
            left += 1

        # í•µì‹¬ì–´ ì¢…ë¥˜ê°€ min_core_hits ê°œ ì´ìƒì´ë©´ í›„ë³´
        if len(distinct) >= min_core_hits:
            start = hits[left][0]
            end = min(start + window_size, len(text))
            span = (start, end)
            if span in seen_spans:
                continue
            seen_spans.add(span)
            seg = text[start:end]
            candidates.append({
                "start": start,
                "end": end,
                "sentence": seg,
                "score": float(len(distinct)),   # ì ìˆ˜ = ì„œë¡œ ë‹¤ë¥¸ í•µì‹¬ì–´ ê°œìˆ˜
                "core_hits": len(distinct),
                "core_terms": sorted(distinct),
            })

    # í•µì‹¬ì–´ ê°œìˆ˜(desc) â†’ ê¸¸ì´ ì§§ì€ ìˆœ â†’ ì‹œì‘ ìœ„ì¹˜ ìˆœ
    candidates.sort(key=lambda c: (-c["core_hits"], c["end"] - c["start"], c["start"]))
    return candidates


# ================== ì‹¬ì˜ ê·œì¹™ ë¡œë”©/ìŠ¤ìº” ==================
def load_rules(path=RULE_PATH):
    global RULES, RULES_INDEX
    try:
        with open(path, "r", encoding="utf-8") as f:
            RULES = yaml.safe_load(f) or {}
        if not isinstance(RULES, dict):
            RULES = {}
        print(f"âœ… ì‹¬ì˜ ê·œì¹™ ë¡œë“œ ì™„ë£Œ: {path}")
        pack = RULES.get("pack")
        ver = RULES.get("version")
        topics = RULES.get("topics")
        if pack or ver or topics:
            print(f"  - pack={pack}, version={ver}, topics={topics}")
        RULES_INDEX = {}
        for r in (RULES.get("rules") or []):
            rid = (r or {}).get("id")
            if rid:
                RULES_INDEX[str(rid)] = r
        print(f"  - rules indexed: {len(RULES_INDEX)}")
    except Exception as e:
        print("[WARN] ì‹¬ì˜ ê·œì¹™ ë¡œë“œ ì‹¤íŒ¨:", e)
        RULES, RULES_INDEX = {}, {}


load_rules()

# === (ADD) ì‚¬ìœ /ë²•ë ¹ ë§¤í•‘ ìœ í‹¸ (YAML meta ì‚¬ìš©) ====================
def _load_reason_index_from_rules():
    meta = (RULES or {}).get("meta") or {}
    cats = meta.get("reason_categories") or {}
    mapping = []
    for ent in meta.get("rule_category_by_prefix") or []:
        try:
            rx = re.compile(ent.get("pattern", ""), re.IGNORECASE)
            mapping.append((rx, ent.get("category")))
        except re.error:
            pass
    return cats, mapping

REASON_CATS, REASON_MAP = _load_reason_index_from_rules()

def _reason_for_rule_id(rule_id: str):
    rid = rule_id or ""
    for rx, cat in (REASON_MAP or []):
        if rx.search(rid):
            info = (REASON_CATS or {}).get(cat) or {}
            reason = info.get("reason")
            legal  = info.get("legal")
            url    = info.get("legal_url")
            if not reason:
                return None, None
            small = None
            if legal and url:
                small = f"<small style='color:#777'>(ê·¼ê±°: {legal} Â· <a href=\"{url}\" target=\"_blank\">ë²•ë ¹</a>)</small>"
            elif legal:
                small = f"<small style='color:#777'>(ê·¼ê±°: {legal})</small>"
            return f"ì‚¬ìœ : {reason}", small
    return None, None

_rule_id_rx = re.compile(r"\(rule:([^)]+)\)")
def attach_reasons(items):
    """
    ê²°ê³¼ í•­ëª©ì— ì‚¬ìš©ì ì¹œí™”ì  ì‚¬ìœ /ì¶œì²˜ë¥¼ ë¶€ì°©í•˜ê³ ,
    ë‚´ë¶€ rule ID ë…¸ì¶œì„ ì œê±°í•œë‹¤.
    ìš°ì„ ìˆœìœ„:
      1) RULES_INDEX[rule_id]ì˜ rationale / legal_ref
      2) meta(reason_categories/rule_category_by_prefix) í´ë°±
    """
    for it in items:
        rid = it.get("rule_id")
        if not rid:
            m = _rule_id_rx.search(it.get("reason","") or "")
            rid = m.group(1) if m else None

        reason_line = None
        legal_small = None

        # 1) ê·œì¹™ ë³¸ë¬¸ì—ì„œ ì§ì ‘ ì·¨ë“
        rule = RULES_INDEX.get(str(rid)) if rid else None
        if rule:
            rationale = rule.get("rationale") or rule.get("description") or ""
            legal     = rule.get("legal_ref")  or ""   # YAMLì— ì„ íƒì ìœ¼ë¡œ ì¶”ê°€
            if rationale:
                reason_line = rationale
            if legal:
                legal_small = f"<small style='color:#777'>(ì¶œì²˜: {legal})</small>"

        # 2) í´ë°±: meta ì¹´í…Œê³ ë¦¬ ë§¤í•‘ ì‚¬ìš©
        if not reason_line:
            rline, small = _reason_for_rule_id(rid or "")
            if rline:
                # rline ì˜ˆ: "ì‚¬ìœ : â€¦" í˜•íƒœ â†’ í†µì¼ ìœ„í•´ ì ‘ë‘ì–´ ì œê±°
                reason_line = rline.replace("ì‚¬ìœ :", "").strip()
            if small:
                legal_small = small

        if reason_line:
            it["reason_line"] = reason_line
        if legal_small:
            it["legal_small"] = legal_small

        # í™”ë©´ ë…¸ì¶œìš© reason ë¬¸ìì—´ì—ì„œ (rule:XXX) ì œê±°
        if it.get("reason"):
            it["reason"] = re.sub(_rule_id_rx, "", it["reason"]).strip()
    return items# =====================================================================

def _iter_rule_patterns(rule):
    patterns = ((rule or {}).get("patterns") or {}).get("any") or []
    for p in patterns:
        if not isinstance(p, dict):
            continue
        if p.get("type") in ("keyword", "regex", "phrase"):
            yield p

def _compile_regex_from_pattern(pat: dict):
    ptype = pat.get("type")
    flags = 0
    if str(pat.get("flags","")).lower().find("i") >= 0 or pat.get("casefold", True):
        flags |= re.IGNORECASE

    if ptype == "keyword":
        val = kr_norm(pat.get("value",""))
        if not val:
            return None, None
        if pat.get("spacing_agnostic", True):
            rx = spacing_agnostic_regex(val)
        else:
            rx = re.escape(val)
        return re.compile(rx, flags), f"kw:{val}"

    if ptype == "regex":
        val = pat.get("value","")
        if not val:
            return None, None
        return re.compile(val, flags), "rx"

    if ptype == "phrase":
        terms = [kr_norm(t) for t in (pat.get("terms") or []) if t]
        if len(terms) < 2:
            return None, None
        max_gap = int(pat.get("max_gap", 6))
        terms_rx = [spacing_agnostic_regex(t) for t in terms]
        rx = terms_rx[0]
        for nxt in terms_rx[1:]:
            rx += rf".{{0,{max_gap}}}" + nxt
        return re.compile(rx, flags), f"ph:{'|'.join(terms)}"
    return None, None

def _rule_excepts(rule):
    ex = ((rule or {}).get("except") or {}).get("any") or []
    out = []
    for r in ex:
        try:
            out.append(re.compile(r, re.IGNORECASE))
        except re.error:
            pass
    return out

def _scan_with_rule(text, rule):
    hits = []
    exceptors = _rule_excepts(rule)
    window = int((rule or {}).get("except_window", 14))
    for p in _iter_rule_patterns(rule):
        rx, _ = _compile_regex_from_pattern(p)
        if not rx:
            continue
        for m in rx.finditer(text):
            s, e = m.start(), m.end()
            seg = text[s:e]
            if exceptors:
                ctx = text[max(0, s - window): min(len(text), e + window)]
                if any(ex.search(ctx) for ex in exceptors):
                    continue
            hits.append((s, e, seg))
    return hits

def rule_scan(text):
    rules = (RULES or {}).get("rules") or []
    items = []
    gid = 0
    for rule in rules:
        rid   = rule.get("id", "")
        topic = rule.get("topic", "")
        sev   = (rule.get("severity") or "warn").lower()
        rationale = rule.get("rationale") or rule.get("description") or ""
        actions   = rule.get("actions") or []
        rule_type = "ì‹¬ì˜ìœ„ë°˜" if sev == "block" else "ì£¼ì˜í‘œí˜„"

        for (s, e, seg) in _scan_with_rule(text, rule):
            before, after = add_context(text, s, e - s)
            sugg = []
            if any(str(a).startswith("suggest:") for a in actions):
                human = "í‘œí˜„ ì™„í™”/ê·¼ê±° ì œì‹œ/ì£¼ì˜ë¬¸ ë³‘ê¸° ê²€í† "
                if "MED" in " ".join(actions):
                    human = "ì˜ë£Œ í‘œí˜„ ì™„í™” ë˜ëŠ” ê°ê´€ ê·¼ê±° ì œì‹œ"
                elif "HFS" in " ".join(actions):
                    human = "ê±´ê¸°ì‹/ì˜ì•½í’ˆ ì˜¤ì¸ ë°©ì§€ ë¬¸êµ¬ë¡œ ìˆ˜ì •"
                elif "GEN_NEED_EVIDENCE" in " ".join(actions):
                    human = "ë¹„êµÂ·ìš°ì›” í‘œí˜„ì€ ì¶œì²˜/ê·¼ê±° ë³‘ê¸°"
                sugg = [human]
            else:
                sugg = ["ë¬¸êµ¬ ì™„í™” ë˜ëŠ” ì‚­ì œ ê²€í† "]

            items.append({
                "id": f"r_{gid}",
                "type": rule_type,
                "original": seg,
                "suggestions": sugg[:3],
                "reason": f"[{topic}] {rationale or 'ê·œì¹™ ë§¤ì¹­'}",
                "rule_id": rid,
                "severity": "high" if sev == "block" else "medium",
                "startIndex": s,
                "endIndex": e,
                "before": before,
                "after": after
            })
            gid += 1

    used = set()
    dedup = []
    for it in sorted(items, key=lambda x: (x["startIndex"], -(x["endIndex"]-x["startIndex"]))):
        k = (it["startIndex"], it["endIndex"], it["type"])
        if k in used:
            continue
        used.add(k)
        dedup.append(it)
    return dedup

# === [ADD] ì™„ê³¡ ë©˜íŠ¸ í¬ë§·í„° =========================================
def _hedged_message(kind: str, score=None, threshold=None):
    """
    kind: "present" | "borderline" | "absent" | "forbid"
    ì¼ê´€ëœ í†¤ ìœ ì§€(ë‹¨ì • ëŒ€ì‹  ì™„ê³¡).
    """
    if kind == "forbid":
        return "ê¸ˆì§€ì–´ë¡œ ë¶„ë¥˜ëœ í‘œí˜„ì´ í¬í•¨ë˜ì—ˆì„ ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤."
    if kind == "present":
        return "í¬í•¨ë˜ì—ˆì„ ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤."  # (ìœ ì‚¬ë„/í‚¤ì›Œë“œ ì¶©ì¡±)
    if kind == "borderline":
        return "ìœ ì‚¬í•˜ê±°ë‚˜ ì¼ë¶€ í¬í•¨ë˜ì–´ í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤."
    return "ëª…í™•í•œ ì¼ì¹˜ê°€ í™•ì¸ë˜ê¸° ì–´ë µìŠµë‹ˆë‹¤."
# ===================================================================

# === [ADD] ê¸ˆì§€ì–´ íŒŒì‹±/ê²€ì‚¬ =========================================
_FORBID_HEAD_RX = re.compile(r"^\s*ê¸ˆì§€ì–´\s*[:\-]\s*(.+)$", re.IGNORECASE)

def _parse_forbid_lines(lines):
    """
    lines: list[str]  (í•„ìˆ˜ê°€ì´ë“œ ì…ë ¥ë€ì˜ ê° ì¤„)
    ì§€ì› í˜•íƒœ:
      - "ê¸ˆì§€ì–´: ë‹¨ì–´1,ë‹¨ì–´2,ë‹¨ì–´3"
      - "ê¸ˆì§€ì–´- ë‹¨ì–´1 ë‹¨ì–´2" (ê³µë°± êµ¬ë¶„ë„ í—ˆìš©)
    ë°˜í™˜: ê¸ˆì§€ì–´ ë¦¬ìŠ¤íŠ¸(list[str])
    """
    out = []
    for ln in (lines or []):
        m = _FORBID_HEAD_RX.match(ln or "")
        if not m:
            continue
        payload = kr_norm(m.group(1))
        # ì‰¼í‘œ ê¸°ì¤€ 1ì°¨ ë¶„ë¦¬ â†’ ì—†ìœ¼ë©´ ê³µë°± ë¶„ë¦¬
        if "," in payload:
            toks = [t.strip() for t in payload.split(",")]
        else:
            toks = [t.strip() for t in re.split(r"\s+", payload)]
        out.extend([t for t in toks if t])
    # ì¤‘ë³µ ì œê±°
    uniq = []
    seen = set()
    for t in out:
        if t not in seen:
            uniq.append(t); seen.add(t)
    return uniq

def _find_forbidden_hits(text: str, forbid_terms: list[str]):
    """
    spacing-agnostic(ê¸€ì ì‚¬ì´ ì„ì˜ ê³µë°± í—ˆìš©) + ëŒ€ì†Œë¬¸ì/ì „ê° í˜¸í™˜.
    ë°˜í™˜: [{"term":..., "start":..., "end":..., "seg":...}, ...]
    """
    text = text or ""
    hits = []
    for term in (forbid_terms or []):
        rx = re.compile(spacing_agnostic_regex(term), re.IGNORECASE)
        for m in rx.finditer(text):
            s, e = m.start(), m.end()
            hits.append({"term": term, "start": s, "end": e, "seg": text[s:e]})
    return hits
# ===================================================================


# ============== (NEW) ë„ëŒì´í‘œ/ìœ ì‚¬ë¬¸ì¥ íƒì§€ ìœ í‹¸ ==============
_punc_rx = re.compile(r"[^\w\u3131-\u318E\uAC00-\uD7A3]+", re.UNICODE)
_ws_rx = re.compile(r"\s+")


def _norm_for_dup(s: str) -> str:
    s = unicodedata.normalize("NFKC", s or "")
    s = s.lower()
    s = _punc_rx.sub(" ", s)
    s = _ws_rx.sub(" ", s).strip()
    return s

def _norm_for_dup_strong(s: str) -> str:
    """
    ë‹¤ë¬¸ì„œ ìœ ì‚¬ë„ ì „ìš© ê°•í•œ ì •ê·œí™”:
    - NFKC ì •ê·œí™” + ì†Œë¬¸ì
    - êµ¬ë‘ì /ê³µë°± ì œê±°
    - ì¡°ì‚¬/ì–´ë¯¸ ê°™ì€ ê»ë°ê¸°ë¥¼ ìµœëŒ€í•œ ì–‡ê²Œ ë§Œë“¤ì–´ì„œ 'ë‚´ìš©'ì— ë” ë¯¼ê°í•˜ê²Œ
    """
    s = unicodedata.normalize("NFKC", s or "")
    s = s.lower()
    s = _punc_rx.sub(" ", s)
    s = _ws_rx.sub(" ", s).strip()
    if not s:
        return ""

    tokens = s.split()
    # ìì£¼ ì“°ëŠ” ì¡°ì‚¬/ì ‘ì†ì‚¬ í† í°
    DROP_TOKENS = {
        "ì€", "ëŠ”", "ì´", "ê°€", "ì„", "ë¥¼", "ë„", "ë§Œ",
        "ì—", "ì—ì„œ", "ì—ê²Œ", "ìœ¼ë¡œ", "ë¡œ", "ì™€", "ê³¼",
        "ë°", "ë˜ëŠ”", "ê·¸ë¦¬ê³ ", "í•˜ì§€ë§Œ", "ê·¸ëŸ¬ë‚˜"
    }

    cleaned = []
    for tok in tokens:
        if tok in DROP_TOKENS:
            continue
        # í•œ ê¸€ìì§œë¦¬ ì¡°ì‚¬/ì–´ë¯¸ ì˜ë¼ë‚´ê¸° (ëŒ€ëµì ì¸ ì²˜ë¦¬)
        if len(tok) > 1 and tok[-1] in "ì€ëŠ”ì´ê°€ì„ë¥¼ë„ë§Œë¡œê³¼ì™€ì—":
            cleaned.append(tok[:-1])
        else:
            cleaned.append(tok)

    return " ".join(t for t in cleaned if t)


def _word_ngrams_for_dup(s: str, n: int = 2, step: int = 1) -> set:
    """
    ë‹¨ì–´ ë‹¨ìœ„ n-gram ì§‘í•©.
    - ë¦¬ë¼ì´íŒ…(ë‹¨ì–´ ì¡°ê¸ˆ ë°”ê¾¸ê¸°/ìˆœì„œ ì‚´ì§ ë³€ê²½)ì— ë” ê°•í•˜ê²Œ ë°˜ì‘í•˜ë„ë¡ ì‚¬ìš©.
    """
    s = _norm_for_dup_strong(s or "")
    if not s:
        return set()
    words = s.split()
    if len(words) < n:
        return set()
    out = set()
    for i in range(0, len(words) - n + 1, max(1, int(step))):
        out.add(" ".join(words[i:i + n]))
    return out


def _char_ngrams(s: str, n: int = 3) -> set[str]:
    """ë‹¨ì¼ ê¸¸ì´ n ì— ëŒ€í•œ ë¬¸ì n-gram ì§‘í•©"""
    s = _norm_for_dup(s)
    return {s[i:i + n] for i in range(max(0, len(s) - n + 1))} if s else set()


def _char_ngrams_multi(s: str, lens=(2, 3, 4)) -> set[str]:
    """
    ì—¬ëŸ¬ ê¸¸ì´ n(2,3,4 ë“±)ì— ëŒ€í•œ ë¬¸ì n-gram ì§‘í•©ì„ í•œ ë²ˆì— ë§Œë“ ë‹¤.
    guide_verify_local / ë„ëŒì´í‘œÂ·ìœ ì‚¬ë¬¸ì¥ íƒì§€ì—ì„œ í…œí”Œë¦¿ ê¸¸ì´ë³„ ìœ ì‚¬ë„ ê³„ì‚°ì— ì‚¬ìš©.
    """
    s = _norm_for_dup(s or "")
    out: set[str] = set()
    for n in lens:
        if not isinstance(n, int) or n <= 0:
            continue
        L = len(s)
        if L < n:
            continue
        for i in range(0, L - n + 1):
            out.add(s[i:i + n])
    return out


def _jaccard(a: set, b: set) -> float:
    if not a and not b:
        return 1.0
    if not a or not b:
        return 0.0
    inter = len(a & b)
    union = len(a | b)
    return inter / union if union else 0.0


def _sentence_spans(text: str):
    """ë¬¸ì¥ ë‹¨ìœ„ë¡œ (start, end, raw_sentence) ë°˜í™˜"""
    sentences = basic_kr_sentence_split(text)
    spans: list[tuple[int, int, str]] = []
    cursor = 0
    for s in sentences:
        idx = text.find(s, cursor)
        if idx == -1:
            idx = text.find(s)
        if idx != -1:
            spans.append((idx, idx + len(s), s))
            cursor = idx + len(s)
    return spans

def _dedup_inter_lite(files,
                     ui_min_percent: float = 0.10,
                     ui_top_k: int = 10,
                     shingle_n: int = 3,
                     shingle_step: int = 2,
                     detail_top_k: int = 3):
    """
    UI/ë³´ê³ ì„œìš© ìš”ì•½ (ê°œì„ íŒ):
    - ë¬¸ì„œ(íŒŒì¼) ë‹¨ìœ„ ìœ ì‚¬ë„(%)ë¥¼
        * ë¬¸ì n-gram ê¸°ë°˜ ì ìˆ˜
        * ë‹¨ì–´ n-gram ê¸°ë°˜ ì ìˆ˜
      ë‘ ê°€ì§€ë¡œ ê³„ì‚°í•´ì„œ í˜¼í•©í•œë‹¤.
    - UIëŠ” ìƒìœ„ ui_top_kë§Œ ë°˜í™˜
    - ìƒì„¸ëŠ” ìƒìœ„ìŒì— ëŒ€í•´ì„œë§Œ ë¬¸ì¥ìƒ˜í”Œ ëª‡ ê°œ ë°˜í™˜
    """
    # ===== 1) ì¤€ë¹„: ë¬¸ì„œ ëª©ë¡ + ì‹œê·¸ë‹ˆì²˜(ë¬¸ì/ë‹¨ì–´) ë§Œë“¤ê¸° =====
    docs = []
    sigs_char = []
    sigs_word = []

    for i, f in enumerate(files or []):
        name = (f.get("name") or f.get("filename") or f"file_{i+1}")
        text = (f.get("text") or "")
        docs.append({"index": i, "name": name, "text": text})
        # ë¬¸ì n-gram (ê¸°ì¡´ ë°©ì‹)
        sigs_char.append(_doc_shingles(text, n=shingle_n, step=shingle_step))
        # ë‹¨ì–´ n-gram (ìƒˆë¡œ ì¶”ê°€)
        sigs_word.append(_word_ngrams_for_dup(text, n=2, step=1))

    n = len(docs)
    if n <= 1:
        return {
            "files": [],
            "bins": {},
            "top_pairs": [],
            "pair_details": [],
        }

    # ===== 2) ë¬¸ì„œìŒë³„ ì ìˆ˜ ê³„ì‚° =====
    # per-file ìµœëŒ€ ìœ ì‚¬ë„(ê° ë¬¸ì„œ ê¸°ì¤€), per-pair ìƒìœ„
    max_score = [0.0] * n
    max_with = [None] * n
    pairs = []  # (score, i, j)

    for i in range(n):
        for j in range(i + 1, n):
            s_char_i = sigs_char[i] or set()
            s_char_j = sigs_char[j] or set()
            s_word_i = sigs_word[i] or set()
            s_word_j = sigs_word[j] or set()

            # ë¬¸ì n-gram ìì¹´ë“œ
            sc_char = _jaccard_set(s_char_i, s_char_j)
            # ë‹¨ì–´ n-gram ìì¹´ë“œ
            sc_word = _jaccard_set(s_word_i, s_word_j) if (s_word_i and s_word_j) else 0.0

            # í˜¼í•© ì ìˆ˜: 0.5 * ë¬¸ì + 0.5 * ë‹¨ì–´
            sc = 0.5 * sc_char + 0.5 * sc_word

            # per-file ìµœëŒ€ê°’ ê°±ì‹  (ê° ë¬¸ì„œ ê¸°ì¤€)
            if sc > max_score[i]:
                max_score[i] = sc
                max_with[i] = j
            if sc > max_score[j]:
                max_score[j] = sc
                max_with[j] = i

            if sc >= float(ui_min_percent):
                pairs.append((sc, i, j))

    pairs.sort(key=lambda x: x[0], reverse=True)

    # ===== 3) UI ìƒë‹¨ìš© ìƒìœ„ NìŒ =====
    top_pairs = []
    top_limit = max(0, int(ui_top_k))
    for sc, i, j in pairs[:top_limit]:
        pct = int(round(sc * 100))
        top_pairs.append({
            "a": docs[i]["name"],
            "b": docs[j]["name"],
            "score": round(float(sc), 3),
            "percent": pct,
        })

    # ===== 4) ìƒì„¸ìš© ìƒìœ„ìŒ ë¬¸ì¥ í˜ì–´ =====
    pair_details = []
    if detail_top_k and top_pairs:
        for p in top_pairs:
            ai = next((d["index"] for d in docs if d["name"] == p["a"]), None)
            bi = next((d["index"] for d in docs if d["name"] == p["b"]), None)
            if ai is None or bi is None:
                continue
            details = _best_sentence_pairs(
                docs[ai]["text"],
                docs[bi]["text"],
                top_k=detail_top_k,
                n=3,
            )
            pair_details.append({
                "a": p["a"],
                "b": p["b"],
                "percent": p["percent"],
                "sentence_pairs": details,
            })

    # ===== 5) íŒŒì¼ë³„ ìš”ì•½ + êµ¬ê°„(bins) =====
    file_rows = []
    bins = {}  # "11~20": ["A (12%)", ...]

    for i, d in enumerate(docs):
        sc = max_score[i]
        pct = int(round(sc * 100))
        with_name = docs[max_with[i]]["name"] if max_with[i] is not None else None

        file_rows.append({
            "name": d["name"],
            "max_score": round(float(sc), 3),
            "max_percent": pct,
            "max_with": with_name,
        })

        # 0~10%ëŠ” ê¸°ë³¸ì ìœ¼ë¡œ í‘œì—ì„œ ì œì™¸ (ì›í•˜ë©´ UIì—ì„œ ë³„ë„ ì²˜ë¦¬)
        if pct <= 10:
            continue
        start = ((pct - 1) // 10) * 10 + 1  # 11~20, 21~30 ...
        end = start + 9
        key = f"{start}~{end}"
        bins.setdefault(key, []).append(f"{d['name']} ({pct}%)")

    # êµ¬ê°„ ì •ë ¬: ë†’ì€ êµ¬ê°„ë¶€í„°
    def _bin_key(k: str):
        try:
            a = int(k.split("~")[0])
            return -a
        except Exception:
            return 0

    bins_sorted = {}
    for k in sorted(bins.keys(), key=_bin_key):
        bins_sorted[k] = bins[k]

    return {
        "files": file_rows,
        "bins": bins_sorted,
        "top_pairs": top_pairs,
        "pair_details": pair_details,
    }

    # ê°€ì¥ ë§ì´ ê²¹ì¹˜ëŠ” ìŒë¶€í„° ì •ë ¬
    pairs.sort(key=lambda p: max(p["ratio_a"], p["ratio_b"]), reverse=True)

    return {"ok": True, "pairs": pairs}

# === [ADD] í•„ìˆ˜ê°€ì´ë“œ(ìœ ì‚¬ë„) ìœ í‹¸ ===================================
def _best_matches_for_template(text: str, template: str, threshold: float = 0.88):
    """
    ë¬¸ì¥ ë‹¨ìœ„ë¡œ í…œí”Œë¦¿ê³¼ 3-gram ìì¹´ë“œ ìœ ì‚¬ë„ ë¹„êµ.
    ë°˜í™˜: dict(present, count, best_score, best_span, matches[...])
    """
    tgrams = _char_ngrams_multi(template or "", (2,3,4))
    spans = _sentence_spans(text or "")
    matches = []
    best = None
    for (s, e, sent) in spans:
        score = _jaccard(tgrams, _char_ngrams_multi(sent, (2,3,4)))
        if score >= max(threshold - 0.06, threshold):  # ë¬¸ì¥ ìŠ¤ìº”ì€ ì‚´ì§ ëŠìŠ¨
            rec = {"start": s, "end": e, "sentence": sent, "score": round(score, 3)}
            matches.append(rec)
            if (not best) or rec["score"] > best["score"]:
                best = rec
    matches.sort(key=lambda x: -x["score"])
    return {
        "present": len(matches) > 0,
        "count": len(matches),
        "best_score": (best or {}).get("score"),
        "best_span": best,
        "matches": matches[:10]
    }

def _scan_by_rolling_window(text, template, threshold, size_lo=0.6, size_hi=1.6):
    T = template or ""
    S = text or ""
    # ìœ ì‚¬ë„ ì „ìš© ì •ê·œí™”ë¡œ ë¹„êµ(ì›ë¬¸ì€ í•˜ì´ë¼ì´íŠ¸ìš©ìœ¼ë¡œ ê·¸ëŒ€ë¡œ ìœ ì§€)
    Tn = _ko_sim_norm(T)
    Sn = _ko_sim_norm(S)
    if not Tn or not Sn:
        return []
    tpl_len = max(1, len(Tn))
    min_len = max(8, int(tpl_len * size_lo))
    max_len = max(min_len, int(tpl_len * size_hi))
    tpl_grams = _char_ngrams_multi(Tn, (2,3,4))

    hits = []
    step1 = max(4, int(tpl_len * 0.20))  # ë” ì´˜ì´˜
    step2 = max(4, int(tpl_len * 0.12))
    n = len(Sn)
    for i in range(0, n, step1):
        for L in range(min_len, min(max_len, n - i) + 1, step2):
            segN = Sn[i:i+L]
            if not segN: continue
            sc = _jaccard(tpl_grams, _char_ngrams_multi(segN, (2,3,4)))
            if sc >= threshold:
                # ì›ë¬¸ ì¢Œí‘œë¡œ ê·¼ì‚¬ ì—­ë§¤í•‘
                # (ì •ê·œí™” ì „ ì›ë¬¸ Sì—ì„œ ê°™ì€ ë²”ìœ„ë¥¼ ì‚¬ìš©)
                raw = S[i:i+L]
                hits.append({"start": i, "end": i+L, "sentence": raw, "score": round(sc, 3)})
    # ìƒìœ„ ë¹„ì¤‘ë³µ 10ê°œë§Œ ìœ ì§€
    hits.sort(key=lambda x: -x["score"])
    keep, used = [], []
    for h in hits:
        if not any(not (h["end"] <= u["start"] or h["start"] >= u["end"]) for u in used):
            keep.append(h); used.append(h)
        if len(keep) >= 10: break
    return keep

# --- [ADD] í•„ìˆ˜ê°€ì´ë“œ ë¬¸ë‹¨ í›„ë³´ íƒì§€ ìœ í‹¸ ---

_PAR_BREAK_RX = re.compile(r"\n\s*\n+")

def _paragraph_spans(text: str):
    spans = []
    if not text:
        return spans
    n = len(text)
    last = 0
    for m in _PAR_BREAK_RX.finditer(text):
        s = last
        e = m.start()
        seg = text[s:e]
        if seg.strip():
            spans.append((s, e, seg))
        last = m.end()
    if last < n:
        seg = text[last:]
        if seg.strip():
            spans.append((last, n, seg))
    return spans


def _extract_core_terms(tpl: str, max_terms: int = 5):
    from string import digits
    norm = kr_norm(tpl)
    toks = tokenize(norm)
    core = []
    for t in toks:
        if len(t) < 2:
            continue
        if all(ch in digits for ch in t):
            continue
        if t in core:
            continue
        core.append(t)
        if len(core) >= max_terms:
            break
    return core


def _guide_paragraph_candidates(
    text: str,
    templates: list[str],
    need_terms: int = 2,
    window_size: int = 80,
    step: int = 40,
):
    """
    í•„ìˆ˜ê°€ì´ë“œ í…œí”Œë¦¿ë³„ë¡œ, ì›ê³ ì—ì„œ window_size ê¸€ì ì•ˆì—
    í•µì‹¬ ë‹¨ì–´ê°€ need_termsê°œ ì´ìƒ ê°™ì´ ë“±ì¥í•˜ëŠ” êµ¬ê°„ì„ í›„ë³´ë¡œ ì¡ëŠ”ë‹¤.

    - ë¬¸ë‹¨/ë¬¸ì¥ ê²½ê³„ ë¬´ì‹œ, ê³ ì • ê¸¸ì´ ë¡¤ë§ ìœˆë„ìš° ê¸°ë°˜
    - ë°˜í™˜: {template: [ {start,end,hit_count,hit_terms,text}, ... ], ...}
    """
    text = text or ""
    n = len(text)
    result: dict[str, list[dict]] = {}
    if n == 0 or not templates:
        return result

    # ë„ˆë¬´ ì‘ì€ ê°’ ë°©ì§€
    window_size = max(40, int(window_size or 80))
    step = max(20, int(step or (window_size // 2)))

    for tpl in templates:
        core = _extract_core_terms(tpl)
        if len(core) < need_terms:
            # í•µì‹¬ ë‹¨ì–´ê°€ ë„ˆë¬´ ì ìœ¼ë©´ ìŠ¤í‚µ
            continue

        cand_map: dict[tuple[int, int], dict] = {}

        # 0 ~ ëê¹Œì§€ window_size ê¸€ì ê¸°ì¤€ìœ¼ë¡œ ìŠ¬ë¼ì´ë”©
        for start in range(0, n, step):
            end = min(start + window_size, n)
            seg = text[start:end]
            if not seg.strip():
                continue

            hit_terms = []
            for term in core:
                if re.search(spacing_agnostic_regex(term), seg, flags=re.IGNORECASE):
                    hit_terms.append(term)

            if len(hit_terms) >= need_terms:
                # ê²€ìˆ˜ìê°€ ë³´ê¸° í¸í•˜ê²Œ, ê°™ì€ ì¤„ ê¸°ì¤€ìœ¼ë¡œ ì•½ê°„ í™•ì¥
                ctx_start = text.rfind("\n", 0, start)
                if ctx_start == -1:
                    ctx_start = start
                else:
                    ctx_start += 1  # ê°œí–‰ ë°”ë¡œ ë’¤ë¶€í„°

                ctx_end = text.find("\n", end)
                if ctx_end == -1:
                    ctx_end = end

                key = (ctx_start, ctx_end)
                prev = cand_map.get(key)
                if (not prev) or (len(hit_terms) > prev["hit_count"]):
                    cand_map[key] = {
                        "start": ctx_start,
                        "end": ctx_end,
                        "hit_count": len(hit_terms),
                        "hit_terms": hit_terms,
                        "text": text[ctx_start:ctx_end],
                    }

        if cand_map:
            # hit_count ë§ì€ ìˆœ + ìœ„ì¹˜ìˆœ ì •ë ¬
            result[tpl] = sorted(
                cand_map.values(),
                key=lambda x: (-x["hit_count"], x["start"]),
            )

    return result


def _band_message(score: float, threshold: float) -> str:
    if score >= max(threshold + 0.04, 0.92):
        return "í¬í•¨ë˜ì—ˆì„ ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤."
    if score >= threshold:
        return "ìœ ì‚¬í•œ ë¬¸êµ¬ê°€ ê°ì§€ë©ë‹ˆë‹¤."
    if score >= max(0.82, threshold - 0.06):
        return "ë¶€ë¶„ì ìœ¼ë¡œ ìœ ì‚¬í•˜ì—¬ í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤."
    return "ëª…í™•í•œ ì¼ì¹˜ê°€ í™•ì¸ë˜ê¸° ì–´ë µìŠµë‹ˆë‹¤."
# ====================================================================


def _dedup_intra(text, min_len=6, sim_threshold=0.85):
    spans = _sentence_spans(text)
    # 1) exact ë„ëŒì´í‘œ(ì •ê·œí™” í›„ ë™ì¼)
    buckets = defaultdict(list)  # norm -> [(i, start, end, raw)]
    for i, (s, e, raw) in enumerate(spans):
        n = _norm_for_dup(raw)
        if len(n) >= min_len:
            buckets[n].append((i, s, e, raw))

    exact_groups = []
    for n, occ in buckets.items():
        if len(occ) >= 2:
            exact_groups.append({
                "norm": n,
                "occurrences": [
                    {"index": i, "start": s, "end": e, "original": raw} for (i,s,e,raw) in occ
                ]
            })

    # 2) similar(ìì¹´ë“œ: ë¬¸ì 3-gram)
    sims = []
    ngrams = []
    for i, (s, e, raw) in enumerate(spans):
        n = _norm_for_dup(raw)
        if len(n) >= min_len:
            ngrams.append((i, s, e, raw, _char_ngrams(raw, 3)))
    for a in range(len(ngrams)):
        i, s1, e1, r1, g1 = ngrams[a]
        for b in range(a+1, len(ngrams)):
            j, s2, e2, r2, g2 = ngrams[b]
            score = _jaccard(g1, g2)
            if score >= sim_threshold and r1 != r2:
                sims.append({
                    "i": i, "j": j, "score": round(score, 3),
                    "a": {"start": s1, "end": e1, "original": r1},
                    "b": {"start": s2, "end": e2, "original": r2}
                })

    return exact_groups, sims

def _dedup_inter(files, min_len=6, sim_threshold=0.85):
    """
    files: [{"name": str, "text": str}, ...]
    êµì°¨ íŒŒì¼ ê°„ ë™ì¼/ìœ ì‚¬ ë¬¸ì¥ íƒì§€
    """
    # ìˆ˜ì§‘
    recs = []  # (file_idx, name, sent_idx, start, end, raw, norm, ngrams)
    for fi, f in enumerate(files):
        name = f.get("name") or f"file_{fi+1}"
        text = f.get("text") or ""
        spans = _sentence_spans(text)
        for si, (s, e, raw) in enumerate(spans):
            n = _norm_for_dup(raw)
            if len(n) >= min_len:
                recs.append((fi, name, si, s, e, raw, n, _char_ngrams(raw, 3)))

    # exact
    exact_map = defaultdict(list)  # norm -> [occ...]
    for fi, name, si, s, e, raw, n, grams in recs:
        exact_map[n].append({"file": name, "fileIndex": fi, "sentIndex": si,
                             "start": s, "end": e, "original": raw})

    exact = []
    for n, occ in exact_map.items():
        # ì„œë¡œ ë‹¤ë¥¸ íŒŒì¼ì—ì„œ 2ê°œ ì´ìƒì¼ ë•Œë§Œ ì˜ë¯¸
        uniq_files = {o["fileIndex"] for o in occ}
        if len(uniq_files) >= 2:
            exact.append({"norm": n, "occurrences": occ})

    # similar
    sims = []
    for a in range(len(recs)):
        fi1, n1, si1, s1, e1, r1, norm1, g1 = recs[a]
        for b in range(a+1, len(recs)):
            fi2, n2, si2, s2, e2, r2, norm2, g2 = recs[b]
            if fi1 == fi2:  # ê°™ì€ íŒŒì¼ì€ intraì—ì„œ ë‹¤ë£¸
                continue
            score = _jaccard(g1, g2)
            if score >= sim_threshold and r1 != r2:
                sims.append({
                    "score": round(score, 3),
                    "a": {"fileIndex": fi1, "file": n1, "sentIndex": si1, "start": s1, "end": e1, "original": r1},
                    "b": {"fileIndex": fi2, "file": n2, "sentIndex": si2, "start": s2, "end": e2, "original": r2},
                })
    return exact, sims

# === [ADD] (Lite) ë‹¤ë¬¸ì„œ ì¤‘ë³µ ìš”ì•½/ë³´ê³ ì„œìš©: ë¬¸ì„œ ë‹¨ìœ„ ìœ ì‚¬ë„ ê³„ì‚° =========
def _doc_shingles(text: str, n: int = 3, step: int = 2):
    """ë¬¸ì„œ ë‹¨ìœ„ ìœ ì‚¬ë„(ê·¼ì‚¬)ë¥¼ ìœ„í•œ char n-gram ì‹œê·¸ë‹ˆì²˜.
    - stepì„ ëŠ˜ë¦¬ë©´ ì†ë„â†‘/ì •ë°€ë„â†“
    """
    if not text:
        return set()
    # ê³µë°±/êµ¬ë‘ì  ì˜í–¥ ìµœì†Œí™”
    t = _norm_for_dup(text)
    if len(t) < n:
        return set()
    out = set()
    # ë©”ëª¨ë¦¬/ì†ë„ë¥¼ ìœ„í•´ í•´ì‹œ(int)ë¡œ ì €ì¥
    for i in range(0, len(t) - n + 1, max(1, int(step))):
        out.add(hash(t[i:i+n]))
    return out


def _jaccard_set(a: set, b: set) -> float:
    if not a or not b:
        return 0.0
    inter = len(a & b)
    union = len(a) + len(b) - inter
    if union <= 0:
        return 0.0
    return inter / union


def _best_sentence_pairs(text_a: str, text_b: str, top_k: int = 3, n: int = 3):
    """ë‘ ë¬¸ì„œ ê°„ 'ê°€ì¥ ë¹„ìŠ·í•œ ë¬¸ì¥ ìŒ' ëª‡ ê°œë§Œ ë½‘ì•„ ì£¼ëŠ” ê²½ëŸ‰ ìƒì„¸ìš©."""
    spans_a = _sentence_spans(text_a or "")
    spans_b = _sentence_spans(text_b or "")
    grams_a = []
    for si, (s, e, raw) in enumerate(spans_a):
        nrm = _norm_for_dup(raw)
        if len(nrm) >= 6:
            grams_a.append((si, s, e, raw, _char_ngrams(raw, n)))
    grams_b = []
    for sj, (s, e, raw) in enumerate(spans_b):
        nrm = _norm_for_dup(raw)
        if len(nrm) >= 6:
            grams_b.append((sj, s, e, raw, _char_ngrams(raw, n)))

    best = []
    for (si, s1, e1, r1, g1) in grams_a:
        for (sj, s2, e2, r2, g2) in grams_b:
            sc = _jaccard(g1, g2)
            if sc <= 0:
                continue
            best.append((sc, si, sj, s1, e1, r1, s2, e2, r2))

    best.sort(key=lambda x: x[0], reverse=True)
    out = []
    for sc, si, sj, s1, e1, r1, s2, e2, r2 in best[: max(0, int(top_k))]:
        out.append({
            "score": round(float(sc), 3),
            "a": {"sentIndex": si, "start": s1, "end": e1, "sentence": (r1 or "")[:200]},
            "b": {"sentIndex": sj, "start": s2, "end": e2, "sentence": (r2 or "")[:200]},
        })
    return out


def _dedup_inter_lite(files,
                     ui_min_percent: float = 0.10,
                     ui_top_k: int = 10,
                     shingle_n: int = 3,
                     shingle_step: int = 2,
                     detail_top_k: int = 3):
    """UI/ë³´ê³ ì„œìš© ìš”ì•½ (ê°œì„ íŒ):
    - ë¬¸ì„œ(íŒŒì¼) ë‹¨ìœ„ ìœ ì‚¬ë„(%)ë¥¼ 'ê° ë¬¸ì„œ ê¸°ì¤€ ê²¹ì¹˜ëŠ” ë¹„ìœ¨'ë¡œ ê³„ì‚°
      * Jaccard(êµì§‘í•©/í•©ì§‘í•©)ê°€ ì•„ë‹ˆë¼
        ê° ë¬¸ì„œì— ëŒ€í•´ |Aâˆ©B| / |A|, |Aâˆ©B| / |B| ê¸°ì¤€ìœ¼ë¡œ ì¡ëŠ”ë‹¤.
    - UIëŠ” ìƒìœ„ ui_top_kë§Œ ë°˜í™˜
    - ìƒì„¸ëŠ” ìƒìœ„ìŒì— ëŒ€í•´ì„œë§Œ ë¬¸ì¥ìƒ˜í”Œ ëª‡ ê°œ ë°˜í™˜
    """
    # ì¤€ë¹„
    docs = []
    sigs = []
    for i, f in enumerate(files or []):
        name = (f.get("name") or f.get("filename") or f"file_{i+1}")
        text = (f.get("text") or "")
        docs.append({"index": i, "name": name, "text": text})
        sigs.append(_doc_shingles(text, n=shingle_n, step=shingle_step))

    n = len(docs)
    if n <= 1:
        return {
            "files": [],
            "bins": {},
            "top_pairs": [],
            "pair_details": [],
        }

    # per-file ìµœëŒ€ ìœ ì‚¬ë„(ê° ë¬¸ì„œ ê¸°ì¤€), per-pair ìƒìœ„
    max_score = [0.0] * n   # ê° ë¬¸ì„œ ê¸°ì¤€ ìµœëŒ€ ê²¹ì¹¨ ë¹„ìœ¨ (0~1)
    max_with = [None] * n
    pairs = []  # (score, i, j)  score = max(ratio_i, ratio_j)

    # ìŒë³„ ê³„ì‚°
    for i in range(n):
        si = sigs[i] or set()
        len_i = float(len(si)) or 1.0
        for j in range(i + 1, n):
            sj = sigs[j] or set()
            len_j = float(len(sj)) or 1.0

            inter = si & sj
            if not inter:
                continue

            inter_len = float(len(inter))
            # ê° ë¬¸ì„œ ê¸°ì¤€ ê²¹ì¹˜ëŠ” ë¹„ìœ¨
            ratio_i = inter_len / len_i
            ratio_j = inter_len / len_j
            pair_score = max(ratio_i, ratio_j)

            # per-file ìµœëŒ€ê°’ ê°±ì‹ 
            if ratio_i > max_score[i]:
                max_score[i] = ratio_i
                max_with[i] = j
            if ratio_j > max_score[j]:
                max_score[j] = ratio_j
                max_with[j] = i

            if pair_score >= float(ui_min_percent):
                pairs.append((pair_score, i, j))

    pairs.sort(key=lambda x: x[0], reverse=True)

    # UI: ìƒìœ„ Nê°œë§Œ
    top_pairs = []
    top_limit = max(0, int(ui_top_k))
    for score, i, j in pairs[:top_limit]:
        pct = int(round(score * 100))
        top_pairs.append({
            "a": docs[i]["name"],
            "b": docs[j]["name"],
            "score": round(float(score), 3),
            "percent": pct,
        })

    # ìƒì„¸(ìƒìœ„ìŒë§Œ, ë¬¸ì¥ 3ê°œ)
    pair_details = []
    if detail_top_k and top_pairs:
        for p in top_pairs:
            ai = next((d["index"] for d in docs if d["name"] == p["a"]), None)
            bi = next((d["index"] for d in docs if d["name"] == p["b"]), None)
            if ai is None or bi is None:
                continue
            details = _best_sentence_pairs(
                docs[ai]["text"],
                docs[bi]["text"],
                top_k=detail_top_k,
                n=3,
            )
            pair_details.append({
                "a": p["a"],
                "b": p["b"],
                "percent": p["percent"],
                "sentence_pairs": details,
            })

    # íŒŒì¼ë³„ ìš”ì•½ + êµ¬ê°„(bins)
    file_rows = []
    bins = {}  # "11~20": ["A (12%)", ...]
    for i, d in enumerate(docs):
        sc = max_score[i]
        pct = int(round(sc * 100))
        with_name = docs[max_with[i]]["name"] if max_with[i] is not None else None

        file_rows.append({
            "name": d["name"],
            "max_score": round(float(sc), 3),
            "max_percent": pct,
            "max_with": with_name,
        })

        # 0~10ì€ ê¸°ë³¸ì ìœ¼ë¡œ ì œì™¸(ì›í•˜ë©´ UIì—ì„œ í‘œì‹œ)
        if pct <= 10:
            continue
        start = ((pct - 1) // 10) * 10 + 1  # 11~20, 21~30 ...
        end = start + 9
        key = f"{start}~{end}"
        bins.setdefault(key, []).append(f"{d['name']} ({pct}%)")

    # bins ì •ë ¬: ë†’ì€ êµ¬ê°„ë¶€í„°
    def _bin_key(k: str):
        try:
            a = int(k.split("~")[0])
            return -a
        except Exception:
            return 0

    bins_sorted = {}
    for k in sorted(bins.keys(), key=_bin_key):
        bins_sorted[k] = bins[k]

    return {
        "files": file_rows,
        "bins": bins_sorted,
        "top_pairs": top_pairs,
        "pair_details": pair_details,
    }
# ======================================================================

# === [ADD] í•„ìˆ˜ë‚´ìš©(í…œí”Œë¦¿) ìœ ì‚¬ë„: ì¤‘ë³µì—”ì§„(_dedup_intra) ì¬ì‚¬ìš© =========

def _guide_match_by_dedup_engine(text: str, templates: list[str],
                                 min_len: int = 6, sim_threshold: float = 0.85):
    """
    templates: í•„ìˆ˜ë‚´ìš© ë¼ì¸ë“¤(ë¹ˆ ì¤„/ì£¼ì„ ì œì™¸)
    - ë¬¸ì¥ ë‹¨ìœ„ + 3-gram ìì¹´ë“œ(ì¤‘ë³µì—”ì§„ê³¼ ë™ì¼)ë¡œ ìŠ¤ìº”
    - ê²°ê³¼ëŠ” í…œí”Œë¦¿ë³„ best hitë§Œ ë°˜í™˜
    """
    spans = _sentence_spans(text or "")
    # í…ìŠ¤íŠ¸ ìª½ ngram ë¯¸ë¦¬ ê³„ì‚° (ì†ë„)
    sent_grams = []
    for (s, e, raw) in spans:
        n = _norm_for_dup(raw)
        if len(n) >= min_len:
            sent_grams.append((s, e, raw, _char_ngrams(raw, 3)))

    out = []
    for tpl in (templates or []):
        tpl = (tpl or "").strip()
        if not tpl: 
            continue
        tgrams = _char_ngrams(tpl, 3)  # ì¤‘ë³µì—”ì§„ê³¼ ë™ì¼ ê¸°ì¤€
        best = None
        for (s, e, raw, g) in sent_grams:
            sc = _jaccard(tgrams, g)
            if sc >= sim_threshold:
                rec = {"start": s, "end": e, "sentence": raw, "score": round(sc, 3)}
                if (not best) or rec["score"] > best["score"]:
                    best = rec
        out.append({
            "template": tpl,
            "present": bool(best),
            "best": best,
        })
    return out
# ==========================================================================


# === [ADD] token-level cosine (unigram+bigram) =========================
_WORD_RE = re.compile(r"[ê°€-í£a-z0-9]+", re.IGNORECASE)
_STOP = {"ì€","ëŠ”","ì´","ê°€","ì„","ë¥¼","ì—","ì˜","ë„","ì™€","ê³¼","ë°","ìœ¼ë¡œ","ì—ì„œ","ë¶€í„°","ê¹Œì§€","í•˜ê³ ","ê·¸ë¦¬ê³ ","ë˜ëŠ”","ìˆ˜","ê²ƒ","ë“±","ì…ë‹ˆë‹¤","í•©ë‹ˆë‹¤","ìˆìŠµë‹ˆë‹¤"}

def _ko_word_norm(s: str) -> list[str]:
    """ê°€ë²¼ìš´ êµ­ë¬¸ í† í° ì •ê·œí™”(ì†Œë¬¸ì, ë¶ˆìš©ì–´ ì œê±°, í”í•œ ì–´ë¯¸ ì†Œê±°)"""
    if not s: return []
    t = kr_norm(s).lower()
    toks = _WORD_RE.findall(t)
    out = []
    for w in toks:
        if w in _STOP: 
            continue
        # ë¼ì´íŠ¸ ìŠ¤í…Œë° (ê³¼í•˜ì§€ ì•Šê²Œ ìì£¼ ë‚˜ì˜¤ëŠ” ì–´ë¯¸ë§Œ)
        for suf in ("ìŠµë‹ˆë‹¤","í•©ë‹ˆë‹¤","ì˜€ë‹¤","ì´ë‹¤","í–ˆë‹¤","ìˆëŠ”","ë°›ì„","í•˜ëŠ”","í•´ì•¼","í•´ì„œ"):
            if w.endswith(suf) and len(w) > len(suf)+1:
                w = w[:-len(suf)]
                break
        if len(w) >= 2:
            out.append(w)
    return out

def _bow_vec(tokens: list[str]) -> dict[str, float]:
    """unigram + bigram count ë²¡í„°"""
    v: dict[str, float] = {}
    for i, w in enumerate(tokens):
        v[w] = v.get(w, 0.0) + 1.0
        if i+1 < len(tokens):
            bg = w + " " + tokens[i+1]
            v[bg] = v.get(bg, 0.0) + 1.0
    return v

def _cosine(a: dict[str, float], b: dict[str, float]) -> float:
    if not a or not b: 
        return 0.0
    dot = 0.0
    for k, va in a.items():
        vb = b.get(k)
        if vb: dot += va * vb
    na = math.sqrt(sum(x*x for x in a.values()))
    nb = math.sqrt(sum(x*x for x in b.values()))
    if na == 0.0 or nb == 0.0:
        return 0.0
    return dot / (na * nb)
# ======================================================================


# ==================== Flask ====================
from flask import Flask, request, jsonify
from flask_cors import CORS
import re

app = Flask(__name__)

# HEADëŠ” ë¼ìš°íŒ… ë§¤ì¹­ì„ í†µê³¼í•´ì•¼ í•˜ë¯€ë¡œ ìºì¹˜ì˜¬ ë¼ìš°íŠ¸ê°€ í•„ìš”
@app.route("/", defaults={"path": ""}, methods=["HEAD"])
@app.route("/<path:path>", methods=["HEAD"])
def __head_ok(path):
    return ("", 200, {"Content-Type": "text/plain; charset=utf-8"})

@app.route("/healthz", methods=["GET", "HEAD"])
def __healthz():
    if request.method == "HEAD":
        return ("", 200, {"Content-Type": "text/plain; charset=utf-8"})
    return jsonify({"ok": True})


# 1) HEAD í—¬ìŠ¤ì²´í¬/í”„ë¡ì‹œ ì•ˆì „ ê°€ë“œ (ëª¨ë“  ê²½ë¡œ ê³µí†µ)
@app.before_request
def _head_guard():
    if request.method == "HEAD":
        return ("", 200, {"Content-Type": "text/plain; charset=utf-8"})

# 2) CORS â€“ ì¼ë‹¨ ì „ë¶€ í—ˆìš©í•´ì„œ CORS ì—ëŸ¬ ì œê±°
CORS(
    app,
    resources={r"/*": {"origins": "*"}},
    supports_credentials=True,
)

# 3) ë£¨íŠ¸ & í—¬ìŠ¤ â€” ë‹¨ì¼ ì •ì˜ë§Œ ìœ ì§€
@app.route("/", methods=["GET", "HEAD"])
def root():
    # HEADëŠ” ìœ„ before_requestì—ì„œ ì´ë¯¸ 200 ì²˜ë¦¬ë¨ â†’ ì—¬ê¸°ì„œëŠ” GETë§Œ ë„ë‹¬
    return jsonify({"ok": True, "service": "glefit-server"}), 200

@app.route("/health", methods=["GET", "HEAD"])
def health():
    if request.method == "HEAD":
        return ("", 200, {"Content-Type": "text/plain; charset=utf-8"})
    return jsonify({"ok": True, "routes": [
        "/","/health",
        "/auth/login","/auth/ping","/auth/me",
        "/admin/issue_user","/admin/set_active","/admin/reset_password",
        "/admin/list_users","/admin/delete_user",
        "/verify","/policy_verify","/dedup_intra","/dedup_inter","/spell/local","/ai_local_detect","/ai_local_detect_v2",
        "/guide_forbid_check","/guide_keyword_count","/guide_verify_local","/guide_verify_dedup"
    ]})

@app.post("/guide_forbid_check")
@require_user
def guide_forbid_check():
    """
    body: {
      "text": str,                    # ì›ê³ 
      "guide_lines": [str] | str      # í•„ìˆ˜ê°€ì´ë“œ ì…ë ¥ë€(ë¼ì¸ ë°°ì—´ ë˜ëŠ” ê°œí–‰ë¬¸ì í¬í•¨ ë¬¸ìì—´)
    }
    - ì˜ˆ) guide_lines ì•ˆì— "ê¸ˆì§€ì–´: ë¹„ë§Œì¹˜ë£Œ, ì „ì•¡ë³´ì¥" ë˜ëŠ” "ê¸ˆì§€ì–´- ê³ íš¨ëŠ¥ ì´ˆíŠ¹ê°€" ë“±
    """
    try:
        data = request.get_json(force=True, silent=True) or {}
        text = (data.get("text") or "")
        lines = data.get("guide_lines")
        if isinstance(lines, str):
            guide_lines = [ln.strip() for ln in lines.splitlines()]
        else:
            guide_lines = [str(x or "").strip() for x in (lines or [])]

        # ì¼ë°˜/ì²´í—˜ 100KB ê°€ë“œ
        limit = enforce_size_limit_or_400(text)
        if limit: return limit

        forbid_terms = _parse_forbid_lines(guide_lines)
        hits = _find_forbidden_hits(text, forbid_terms)

        # ì™„ê³¡ ë©˜íŠ¸
        message = _hedged_message("absent")
        if hits:
            message = _hedged_message("forbid")

        # í•˜ì´ë¼ì´íŠ¸/íŒ¨ë„ë¡œ ë°”ë¡œ ì“¸ ìˆ˜ ìˆê²Œ ê°€ê³µ
        items = []
        gid = 0
        for h in hits[:200]:
            items.append({
                "id": f"forbid_{gid}",
                "type": "ê¸ˆì§€ì–´",
                "original": h["seg"],
                "reason": f"[ê¸ˆì§€ì–´] '{h['term']}'",
                "severity": "high",
                "startIndex": h["start"],
                "endIndex": h["end"],
                "suggestions": ["ë¬¸êµ¬ ì™„í™” ë˜ëŠ” ì‚­ì œ ê²€í† "]
            })
            gid += 1

        return jsonify({
            "ok": True,
            "message": message,           # ë‹¨ì • ëŒ€ì‹  ì™„ê³¡
            "terms": forbid_terms,
            "count": len(hits),
            "items": items
        })
    except Exception as e:
        return jsonify({"ok": False, "error": str(e)}), 500

@app.route("/auth/login", methods=["POST"])
def auth_login():
    payload = request.get_json(force=True, silent=True) or {}

    # ğŸ”¹ usernameì„ normalizeí•´ì„œ ëŒ€ì†Œë¬¸ìÂ·ê³µë°± ë¬¸ì œ ì œê±°
    raw_username = payload.get("username") or ""
    username = normalize_username(raw_username)
    password = payload.get("password") or ""

    u = _get_user(username)

    # ì•„ì´ë”” or ë¹„ë°€ë²ˆí˜¸ í‹€ë¦¼
    if not u or not bcrypt.verify(password, u["password_hash"]):
        return jsonify({"error": "Bad credentials"}), 401

    # ğŸ”¹ ê´€ë¦¬ì(admin)ëŠ” ê²°ì œ/ê¸°ê°„ ì²´í¬ ì—†ì´ í•­ìƒ ë¡œê·¸ì¸ í—ˆìš©
    if u.get("role") != "admin" and not _is_paid_and_active(u):
        return jsonify({"error": "Payment required", "code": "PAYMENT"}), 402

    # (ì´ ì•„ë˜ëŠ” ê¸°ì¡´ ê·¸ëŒ€ë¡œ)
    conn = sqlite3.connect(DB_PATH)
    cur = conn.cursor()
    cur.execute("SELECT token_version, allow_concurrent FROM users WHERE username=?", (username,))
    row = cur.fetchone()
    cur_ver = int((row[0] if row else 0) or 0)
    allow_concurrent = bool(row[1]) if row and row[1] is not None else False

    if allow_concurrent:
        new_ver = cur_ver
        new_jti = ""
    else:
        new_ver = cur_ver + 1
        new_jti = str(uuid.uuid4())
        cur.execute(
            "UPDATE users SET token_version=?, last_jti=? WHERE username=?",
            (new_ver, new_jti, username)
        )
        conn.commit()
    conn.close()

    token = jwt.encode({
        "sub": u["username"],
        "role": u["role"],
        "ver": new_ver,
        "jti": new_jti,
        "exp": datetime.utcnow() + timedelta(hours=12)
    }, JWT_SECRET, algorithm=JWT_ALG)

    log_usage(username, "login", 0)

    return jsonify({"access_token": token, "token_type": "bearer"})

        # (ë‹¨ì¼ ë¡œê·¸ì¸ìš© ë²„ì „/í† í°ID ì—…ë°ì´íŠ¸) â€” ë™ì‹œì ‘ì† í—ˆìš© ê³„ì •ì€ ì˜ˆì™¸
    conn = sqlite3.connect(DB_PATH)
    cur = conn.cursor()
    cur.execute("SELECT token_version, allow_concurrent FROM users WHERE username=?", (username,))
    row = cur.fetchone()
    cur_ver = int((row[0] if row else 0) or 0)
    allow_concurrent = bool(row[1]) if row and row[1] is not None else False

    if allow_concurrent:
        new_ver = cur_ver
        new_jti = ""
    else:
        new_ver = cur_ver + 1
        new_jti = str(uuid.uuid4())
        cur.execute(
            "UPDATE users SET token_version=?, last_jti=? WHERE username=?",
            (new_ver, new_jti, username)
        )
        conn.commit()
    conn.close()

    token = jwt.encode({
        "sub": u["username"],
        "role": u["role"],
        "ver": new_ver,     # í† í° ë²„ì „
        "jti": new_jti,     # í† í° ê³ ìœ ID
        "exp": datetime.utcnow() + timedelta(hours=12)
    }, JWT_SECRET, algorithm=JWT_ALG)

    log_usage(username, "login", 0)

    return jsonify({"access_token": token, "token_type": "bearer"})

@app.route("/auth/ping", methods=["GET", "OPTIONS"])
@require_user
def auth_ping():
    if request.method == "OPTIONS":
        return "", 200
    return jsonify({"ok": True})

@app.route("/auth/agree_refund", methods=["POST"])
@require_user
def auth_agree_refund():
    username = _username_from_req()
    conn = sqlite3.connect(DB_PATH); cur = conn.cursor()
    # ë™ì˜ ì‹œê°ì„ í•œêµ­ ì‹œê°„(KST, UTC+9) ê¸°ì¤€ìœ¼ë¡œ ì €ì¥
    now_kst = datetime.utcnow() + timedelta(hours=9)
    cur.execute("INSERT OR REPLACE INTO agreements (username, agreed_at) VALUES (?,?)",
                (username, now_kst.isoformat()))
    conn.commit(); conn.close()
    return jsonify({"ok": True})

@app.post("/track/visit")
def track_visit():
    try:
        body = request.get_json(force=True, silent=True) or {}
        path = (body.get("path") or "/").strip()
        # í† í°ì´ ìˆìœ¼ë©´ ìœ ì €ëª… ì¶”ì¶œ, ì—†ìœ¼ë©´ ê³µë°± ì²˜ë¦¬
        username = _username_from_req()
        log_visit(username, path)
        return jsonify({"ok": True})
    except Exception:
        return jsonify({"ok": False}), 200

@app.route("/auth/me", methods=["GET", "OPTIONS"])
@require_user
def auth_me():
    if request.method == "OPTIONS":
        return "", 200
    auth = request.headers.get("Authorization","")
    token = auth.split(" ",1)[1]
    data = jwt.decode(token, JWT_SECRET, algorithms=[JWT_ALG])
    u = _get_user(data.get("sub",""))
    return jsonify({
        "username": u["username"],
        "role": u["role"],
        "is_active": u["is_active"],
        "paid_until": u["paid_until"],
        "remaining_days": _remaining_days(u["paid_until"]),
    })

@app.route("/admin/create_user", methods=["POST"])
@require_admin
def admin_create_user():
    body = request.get_json(force=True, silent=True) or {}
    username = normalize_username(body.get("username") or "")
    password = body.get("password") or ""
    days = int(body.get("days") or 0)

    try:
        validate_username(username)
    except ValueError as e:
        return jsonify({"error": str(e)}), 400
    if not password:
        return jsonify({"error":"username/password required"}), 400

    conn = sqlite3.connect(DB_PATH)
    cur = conn.cursor()
    try:
        # í•œêµ­ ì‹œê°„(KST, UTC+9) ê¸°ì¤€ìœ¼ë¡œ ì´ìš© ê¸°ê°„ ê³„ì‚°
        now_kst = datetime.utcnow() + timedelta(hours=9)
        paid_until = now_kst + timedelta(days=days) if days>0 else now_kst
        cur.execute(
            "INSERT INTO users (username, password_hash, is_active, paid_until, role) VALUES (?,?,?,?,?)",
            (username, bcrypt.hash(password), 1 if days>0 else 0, paid_until.isoformat(), "user")
        )
        conn.commit()
        return jsonify({"ok":True})
    except sqlite3.IntegrityError:
        return jsonify({"error":"username exists"}), 409
    finally:
        conn.close()

@app.route("/admin/approve", methods=["POST"])
@require_admin
def admin_approve():
    body = request.get_json(force=True, silent=True) or {}
    username = (body.get("username") or "").strip()
    extend_days = int(body.get("extend_days") or 0)
    note = (body.get("note") or "").strip()

    u = _get_user(username)
    if not u:
        return jsonify({"error":"user not found"}), 404

    # í•œêµ­ ì‹œê°„(KST, UTC+9) ê¸°ì¤€ìœ¼ë¡œ ì—°ì¥ ê¸°ì¤€ì¼ ì„¤ì •
    base = datetime.utcnow() + timedelta(hours=9)
    try:
        if u.get("paid_until"):
            base = max(base, datetime.fromisoformat(u["paid_until"]))
    except Exception:
        pass

    new_until = base + timedelta(days=max(1, extend_days))

    conn = sqlite3.connect(DB_PATH)
    cur = conn.cursor()
    cur.execute(
        "UPDATE users SET is_active=?, paid_until=?, notes=? WHERE username=?",
        (1, new_until.isoformat(), note, username)
    )
    conn.commit()
    conn.close()
    return jsonify({"ok":True, "paid_until": new_until.isoformat()})

@app.route("/admin/issue_user", methods=["POST"])
@require_admin
def admin_issue_user():
    """
    ê´€ë¦¬ì ì „ìš©: ì‹ ê·œ ë°œê¸‰ ë˜ëŠ” ê¸°ì¡´ ì—°ì¥/ì •ë³´ ì—…ë°ì´íŠ¸ë¥¼ í•œ ë²ˆì— ì²˜ë¦¬
    body: { username, password?, days=32, site_url?, role? }
    ì‹ ê·œ: password í•„ìˆ˜, ê¸°ì¡´: ìƒëµ ê°€ëŠ¥
    """
    body = request.get_json(force=True, silent=True) or {}
    username = normalize_username(body.get("username") or "")
    password = (body.get("password") or "").strip()
    days     = int(body.get("days") or 32)   # ê¸°ë³¸ 32ì¼
    site_url = (body.get("site_url") or "").strip()
    role     = (body.get("role") or "user").strip()
    note     = (body.get("note") or "").strip()

    try:
        validate_username(username)
    except ValueError as e:
        return jsonify({"error": str(e)}), 400
    if not username:
        return jsonify({"error":"username required"}), 400

    conn = sqlite3.connect(DB_PATH); cur = conn.cursor()
    cur.execute("SELECT username, paid_until FROM users WHERE username=?", (username,))
    row = cur.fetchone()

    base = datetime.utcnow()
    if row and row[1]:
        try:
            prev = datetime.fromisoformat(row[1])
            if prev > base:
                base = prev
        except Exception:
            pass
    new_until = base + timedelta(days=max(1, days))

    if row is None:
        if not password:
            return jsonify({"error":"password required for new user"}), 400
        cur.execute(
            "INSERT INTO users (username, password_hash, is_active, paid_until, role, notes, site_url, created_at) VALUES (?,?,?,?,?,?,?,?)",
            (username, bcrypt.hash(password), 1, new_until.isoformat(), role, note, (site_url or None), datetime.utcnow().isoformat())
        )
    else:
        # ê¸°ì¡´ ì‚¬ìš©ì: ì—°ì¥ + ì„ íƒ í•„ë“œ ê°±ì‹ 
        if password:
            cur.execute("UPDATE users SET password_hash=? WHERE username=?", (bcrypt.hash(password), username))
        cur.execute(
            "UPDATE users SET is_active=?, paid_until=?, role=?, notes=?, site_url=? WHERE username=?",
            (1, new_until.isoformat(), role, note, (site_url or None), username)
        )

    conn.commit(); conn.close()
    return jsonify({"ok": True, "username": username, "paid_until": new_until.isoformat(), "remaining_days": _remaining_days(new_until.isoformat())})

@app.route("/admin/set_active", methods=["POST"])
@require_admin
def admin_set_active():
    body = request.get_json(force=True, silent=True) or {}
    username = (body.get("username") or "").strip()
    is_active = 1 if body.get("is_active") else 0
    if not username:
        return jsonify({"error":"username required"}), 400
    conn = sqlite3.connect(DB_PATH); cur = conn.cursor()
    cur.execute("UPDATE users SET is_active=? WHERE username=?", (is_active, username))
    conn.commit(); conn.close()
    return jsonify({"ok": True})

@app.route("/admin/set_allow_concurrent", methods=["POST"])
@require_admin
def admin_set_allow_concurrent():
    body = request.get_json(force=True, silent=True) or {}
    username = (body.get("username") or "").strip()
    allow = 1 if body.get("allow") else 0
    if not username:
        return jsonify({"error": "username required"}), 400

    conn = sqlite3.connect(DB_PATH); cur = conn.cursor()
    if allow:
        # í—ˆìš©: ê·¸ëŒ€ë¡œ í”Œë˜ê·¸ë§Œ ì¼œê¸°
        cur.execute(
            "UPDATE users SET allow_concurrent=? WHERE username=?",
            (allow, username)
        )
    else:
        # í•´ì œ: ê¸°ì¡´ ì„¸ì…˜ ë¬´íš¨í™”ë¥¼ ìœ„í•´ ë²„ì „++ ë° ìƒˆë¡œìš´ jti ì„¸íŒ…
        import uuid
        new_jti = str(uuid.uuid4())
        cur.execute(
            "UPDATE users SET allow_concurrent=?, token_version=token_version+1, last_jti=? WHERE username=?",
            (allow, new_jti, username)
        )
    conn.commit(); conn.close()
    return jsonify({"ok": True, "username": username, "allow_concurrent": bool(allow)})

@app.route("/admin/reset_password", methods=["POST"])
@require_admin
def admin_reset_password():
    body = request.get_json(force=True, silent=True) or {}
    username = (body.get("username") or "").strip()
    new_pw = (body.get("new_password") or "").strip()
    if not username or not new_pw:
        return jsonify({"error":"username/new_password required"}), 400
    conn = sqlite3.connect(DB_PATH); cur = conn.cursor()
    cur.execute("UPDATE users SET password_hash=? WHERE username=?", (bcrypt.hash(new_pw), username))
    conn.commit(); conn.close()
    return jsonify({"ok": True})

# --- ê´€ë¦¬ì: ì‚¬ìš©ì ì‚­ì œ(ìê¸° ìì‹ /ìµœìƒìœ„ admin ë³´í˜¸, ì¡´ì¬ì—¬ë¶€ ì²´í¬) ---
@app.route("/admin/delete_user", methods=["POST"])
@require_admin
def admin_delete_user():
    body = request.get_json(silent=True) or {}
    username = (body.get("username") or "").strip()

    if not username:
        return jsonify({"error": "username required"}), 400
    # ìµœìƒìœ„ ê´€ë¦¬ì ë³´í˜¸ (í•„ìš” ì—†ë‹¤ë©´ ì£¼ì„ ì²˜ë¦¬)
    if username == "admin":
        return jsonify({"error": "cannot delete admin"}), 400

    # ë³¸ì¸ ê³„ì • ì‚­ì œ ë°©ì§€
    try:
        auth = request.headers.get("Authorization", "")
        token = auth.split(" ", 1)[1]
        sub = jwt.decode(token, JWT_SECRET, algorithms=[JWT_ALG]).get("sub")
        if sub == username:
            return jsonify({"error": "ë³¸ì¸ ê³„ì •ì€ ì‚­ì œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤"}), 400
    except Exception:
        # í† í° íŒŒì‹± ì‹¤íŒ¨ ì‹œ ìê¸°ì‚­ì œ ë°©ì§€ëŠ” ê±´ë„ˆë›°ë˜, admin ë³´í˜¸ëŠ” ìœ„ì—ì„œ ì´ë¯¸ ì ìš©ë¨
        pass

    # ì‹¤ì œ ì‚­ì œ
    conn = sqlite3.connect(DB_PATH)
    cur = conn.cursor()
    cur.execute("DELETE FROM users WHERE username = ?", (username,))
    conn.commit()
    deleted = cur.rowcount
    conn.close()

    if deleted == 0:
        return jsonify({"error": "ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ì‚¬ìš©ì"}), 404

    return jsonify({"ok": True, "deleted": username})

@app.route("/admin/list_users", methods=["GET"])
@require_admin
def admin_list_users():
    q = (request.args.get("q") or "").strip()
    conn = sqlite3.connect(DB_PATH); cur = conn.cursor()
    if q:
        like = f"%{q}%"
        cur.execute(
            """
            SELECT username,is_active,paid_until,role,notes,site_url,created_at,allow_concurrent
            FROM users
            WHERE username LIKE ? OR IFNULL(notes,'') LIKE ? OR IFNULL(site_url,'') LIKE ?
            ORDER BY
              CASE WHEN paid_until IS NULL THEN 1 ELSE 0 END,   -- ë§Œë£Œì¼ ì—†ëŠ” ê³„ì •ì€ ë’¤ë¡œ
              datetime(paid_until) ASC,                         -- ë§Œë£Œ ì„ë°•ìˆœ
              username ASC
            """,
            (like, like, like)
        )
    else:
        cur.execute(
            """
            SELECT username,is_active,paid_until,role,notes,site_url,created_at,allow_concurrent
            FROM users
            ORDER BY
              CASE WHEN paid_until IS NULL THEN 1 ELSE 0 END,
              datetime(paid_until) ASC,
              username ASC
            """
        )
    rows = cur.fetchall()
    conn.close()

    out = []
    for u,a,pu,r,nt,site,created,ac in rows:
        out.append({
            "username": u,
            "is_active": bool(a),
            "paid_until": pu,
            "remaining_days": _remaining_days(pu),
            "role": r,
            "note": nt,
            "site_url": site,
            "created_at": created,
            "allow_concurrent": bool(ac),
        })
    return jsonify({"users": out})

# === [ADD] ë¶€ë¶„ì¼ì¹˜ ì‚¬ìš©ì ê²€ìƒ‰ (ê²Œì‹œê¸€ì´ ì—†ì–´ë„ ê²€ìƒ‰) ===
@app.get("/admin/board_user_search")
@require_admin
def admin_board_user_search():
    q = (request.args.get("q") or "").strip()
    if not q:
        return jsonify({"ok": True, "items": []})

    # LIKE ì•ˆì „ ì´ìŠ¤ì¼€ì´í”„
    esc = q.replace("\\", "\\\\").replace("%", "\\%").replace("_", "\\_")
    like = f"%{esc}%"

    conn = sqlite3.connect(DB_PATH); cur = conn.cursor()
    cur.execute("""
        WITH users_union AS (
          SELECT DISTINCT username FROM users
          UNION
          SELECT DISTINCT username FROM board_posts
        )
        SELECT uu.username,
               COALESCE((SELECT daily_limit
                         FROM board_limits bl
                         WHERE bl.username = uu.username), 2) AS daily_limit
        FROM users_union uu
        WHERE uu.username LIKE ? ESCAPE '\\'
        ORDER BY uu.username
        LIMIT 50
    """, (like,))
    rows = cur.fetchall()
    conn.close()

    items = [{"username": r[0], "blocked": (int(r[1] or 2) <= 0), "daily_limit": int(r[1] or 2)} for r in rows]
    return jsonify({"ok": True, "items": items})

# === board posts: ìµœëŒ€ 200ê°œ ìœ ì§€ ===
MAX_BOARD_POSTS = 200

def trim_board_to_max():
    """
    hidden=0(ë…¸ì¶œ ëŒ€ìƒ) ê²Œì‹œê¸€ì„ MAX_BOARD_POSTS ê°œê¹Œì§€ë§Œ ìœ ì§€.
    1) ë¹„ê³ ì •(pinned=0)ì—ì„œ ì˜¤ë˜ëœ ìˆœìœ¼ë¡œ ë¨¼ì € ì‚­ì œ
    2) ê·¸ë˜ë„ ì´ˆê³¼ë©´ ì „ì²´ì—ì„œ ì˜¤ë˜ëœ ìˆœìœ¼ë¡œ ì¶”ê°€ ì‚­ì œ
    """
    conn = sqlite3.connect(DB_PATH); cur = conn.cursor()

    # í˜„ì¬ ë…¸ì¶œ ê°œìˆ˜
    cur.execute("SELECT COUNT(*) FROM board_posts WHERE hidden=0")
    total = int(cur.fetchone()[0] or 0)
    excess = total - MAX_BOARD_POSTS
    if excess > 0:
        # 1) ë¹„ê³ ì • ë¨¼ì € ì •ë¦¬
        cur.execute("""
            SELECT id FROM board_posts
            WHERE hidden=0 AND pinned=0
            ORDER BY ts ASC
            LIMIT ?
        """, (excess,))
        ids = [r[0] for r in cur.fetchall()]
        if ids:
            cur.executemany("DELETE FROM board_posts WHERE id=?", [(i,) for i in ids])
            conn.commit()

        # ë‹¤ì‹œ ê³„ì‚°
        cur.execute("SELECT COUNT(*) FROM board_posts WHERE hidden=0")
        total = int(cur.fetchone()[0] or 0)
        excess = total - MAX_BOARD_POSTS

        # 2) ê·¸ë˜ë„ ë‚¨ìœ¼ë©´ ì „ì²´ì—ì„œ ì˜¤ë˜ëœ ìˆœ ì‚­ì œ
        if excess > 0:
            cur.execute("""
                SELECT id FROM board_posts
                WHERE hidden=0
                ORDER BY ts ASC
                LIMIT ?
            """, (excess,))
            ids2 = [r[0] for r in cur.fetchall()]
            if ids2:
                cur.executemany("DELETE FROM board_posts WHERE id=?", [(i,) for i in ids2])
                conn.commit()

    conn.close()

# ===== ê´€ë¦¬ì: í•œ ì¤„ í™ë³´ ê²Œì‹œíŒ =====
@app.get("/admin/board_list")
@require_admin
def admin_board_list():
    args = request.args
    username = (args.get("username") or "").strip()
    keyword  = (args.get("q") or "").strip()
    pinned_only = args.get("pinned_only") in ("1","true","True")
    include_hidden = args.get("include_hidden") in ("1","true","True")

    conn = sqlite3.connect(DB_PATH); conn.row_factory = sqlite3.Row
    cur = conn.cursor()

    where = []
    params = []

    if username:
        where.append("p.username = ?")
        params.append(username)
    if keyword:
        where.append("p.text LIKE ?")
        params.append(f"%{keyword}%")
    if not include_hidden:
        where.append("p.hidden = 0")
    if pinned_only:
        where.append("p.pinned = 1")

    where_sql = (" WHERE " + " AND ".join(where)) if where else ""
    sql = f"""
    SELECT p.id, p.username, p.text, p.pinned, p.ts, p.hidden,
           COALESCE(u.posting_blocked,0) AS user_blocked
      FROM board_posts p
      LEFT JOIN users u ON u.username = p.username
      {where_sql}
      ORDER BY p.pinned DESC, p.ts DESC
      LIMIT ?
    """
    cur.execute(sql, params + [MAX_BOARD_POSTS])
    rows = cur.fetchall()
    conn.close()

    def to_iso(ts):
        try:
            return datetime.utcfromtimestamp(int(ts)).isoformat()+"Z"
        except Exception:
            return None

    posts = []
    for r in rows:
        posts.append({
            "id": r["id"],
            "username": r["username"],
            "content": r["text"],
            "pinned": bool(r["pinned"]),
            "created_at": to_iso(r["ts"]),
            "user_blocked": bool(r["user_blocked"]),
            "hidden": int(r["hidden"] or 0),
        })
    return jsonify({"posts": posts})

@app.post("/admin/board_update")
@require_admin
def admin_board_update():
    body = request.get_json(force=True, silent=True) or {}
    pid = (body.get("id") or "").strip()
    content = (body.get("content") or "").strip()
    if not pid: return jsonify({"error":"id required"}), 400
    if not content: return jsonify({"error":"content required"}), 400

    conn = sqlite3.connect(DB_PATH); cur = conn.cursor()
    cur.execute("UPDATE board_posts SET text=? WHERE id=?", (content, pid))
    conn.commit(); conn.close()
    return jsonify({"ok": True})

@app.post("/admin/board_delete")
@require_admin
def admin_board_delete():
    body = request.get_json(force=True, silent=True) or {}
    pid = (body.get("id") or "").strip()
    if not pid: return jsonify({"error":"id required"}), 400

    conn = sqlite3.connect(DB_PATH); cur = conn.cursor()
    # ì†Œí”„íŠ¸ ì‚­ì œ: hidden=1
    cur.execute("UPDATE board_posts SET hidden=1 WHERE id=?", (pid,))
    conn.commit(); conn.close()
    return jsonify({"ok": True})

@app.post("/admin/board_pin")
@require_admin
def admin_board_pin():
    body = request.get_json(force=True, silent=True) or {}
    pid = (body.get("id") or "").strip()
    pinned = 1 if body.get("pinned") in (True, "1", "true", "True") else 0
    if not pid: return jsonify({"error":"id required"}), 400

    conn = sqlite3.connect(DB_PATH); cur = conn.cursor()
    cur.execute("UPDATE board_posts SET pinned=? WHERE id=?", (pinned, pid))
    conn.commit(); conn.close()
    return jsonify({"ok": True})

@app.post("/admin/board_block_user")
@require_admin
def admin_board_block_user():
    body = request.get_json(force=True, silent=True) or {}
    username = (body.get("username") or "").strip()
    blocked = 1 if body.get("blocked") in (True, "1", "true", "True") else 0
    if not username: return jsonify({"error":"username required"}), 400

    conn = sqlite3.connect(DB_PATH); cur = conn.cursor()
    cur.execute("UPDATE users SET posting_blocked=? WHERE username=?", (blocked, username))
    conn.commit(); conn.close()
    return jsonify({"ok": True})

@app.route("/board/create", methods=["POST"])
@require_user
def create_board_post():
    # í˜„ì¬ íŒŒì¼ì—ëŠ” g.userë¥¼ ì„¸íŒ…í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ í† í°ì—ì„œ ì¶”ì¶œ
    username = _username_from_req()
    if not username:
        return jsonify({"error": "Unauthorized"}), 401

    conn = sqlite3.connect(DB_PATH); cur = conn.cursor()

    # ì‘ì„±ì •ì§€ ì—¬ë¶€ í™•ì¸
    r = cur.execute(
        "SELECT IFNULL(posting_blocked,0) FROM users WHERE username=?",
        (username,)
    ).fetchone()
    if r and int(r[0]) == 1:
        conn.close()
        return jsonify({"error": "ì‘ì„±ì •ì§€ ì‚¬ìš©ìì…ë‹ˆë‹¤"}), 403

    payload = request.get_json(force=True, silent=True) or {}
    content = (payload.get("content") or "").strip()   # â† .trim() â†’ .strip()
    if not content:
        conn.close()
        return jsonify({"error": "ë‚´ìš©ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤"}), 400

    # board_posts ìŠ¤í‚¤ë§ˆ: (id, username, text, pinned, hidden, ts)
    cur.execute(
        "INSERT INTO board_posts (id, username, text, pinned, hidden, ts) "
        "VALUES (?, ?, ?, 0, 0, strftime('%s','now'))",
        (str(uuid.uuid4()), username, content)
    )
    conn.commit()

    # â–¶ ìƒˆ ê¸€ ì¶”ê°€ ì§í›„ ì´ˆê³¼ë¶„ ì •ë¦¬
    try:
        trim_board_to_max()
    except Exception:
        pass

    conn.close()
    return jsonify({"ok": True})

# ë¡œì»¬ ë§ì¶¤ë²• ì´ˆê¸°í™”
_init_symspell()

@app.route("/verify", methods=["POST", "OPTIONS"])
@require_user
def verify():
    if request.method == "OPTIONS":
        return "", 200  # preflight OK

    try:
        data = request.get_json(force=True, silent=True) or {}
        text = (data.get("text") or "").strip()
        if not text:
            return jsonify({"error": "No text provided", "results": []}), 400

        # [ADD] non-admin size guard (í•­ëª©ë‹¹ 100KB)
        limit_resp = enforce_size_limit_or_400(text)
        if limit_resp:
            return limit_resp

        all_items = []

        # ì—¬ê¸°ì— GPT ë§ì¶¤ë²•/ë¬¸ë§¥ ê²€ì‚¬ í˜¸ì¶œ ë¡œì§ ì‚½ì…
        # (chunk_text_with_offsets + gpt_call í™œìš©)

        # === GPT ê¸°ë°˜ ë§ì¶¤ë²•/ë¬¸ë§¥ì˜¤ë¥˜ ê²€ì‚¬ ===
        chunks = chunk_text_with_offsets(text)  # (base_offset, chunk_text)
        for base, chunk in chunks:
            prompt = f"""
ë„ˆëŠ” í•œêµ­ì–´ ë¬¸ì¥ êµì • ì „ë¬¸ê°€ë‹¤.
ì•„ë˜ ê¸€ì—ì„œ **ë§ì¶¤ë²•/ë„ì–´ì“°ê¸°/ì–´ë²• ì˜¤ë¥˜(=type: "ë§ì¶¤ë²•")**,
**ë¶€ìì—°ìŠ¤ëŸ½ê±°ë‚˜ ëŠê¸´ ë¬¸ì¥(=type: "ë¬¸ë§¥ì˜¤ë¥˜")**ë§Œ ì°¾ì•„ë¼.
ì‹¬ì˜/ê´‘ê³ ë²•/ì˜ë£Œ ê·œì •(íš¨ê³¼Â·ì¬ë°œë¥ Â·ë³´ì¥ ë“±)ì€ **ì™„ì „íˆ ë¬´ì‹œ**í•œë‹¤.

ë°˜ë“œì‹œ JSON ë°°ì—´ë§Œ ì¶œë ¥í•˜ë©°, ê° í•­ëª©ì€ ì•„ë˜ ìŠ¤í‚¤ë§ˆë¥¼ ë”°ë¥¸ë‹¤:
{{
  "type": "ë§ì¶¤ë²•" | "ë¬¸ë§¥ì˜¤ë¥˜",
  "original": "ì›ë¬¸ì—ì„œ ê·¸ëŒ€ë¡œ ë³µì‚¬(ê³µë°±/ê¸°í˜¸ë„ ë™ì¼)",
  "reason": "ê°„ë‹¨ ì„¤ëª…",
  "severity": "low" | "medium" | "high",
  "suggestions": ["ëŒ€ì•ˆ1","ëŒ€ì•ˆ2"],   // ì—†ìœ¼ë©´ ë¹ˆ ë°°ì—´
  "start": ì •ìˆ˜,   // ì´ ì²­í¬ ë‚´ ì‹œì‘ ì˜¤í”„ì…‹
  "end": ì •ìˆ˜      // ì´ ì²­í¬ ë‚´ ë ì˜¤í”„ì…‹(í¬í•¨X)
}}

ê²€ì‚¬í•  ê¸€:
\"\"\"{chunk}\"\"\"
""".strip()

            resp = gpt_call(MODEL, [{"role": "user", "content": prompt}])
            raw = (resp.choices[0].message.content or "").strip()
            items = extract_json_array(raw)

            used_local = set()
            for it in (items or []):
                origin = (it.get("original") or "").strip()
                if not origin:
                    continue

                # 1) GPTê°€ ì¤€ start/end ì˜¤í”„ì…‹ì„ ìµœìš°ì„  ì‹ ë¢°
                s_rel = it.get("start")
                e_rel = it.get("end")
                locs = []
                if isinstance(s_rel, int) and isinstance(e_rel, int) and 0 <= s_rel < e_rel <= len(chunk):
                    locs = [s_rel]
                else:
                    # 2) ì‹¤íŒ¨ ì‹œ fallback: ë¶€ë¶„ ë¬¸ìì—´ íƒìƒ‰
                    locs = find_all(chunk, origin)

                for local_idx in locs:
                    gidx = base + local_idx
                    if gidx in used_local:
                        continue
                    used_local.add(gidx)

                    before, after = add_context(text, gidx, len(origin))
                    all_items.append({
                        "id": f"v_{len(all_items)}",
                        "type": it.get("type") or "ë§ì¶¤ë²•",
                        "original": origin,
                        "suggestions": (it.get("suggestions") or [])[:3],
                        "reason": it.get("reason") or "",
                        "severity": it.get("severity") or "low",
                        "startIndex": gidx,
                        "endIndex": gidx + len(origin),
                        "before": before,
                        "after": after
                    })
                    break  # ë™ì¼ í•­ëª© ì¤‘ë³µ ë°©ì§€


        # ë¬¸ì¥ ë‹¨ìœ„ íœ´ë¦¬ìŠ¤í‹± ì¶”ê°€ (ë£¨í”„ ë°–ì—ì„œ í•œ ë²ˆë§Œ)
        all_items.extend(find_fragments_by_sentence(text))
        all_items.extend(find_context_issues(text))

        # ì¶œì²˜ íƒœê·¸
        for it in all_items:
            it["source"] = "verify"

        return jsonify({"results": all_items, "aiSummary": None})
    except Exception as e:
        import traceback
        print("âŒ /verify ì˜¤ë¥˜:", e)
        traceback.print_exc()
        return jsonify({"error": str(e), "results": []}), 500

@app.post("/spell/local")
def spell_local():
    """
    ë¡œì»¬ ì² ì(ë§ì¶¤ë²•)ë§Œ ê²€ì‚¬. ë¬¸ë§¥/ì–´ë²•/ì‹¬ì˜ íŒë‹¨ ì—†ìŒ.
    body: { "text": str, "min_len": 3, "max_sug": 3 }
    """
    try:
        data = request.get_json(force=True, silent=True) or {}
        text = (data.get("text") or "")

        # [ADD] non-admin size guard (í•­ëª©ë‹¹ 100KB)
        limit_resp = enforce_size_limit_or_400(text)
        if limit_resp:
            return limit_resp

        if not text.strip() or _sym is None:
            return jsonify({"results": []})

        min_len = int(data.get("min_len") or 3)   # ì§§ì€ í† í° ì œì™¸(ì˜¤íƒ‘â†“)
        max_sug = int(data.get("max_sug") or 3)   # ì œì•ˆ ìƒìœ„ Nê°œ

        results = []
        for s, e, tok in _token_spans_ko(text):
            if len(tok) < min_len:
                continue
            # í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸/ì‚¬ì „ì— ìˆìœ¼ë©´ í†µê³¼
            if tok in _whitelist or _sym.words.get(tok, 0) > 0:
                continue

            suggs = _sym.lookup(tok, Verbosity.TOP, max_edit_distance=2, include_unknown=False)
            if not suggs:
                continue

            suggs = _sym.lookup(tok, Verbosity.TOP, max_edit_distance=2, include_unknown=False)
            if not suggs:
                continue

            cand = [su.term for su in suggs[:max_sug]]
            results.append({
                "type": "ë§ì¶¤ë²•",
                "original": tok,
                "reason": "ì‚¬ì „ì— ì—†ëŠ” ë‹¨ì–´ë¡œ ì¶”ì •",
                "severity": "low",
                "suggestions": cand,
                "startIndex": s,
                "endIndex": e,
                "before": text[max(0, s-24):s],
                "after":  text[e:min(len(text), e+24)],
                "source": "local-spell"
            })

        results.sort(key=lambda x: (x["startIndex"], x["endIndex"]))
        return jsonify({"results": results})
    except Exception as e:
        import traceback; traceback.print_exc()
        return jsonify({"results": [], "error": str(e)}), 500

# ============================
# (NEW) AI íƒì§€(v1 Â· ë¡œì»¬)
# ============================
@app.post("/ai_local_detect")
@require_user
def ai_local_detect():
    """
    v1 ë¡œì»¬ AI íƒì§€ â€“ ì™„ì „ ë¡œì»¬ ê¸°ë°˜ íœ´ë¦¬ìŠ¤í‹± ì ìˆ˜
    ë°˜í™˜ êµ¬ì¡°:
      { ok:True, score:0.0~1.0, label:'human|mixed|ai', message:str }
    """
    try:
        body = request.get_json(force=True, silent=True) or {}
        text = (body.get("text") or "").strip()

        if not text:
            return jsonify({"ok": False, "error": "EMPTY_TEXT"}), 400

        # === 1) ë¬¸ì¥ ë¶„ë¦¬ ===
        spans = _sentence_spans(text)
        sentences = [s for (_, _, s) in spans]

        if not sentences:
            return jsonify({"ok": False, "error": "NO_SENTENCE"}), 400

        # === 2) ë¬¸ì¥ë³„ AI íŒ¨í„´ ìŠ¤ì½”ì–´ ===
        scores = []
        for s in sentences:
            n = _norm_for_dup(s)
            toks = _ko_word_norm(s)

            # 2-1) ë¬¸ì¥ ê¸¸ì´ ê· ì¼ì„±(ê¸°ê³„ì  ë¦¬ë“¬)
            L = len(n)
            len_score = 0
            if 60 <= L <= 90:
                len_score = 0.4
            elif 40 <= L <= 110:
                len_score = 0.2

            # 2-2) ì ‘ì†ì‚¬/AI ì „í˜• íŒ¨í„´ ë°˜ë³µ (í™•ì¥)
            ai_connect = [
                "ë˜í•œ", "ê·¸ë¦¬ê³ ", "ê·¸ëŸ¬ë¯€ë¡œ", "ê·¸ë˜ì„œ", "ë”°ë¼ì„œ",
                "í•˜ì§€ë§Œ", "ê·¸ëŸ¬ë‚˜", "í•œí¸", "ê²Œë‹¤ê°€", "ì´ì™€ í•¨ê»˜",
                "ë¿ë§Œ ì•„ë‹ˆë¼", "ì¢…í•©í•˜ë©´", "ë¬´ì—‡ë³´ë‹¤", "ë¨¼ì €", "ë‹¤ìŒìœ¼ë¡œ",
            ]
            connect_count = sum(s.count(c) for c in ai_connect)
            # ì ‘ì†ì‚¬ê°€ 1ê°œë©´ 0.1, 2ê°œë©´ 0.2, 3ê°œ ì´ìƒì´ë©´ 0.3ìœ¼ë¡œ ìº¡
            connect_score = min(0.3, connect_count * 0.1)

            # 2-3) "í•©ë‹ˆë‹¤."/"ìŠµë‹ˆë‹¤." ë°˜ë³µ (AI í”í•¨)
            rep_score = 0.25 if s.endswith(("ìŠµë‹ˆë‹¤.", "í•©ë‹ˆë‹¤.")) else 0.0

            # 2-4) ë‹¨ì–´ ë‹¤ì–‘ì„± ë¶€ì¡±(ì¤‘ë³µ ë‹¨ì–´ ë°˜ë³µ)
            freq = {}
            for t in toks:
                freq[t] = freq.get(t, 0) + 1
            dup_count = sum(1 for v in freq.values() if v >= 3)
            dup_score = min(0.3, dup_count * 0.15)

            # ì´í•©
            sent_score = len_score + connect_score + rep_score + dup_score
            sent_score = min(1.0, sent_score)
            scores.append(sent_score)

        # === 3) ì „ì²´ í‰ê·  + ê³ ìœ„í—˜ ë¹„ìœ¨ ===
        avg = sum(scores) / len(scores)
        high = sum(1 for x in scores if x >= 0.7)

        # === 4) ë¼ë²¨ ê²°ì • â€“ ì„ê³„ê°’ì„ ë” ê³µê²©ì ìœ¼ë¡œ ì¡°ì • ===
        #  - avg 0.55 ì´ìƒì´ê±°ë‚˜, ê³ ìœ„í—˜ ë¬¸ì¥ì´ 1ê°œ ì´ìƒ + ì „ì²´ì˜ 30% ì´ìƒì´ë©´ AIë¡œ ê°„ì£¼
        #  - 0.35~0.55 êµ¬ê°„ì€ í˜¼í•©/ì˜ì‹¬ êµ¬ê°„
        if avg >= 0.55 or high >= max(1, int(len(scores) * 0.3)):
            label = "ai"
            msg = "AI ì‘ì„± íŒ¨í„´ì— ê½¤ ê°€ê¹ìŠµë‹ˆë‹¤. (ë¡œì»¬ íœ´ë¦¬ìŠ¤í‹± ê¸°ì¤€, ì°¸ê³ ìš©ì…ë‹ˆë‹¤.)"
        elif avg >= 0.35:
            label = "mixed"
            msg = "AIì™€ ì‚¬ëŒ íŒ¨í„´ì´ ì„ì˜€ê±°ë‚˜, AI ì˜í–¥ì´ ì¼ë¶€ ìˆëŠ” ë¬¸ì¥ìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤."
        else:
            label = "human"
            msg = "ì‚¬ëŒì´ ì“´ ê¸€ì— ë” ê°€ê¹Œìš´ ë¬¸ì²´ì…ë‹ˆë‹¤. (ì—­ì‹œ ì°¸ê³ ìš©ì…ë‹ˆë‹¤.)"

        return jsonify({
            "ok": True,
            "score": round(avg, 3),
            "label": label,
            "message": msg,
            "sentences": len(scores),
            "high_risk": high,
        })

    except Exception as e:
        return jsonify({"ok": False, "error": str(e)}), 500

# === [NEW] ë¬¸ì„œ ìŠ¤íƒ€ì¼/ì„œìˆ í˜• í”„ë¡œíŒŒì¼ (ì •ë³´í˜•/í›„ê¸°/í”„ë¡œëª¨ì…˜) ===

_DOC_INFO_HINTS = [
    "ì •ì˜", "íŠ¹ì§•", "ì¢…ë¥˜", "ì¥ì ", "ë‹¨ì ", "ì£¼ì˜ì‚¬í•­",
    "ì ˆì°¨", "ë°©ë²•", "ìˆœì„œ", "ì¤€ë¹„ë¬¼",
    "ì‹ ì²­", "ì ‘ìˆ˜", "ë°œê¸‰", "ë“±ë¡",
    "ë¹„ìš©", "ê°€ê²©", "ìš”ê¸ˆ", "ê¸°ê°„", "ëŒ€ìƒ", "ì¡°ê±´",
    "í•„ìš”í•©ë‹ˆë‹¤", "ì°¸ê³ í•´ ì£¼ì„¸ìš”", "ì°¸ê³ í•˜ì„¸ìš”", "ì•Œì•„ë‘ì„¸ìš”",
]

_DOC_REVIEW_HINTS = [
    "í›„ê¸°", "ê²½í—˜ë‹´", "ë¦¬ë·°",
    "ì§ì ‘", "ì‚¬ìš©í•´ë³´", "ì¨ë³´", "ì¨ë´¤",
    "ë‹¤ë…€ì™”", "ë°©ë¬¸í–ˆ", "ë°›ì•„ë´¤", "ë°›ì•˜ì–´ìš”",
    "ëŠê¼ˆ", "ëŠê»´ì¡Œ", "ìƒê°í–ˆ", "ìƒê°ì´ ë“¤ì—ˆ",
]

_DOC_CTA_HINTS = [
    "ìƒë‹´", "ë¬¸ì˜", "ì˜ˆì•½", "í´ë¦­", "ì—°ë½",
    "ì „í™”", "ëŒ€í‘œë²ˆí˜¸", "ì¹´ì¹´ì˜¤í†¡", "ì¹´í†¡", "ì±„ë„ ì¶”ê°€",
    "ì§€ê¸ˆ", "ë°”ë¡œ", "ì§€ê¸ˆ ë°”ë¡œ", "í˜œíƒ", "ì´ë²¤íŠ¸",
    "í• ì¸", "í”„ë¡œëª¨ì…˜", "íŠ¹ê°€", "í•œì •", "ë§ˆê°",
]

_DOC_FIRST_PERSON = [
    "ì €", "ì œê°€", "ì €ëŠ”", "ì œ", "ì €í¬", "ì €í¬ëŠ”",
    "ìš°ë¦¬", "ìš°ë¦¬ëŠ”", "ìš°ë¦¬ê°€",
]

_DOC_OPINION_ADJ = [
    "ì¢‹ì•˜", "ê´œì°®ì•˜", "ë§Œì¡±", "ì•„ì‰¬ì› ", "ë¶ˆí¸í–ˆ",
    "í¸í–ˆ", "ë¶ˆë§Œì¡±", "ì¶”ì²œ", "ì¶”ì²œë“œ", "ë„ì›€ì´ ë",
]

_TIME_EXPR_RX = re.compile(
    r"(ì–´ì œ|ì˜¤ëŠ˜|ì´ë²ˆì—|ì²˜ìŒì—|ì²˜ìŒì—ëŠ”|ê·¸ ë’¤ì—|ê·¸í›„ì—|ê·¸ í›„ì—|ì´í›„ì—|ê·¸ë•Œ|ê·¸ ë‹¹ì‹œ)"
)

_PHONE_RX = re.compile(r"\d{2,4}-\d{3,4}-\d{4}")

# --- [NEW] ë¬¸ì²´/AI í”ì  ê´€ë ¨ íŒ¨í„´ -------------------
# ê·¼ê±° ì—†ì´ 'ë§ì´ ëŠ˜ì–´ë‚˜ê³  ìˆìŠµë‹ˆë‹¤' ì‹ìœ¼ë¡œ ì“°ì´ëŠ” ì¶”ì„¸ í‘œí˜„
_VAGUE_TREND_RX = re.compile(
    r"(ëŠ˜ì–´ë‚˜ê³  ìˆìŠµ|ì¦ê°€í•˜ê³  ìˆìŠµ|ë§ì•„ì§€ê³  ìˆìŠµ|ë†’ì•„ì§€ê³  ìˆìŠµ|"
    r"ìˆ˜ìš”ê°€ ë§ì•„ì§€ê³  ìˆìŠµ|ê´€ì‹¬ì´ ë†’ì•„ì§€ê³  ìˆìŠµ)"
)

# 'ì‚¬ëŒë“¤ì´ ë§ì´ ~í•˜ê³  ìˆìŠµë‹ˆë‹¤' ê°™ì€ ë­‰ëš±ê·¸ë¦° í‘œí˜„
_GENERIC_MANY_RX = re.compile(
    r"(ì‚¬ëŒë“¤ì´ ë§ì´|ìˆ˜ìš”ë„ ë§ì´|ë§ì€ ë¶„ë“¤ì´|ë¬¸ì˜ê°€ ë§ì•„ì§€ê³  ìˆìŠµ|"
    r"ìš”ì²­ì´ ë§ì•„ì§€ê³  ìˆìŠµ|ê´€ì‹¬ë„ ë†’ì•„ì§€ê³  ìˆìŠµ)"
)

# ë˜í•œ/ê·¸ë¦¬ê³ /ê·¸ëŸ¬ë‚˜/ë”°ë¼ì„œ/ê·¸ëŸ¬ë¯€ë¡œ/ê·¸ë˜ì„œ/í•œí¸/ê²Œë‹¤ê°€/ë¬´ì—‡ë³´ë‹¤...
_CONNECTIVE_WORDS = [
    "ë˜í•œ", "ê·¸ë¦¬ê³ ", "ê·¸ëŸ¬ë‚˜", "í•˜ì§€ë§Œ", "ë”°ë¼ì„œ", "ê·¸ëŸ¬ë¯€ë¡œ", "ê·¸ë˜ì„œ",
    "í•œí¸", "ê²Œë‹¤ê°€", "ë¬´ì—‡ë³´ë‹¤", "ë¨¼ì €", "ë‹¤ìŒìœ¼ë¡œ", "ë§ˆì§€ë§‰ìœ¼ë¡œ",
    "ì´ì™€ ê°™ì´", "ì´ì™€ í•¨ê»˜", "ë¿ë§Œ ì•„ë‹ˆë¼",
]

# ë„ì™€ë“œë¦¬ê² ìŠµë‹ˆë‹¤/ì•ˆë‚´í•´ ë“œë¦¬ê² ìŠµë‹ˆë‹¤/ì§„í–‰í•´ ë“œë¦¬ê² ìŠµë‹ˆë‹¤... ê°™ì€ ì•ˆë‚´ ë©˜íŠ¸
_HELP_PHRASES = [
    "ë„ì™€ë“œë¦¬ê² ìŠµë‹ˆë‹¤", "ë„ì™€ ë“œë¦¬ê² ìŠµë‹ˆë‹¤",
    "ì•ˆë‚´í•´ ë“œë¦¬ê² ìŠµë‹ˆë‹¤", "ì•ˆë‚´í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤", "ì•ˆë‚´ ë“œë¦¬ê² ìŠµë‹ˆë‹¤",
    "ì§„í–‰í•´ ë“œë¦¬ê² ìŠµë‹ˆë‹¤", "ì§„í–‰í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤",
    "í™•ì¸í•´ ë“œë¦¬ê² ìŠµë‹ˆë‹¤", "í™•ì¸í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤",
    "ì ìš©í•´ ë“œë¦¬ê² ìŠµë‹ˆë‹¤", "ì ìš©í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤",
    "ì•ˆë‚´ ë„ì™€ë“œë¦¬ê² ìŠµë‹ˆë‹¤", "ë„ì›€ ë“œë¦¬ê² ìŠµë‹ˆë‹¤",
]


def _count_hits(text: str, patterns: list[str]) -> int:
    c = 0
    t = text or ""
    for p in patterns:
        if not p:
            continue
        c += len(re.findall(re.escape(p), t))
    return c


def analyze_doc_style(text: str) -> dict:
    """
    ê¸€í• ì „ìš© ë¬¸ì„œ ìŠ¤íƒ€ì¼ ë¶„ì„ (ê°„ë‹¨ ë²„ì „)

    - ì •ë³´/í›„ê¸°/í”„ë¡œëª¨ì…˜ ë¶„ë¥˜ëŠ” í•˜ì§€ ì•Šê³ ,
      ë¬¸ì¥ ê¸¸ì´/ê³µì†ì²´/ëŠë‚Œí‘œ/ë¬¸ì˜ í‘œí˜„ ë“±ì„ ê¸°ì¤€ìœ¼ë¡œ
      ê²€ìƒ‰ì—”ì§„ì´ ì‹«ì–´í•  ìˆ˜ ìˆëŠ” íŒ¨í„´ë§Œ ê³¨ë¼ issues ë¡œ ëŒë ¤ì¤€ë‹¤.
    - í”„ë¡ íŠ¸ í˜¸í™˜ì„ ìœ„í•´ doc_type/type/scores/features êµ¬ì¡°ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€í•œë‹¤.
    """
    text = (text or "").strip()
    if not text:
        return {
            "ok": True,
            "doc_type": "general",
            "type": "general",
            "scores": {"info": 0.0, "review": 0.0, "promo": 0.0},
            "issues": [],
            "features": {},
        }

    # ê¸°ë³¸ ì „ì²˜ë¦¬
    norm = kr_norm(text)
    tokens = re.findall(r"[ê°€-í£A-Za-z0-9]+", norm)
    n_tok = max(1, len(tokens))

    sents = basic_kr_sentence_split(text)
    sents = [s.strip() for s in sents if s.strip()]
    n_sent = max(1, len(sents))

    # ë¬¸ì¥ ê¸¸ì´ í†µê³„
    sent_lens = [len(s) for s in sents]
    avg_len = sum(sent_lens) / len(sent_lens) if sent_lens else 0.0
    long_sent_count = sum(1 for L in sent_lens if L >= 50)

    # íŒ¨í„´ ì¹´ìš´íŠ¸ (ê¸°ì¡´ íŒíŠ¸ ëª©ë¡ ì¬ì‚¬ìš©)
    info_hits = _count_hits(text, _INFO_HINTS)
    review_hits = _count_hits(text, _REVIEW_HINTS)
    cta_hits = _count_hits(text, _CTA_HINTS)

    fp_hits = _count_hits(text, _FIRST_PERSON)
    opinion_hits = _count_hits(text, _OPINION_ADJ)
    time_expr_hits = len(_TIME_EXPR_RX.findall(text))
    phone_hits = len(_DOC_PHONE_RX.findall(text))

    # ì ‘ì†ì‚¬/ì¶”ì„¸ í‘œí˜„ ê´€ë ¨ ì§€í‘œ
    connective_hits = 0
    connective_start_hits = 0
    for s in sents:
        st = s.strip()
        for w in _CONNECTIVE_WORDS:  # ì´ë¯¸ ì•„ë˜ìª½ì— ì •ì˜ë¼ ìˆìŒ
            if w in st:
                connective_hits += st.count(w)
            if st.startswith(w):
                connective_start_hits += 1
                break

    vague_trend_hits = len(_VAGUE_TREND_RX.findall(text))
    generic_many_hits = len(_GENERIC_MANY_RX.findall(text))
    trend_hits = vague_trend_hits + generic_many_hits
    has_number = bool(re.search(r"\d", text)) or ("í¼ì„¼íŠ¸" in text) or ("ë°°" in text)

    # ê³µì†ì²´/ëŠë‚Œí‘œ ë¹„ìœ¨
    polite_ends = ("ìŠµë‹ˆë‹¤", "í•©ë‹ˆë‹¤", "ë©ë‹ˆë‹¤", "ë˜ì–´ìš”", "í•´ìš”")
    polite_sent = 0
    exclam_sent = 0
    for s in sents:
        st = s.strip()
        if any(st.endswith(e) for e in polite_ends):
            polite_sent += 1
        if "!" in st:
            exclam_sent += 1
    polite_ratio = polite_sent / n_sent
    exclam_ratio = exclam_sent / n_sent

    # ==== ìŠ¤íƒ€ì¼ ì´ìŠˆ ìƒì„± (ë„ì™€ì£¼ëŠ” ìš©ë„) ====
    issues = []

    # 1) ë¬¸ì¥ì´ ë„ˆë¬´ ê¸¸ ë•Œ
    if avg_len >= 45 or long_sent_count >= max(2, n_sent // 3):
        issues.append({
            "code": "LONG_SENTENCE",
            "label": "ë¬¸ì¥ì´ ë„ˆë¬´ ê¸¸ê²Œ ì´ì–´ì§",
            "reason": "ë¬¸ì¥ í‰ê·  ê¸¸ì´ê°€ ê¸¸ê±°ë‚˜ ê¸´ ë¬¸ì¥ì´ ì—°ì†ë˜ì–´ ìˆì–´, 2~3ê°œì˜ ì§§ì€ ë¬¸ì¥ìœ¼ë¡œ ë‚˜ëˆ„ë©´ ë” ìì—°ìŠ¤ëŸ½ìŠµë‹ˆë‹¤.",
        })

    # 2) ê³µì†ì²´ ì–´ë¯¸ ë¹„ìœ¨ì´ ë†’ì„ ë•Œ
    if polite_ratio >= 0.7 and n_sent >= 4:
        issues.append({
            "code": "POLITE_HIGH",
            "label": "ê³µì†ì²´ ì–´ë¯¸ ë¹„ìœ¨ì´ ë†’ìŒ",
            "reason": "ìŠµë‹ˆë‹¤/í•©ë‹ˆë‹¤/ë¼ìš” ê°™ì€ ê³µì†ì²´ ì–´ë¯¸ ë¹„ìœ¨ì´ ë†’ì•„ ê¸°ê³„ì ì¸ ëŠë‚Œì„ ì¤„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. "
                      " '-ë‹¤', '-ì¸ í¸ì´ë‹¤', '-í•  ìˆ˜ ìˆë‹¤' ê°™ì€ ì–´ë¯¸ë„ ì„ì–´ ë³´ì„¸ìš”.",
        })

    # 3) ëŠë‚Œí‘œ ê³¼ë‹¤
    if exclam_ratio >= 0.25:
        issues.append({
            "code": "EXCLAM_HIGH",
            "label": "ëŠë‚Œí‘œ ì‚¬ìš©ì´ ë§ìŒ",
            "reason": "ëŠë‚Œí‘œê°€ ë§ì´ ì‚¬ìš©ë˜ì–´ ê´‘ê³ Â·ì„ ë™ì„± ë¬¸ì¥ì²˜ëŸ¼ ë³´ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. "
                      "ì¤‘ìš”í•œ í•œë‘ ë¬¸ì¥ì—ë§Œ ëŠë‚Œí‘œë¥¼ ë‚¨ê¸°ê³  ë‚˜ë¨¸ì§€ëŠ” ë§ˆì¹¨í‘œë¡œ ì •ë¦¬í•´ ì£¼ì„¸ìš”.",
        })

    # 4) ë¬¸ì˜/ìƒë‹´ í‘œí˜„ & ì „í™”ë²ˆí˜¸
    if cta_hits >= 3 or phone_hits > 0:
        issues.append({
            "code": "PROMO_STRONG",
            "label": "ë¬¸ì˜/ìƒë‹´ í‘œí˜„ ë¹„ì¤‘ì´ ë†’ìŒ",
            "reason": "ìƒë‹´/ë¬¸ì˜/ì˜ˆì•½/ì´ë²¤íŠ¸/ì „í™”ë²ˆí˜¸ ë“±ì˜ í‘œí˜„ì´ ë§ì•„ ê³¼ë„í•œ ì˜ì—…ì„± ë¬¸ì¥ì²˜ëŸ¼ ë³´ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. "
                      "ë³¸ë¬¸ì—ì„œëŠ” ì •ë³´Â·ê²½í—˜ ìœ„ì£¼ë¡œ ì“°ê³  ë§ˆì§€ë§‰ì—ë§Œ ê°„ë‹¨íˆ ì•ˆë‚´ë¥¼ ë„£ëŠ” í¸ì´ ì¢‹ìŠµë‹ˆë‹¤.",
        })

    # 5) ì •ë³´ ì•ˆë‚´ + í›„ê¸° ë¬¸ì¥ì´ ë§ì´ ì„ì—¬ ìˆì„ ë•Œ
    if info_hits >= 5 and review_hits >= 5:
        issues.append({
            "code": "INFO_REVIEW_MIXED",
            "label": "ì •ë³´ ì•ˆë‚´ì™€ í›„ê¸° ë¬¸ì¥ì´ ì„ì—¬ ìˆìŒ",
            "reason": "ì•ˆë‚´/ì •ì˜/ì¡°ê±´ ì„¤ëª…ê³¼ ì‹¤ì œ ì‚¬ìš© í›„ê¸° ë¬¸ì¥ì´ ì„ì—¬ ìˆìŠµë‹ˆë‹¤. "
                      "'ì •ë³´ ì•ˆë‚´'ì™€ 'ì‚¬ìš© í›„ê¸°'ë¥¼ ì†Œì œëª©ìœ¼ë¡œ ë‚˜ëˆ„ë©´ ê°€ë…ì„±ì´ ì¢‹ì•„ì§‘ë‹ˆë‹¤.",
        })

    # 6) ë˜í•œ/ê·¸ë¦¬ê³ /ê·¸ëŸ¬ë‚˜/ë”°ë¼ì„œ ë“± ì ‘ì†ì‚¬ ë°˜ë³µ
    if connective_start_hits >= 3 or connective_hits >= max(5, n_sent):
        issues.append({
            "code": "CONNECTIVE_REPEAT",
            "label": "ë˜í•œ/ê·¸ë¦¬ê³  ë“± ì ‘ì†ì‚¬ê°€ ë°˜ë³µë¨",
            "reason": "ë˜í•œ/ê·¸ë¦¬ê³ /ê·¸ëŸ¬ë‚˜/ë”°ë¼ì„œ ê°™ì€ ì ‘ì†ì‚¬ê°€ ìì£¼ ë°˜ë³µë©ë‹ˆë‹¤. "
                      "ëª¨ë“  ë¬¸ì¥ì„ ì ‘ì†ì‚¬ë¡œ ì‹œì‘í•˜ê¸°ë³´ë‹¤ëŠ”, í•µì‹¬ í‚¤ì›Œë“œë¥¼ ë¬¸ì¥ ì•ì— ë‘ê³  "
                      "ë¬¸ë‹¨ ì‚¬ì´ ì—°ê²°ì´ í•„ìš”í•  ë•Œë§Œ ì ‘ì†ì‚¬ë¥¼ ì‚¬ìš©í•˜ëŠ” í¸ì´ ìì—°ìŠ¤ëŸ½ìŠµë‹ˆë‹¤.",
        })

    # 7) 'ë§ì´ ëŠ˜ì–´ë‚˜ê³  ìˆìŠµë‹ˆë‹¤' ì‹ ì¶”ì„¸ í‘œí˜„ì¸ë° ìˆ«ì/ê¸°ì¤€ì´ ì—†ëŠ” ê²½ìš°
    if trend_hits >= 1 and not has_number:
        issues.append({
            "code": "VAGUE_TREND",
            "label": "ê·¼ê±° ì—†ëŠ” ì¶”ì„¸ í‘œí˜„ì´ ìˆìŒ",
            "reason": "â€˜ë§ì´ ëŠ˜ì–´ë‚˜ê³  ìˆìŠµë‹ˆë‹¤â€™, â€˜ìˆ˜ìš”ê°€ ë†’ì•„ì§€ê³  ìˆìŠµë‹ˆë‹¤â€™ ê°™ì€ ì¶”ì„¸ í‘œí˜„ì´ ë‚˜ì˜¤ì§€ë§Œ "
                      "êµ¬ì²´ì ì¸ ìˆ«ìë‚˜ ê¸°ê°„, ê¸°ì¤€ì´ ì—†ìŠµë‹ˆë‹¤. ìµœê·¼ 1~2ë…„ ê¸°ì¤€ì˜ í†µê³„Â·ê²€ìƒ‰ëŸ‰Â·ë¬¸ì˜ ê±´ìˆ˜ ë“± "
                      "ê°„ë‹¨í•œ ê·¼ê±°ë¥¼ í•¨ê»˜ ì ì–´ ì£¼ë©´ ì„¤ë“ë ¥ê³¼ ì‹ ë¢°ë„ê°€ ì˜¬ë¼ê°‘ë‹ˆë‹¤.",
        })

    # í”„ë¡ íŠ¸ í˜¸í™˜ìš© í•„ë“œë“¤
    return {
        "ok": True,
        "doc_type": "general",
        "type": "general",
        "scores": {"info": 0.0, "review": 0.0, "promo": 0.0},
        "issues": issues,
        "features": {
            "tokens": n_tok,
            "sentences": n_sent,
            "avg_sentence_len": round(avg_len, 1),
            "long_sentence_count": long_sent_count,
            "info_hits": info_hits,
            "review_hits": review_hits,
            "cta_hits": cta_hits,
            "first_person_hits": fp_hits,
            "opinion_hits": opinion_hits,
            "time_expr_hits": time_expr_hits,
            "phone_hits": phone_hits,
            "polite_ratio": round(polite_ratio, 3),
            "exclam_ratio": round(exclam_ratio, 3),
            "connective_hits": connective_hits,
            "connective_start_hits": connective_start_hits,
            "trend_hits": trend_hits,
        },
    }

@app.post("/ai_local_detect_v2")
@require_user
def ai_local_detect_v2():
    """
    ë¡œì»¬ + íœ´ë¦¬ìŠ¤í‹± ê°•í™” AI íƒì§€ v2 (ì˜ˆë¹„ í•„í„°ìš©)
    - í† í° ê¸¸ì´ / TTR / ë°˜ë³µ bigram
    - ë¬¸ì¥ ë ì–´ë¯¸(ìŠµë‹ˆë‹¤/í•©ë‹ˆë‹¤ ê³„ì—´) + ì ‘ì†ì‚¬ ë°˜ë³µ + ì „ì²´ ê¸¸ì´ íŒ¨í„´ê¹Œì§€ ë°˜ì˜
    - ê²°ê³¼ ì ìˆ˜: 0~100, score â†‘ì¼ìˆ˜ë¡ AI ê°€ëŠ¥ì„± â†‘
    - ì¶”ê°€: ë¬¸ì¥ ë‹¨ìœ„ë¡œ ai_pattern êµ¬ê°„(items)ë„ ê°™ì´ ë°˜í™˜
    """
    try:
        data = request.get_json(force=True, silent=True) or {}
        text = (data.get("text") or "")
        text = text.strip()

        if not text:
            return jsonify({"ok": False, "error": "No text provided"}), 400

        # í¬ê¸° ì œí•œ ê³µí†µ ì ìš©
        limit_resp = enforce_size_limit_or_400(text)
        if limit_resp:
            return limit_resp

        # 0) ì „ì²˜ë¦¬ / ê¸°ë³¸ ë‹¨ìœ„
        norm = kr_norm(text)

        # --- í† í°í™”: ê¸°ì¡´ tokenize + í•œêµ­ì–´ í´ë°± ---
        tokens = tokenize(norm)
        # ê¸°ì¡´ ì •ê·œì‹ì´ í•œêµ­ì–´ë¥¼ ëª» ì¡ëŠ” ê²½ìš°ë¥¼ ëŒ€ë¹„í•œ í´ë°±
        if not tokens:
            # í•œê¸€/ì˜ë¬¸/ìˆ«ìë¡œ ì´ë£¨ì–´ì§„ ë©ì–´ë¦¬ë¥¼ ì „ë¶€ í† í°ìœ¼ë¡œ ì‚¬ìš©
            tokens = re.findall(r"[ê°€-í£A-Za-z0-9]+", norm)

        n_tokens = len(tokens)
        sentences = basic_kr_sentence_split(text)
        n_sent = max(1, len(sentences))

        # 1) ê¸°ë³¸ í†µê³„
        avg_len = n_tokens / n_sent if n_sent else 0.0  # ë¬¸ì¥ë‹¹ í† í° ìˆ˜
        uniq = len(set(tokens)) if tokens else 0
        ttr = (uniq / n_tokens) if n_tokens else 0.0    # type-token ratio

        # 2) ë°˜ë³µ bigram ë¹„ìœ¨
        from collections import Counter
        bigrams = []
        for i in range(len(tokens) - 1):
            bigrams.append((tokens[i], tokens[i + 1]))
        rep_rate = 0.0
        if bigrams:
            c = Counter(bigrams)
            repeated = sum(v for v in c.values() if v >= 2)
            rep_rate = repeated / len(bigrams)

        # 3) ë¬¸ì²´ íŒ¨í„´ íœ´ë¦¬ìŠ¤í‹± (ì „ì²´ ë¬¸ë‹¨ ê¸°ì¤€)
        polite_ends = ("ìŠµë‹ˆë‹¤.", "í•©ë‹ˆë‹¤.", "ë©ë‹ˆë‹¤.", "ìˆìŠµë‹ˆë‹¤.", "ê²ƒì…ë‹ˆë‹¤.")
        ai_connect = [
            "ë˜í•œ", "ê·¸ë¦¬ê³ ", "ê·¸ëŸ¬ë¯€ë¡œ", "ê·¸ë˜ì„œ", "ë”°ë¼ì„œ",
            "í•˜ì§€ë§Œ", "ê·¸ëŸ¬ë‚˜", "í•œí¸", "ê²Œë‹¤ê°€", "ì´ì™€ í•¨ê»˜",
            "ë¿ë§Œ ì•„ë‹ˆë¼", "ë¬´ì—‡ë³´ë‹¤", "ë¨¼ì €", "ë‹¤ìŒìœ¼ë¡œ", "ì¢…í•©í•˜ë©´",
        ]

        polite_count = 0
        connect_sent_count = 0

        for s in sentences:
            s_stripped = s.strip()
            if s_stripped.endswith(polite_ends):
                polite_count += 1
            if any(c in s_stripped for c in ai_connect):
                connect_sent_count += 1

        polite_ratio = polite_count / n_sent if n_sent else 0.0
        connect_ratio = connect_sent_count / n_sent if n_sent else 0.0

        pattern_score = 0.0
        # ê³µì†ì²´ ì–´ë¯¸ ë°˜ë³µ ë¹„ìœ¨
        if polite_ratio >= 0.7:
            pattern_score += 0.35
        elif polite_ratio >= 0.4:
            pattern_score += 0.20

        # ì ‘ì†ì‚¬ ë¬¸ì¥ ë¹„ìœ¨
        if connect_ratio >= 0.5:
            pattern_score += 0.25
        elif connect_ratio >= 0.3:
            pattern_score += 0.15

        # ì „ì²´ ê¸¸ì´ê°€ ì–´ëŠ ì •ë„ ì´ìƒì´ë©´ ì¶”ê°€ ê°€ì¤‘ì¹˜
        if n_tokens >= 260:
            pattern_score += 0.20
        elif n_tokens >= 160:
            pattern_score += 0.10

        pattern_score = min(1.0, pattern_score)

        # 4) ìŠ¤ì½”ì–´ë§ (0~100)
        #   - avg_len â†‘, ttr â†“, rep_rate â†‘, pattern_score â†‘ => AI íŒ¨í„´ ì˜ì‹¬ë„ â†‘
        #   - score: 0~100, ê°’ì´ í´ìˆ˜ë¡ AI ìŠ¤íƒ€ì¼ í”ì ì´ ë§ë‹¤ëŠ” ì˜ë¯¸ (ê¸€í• ai ì „ëµ)
        s_len = min(1.0, avg_len / 30.0)          # 30 í† í° ì´ìƒì´ë©´ ìµœëŒ“ê°’ (ë” ê³µê²©ì )
        s_ttr = 1.0 - min(1.0, ttr / 0.70)        # TTR 0.7 ì´í•˜ë¶€í„° ì ìˆ˜â†‘
        s_rep = min(1.0, rep_rate / 0.18)         # ë°˜ë³µ bigram 18% ì´ìƒì´ë©´ ìµœëŒ“ê°’

        # ê¸¸ì´/TTR/ë°˜ë³µ + ë¬¸ì²´ íŒ¨í„´ì„ ì„ì–´ì„œ ê³„ì‚°
        raw = (
            s_len * 0.30 +
            s_ttr * 0.30 +
            s_rep * 0.15 +
            pattern_score * 0.25
        )

        # ì•„ì£¼ ì§§ì€ ê¸€(ë¬¸ì¥ 1~2ê°œ ìˆ˜ì¤€)ì€ ì˜ˆë¹„í•„í„° ì˜ë¯¸ê°€ ì•½í•˜ë¯€ë¡œ ì ìˆ˜ ì¶•ì†Œ
        if n_tokens < 60:
            raw *= 0.4
        elif n_tokens < 120:
            raw *= 0.7

        raw = max(0.0, min(1.0, raw))
        score = int(round(raw * 100))  # 0~100 â†’ "AI íŒ¨í„´ ì˜ì‹¬ ì§€ìˆ˜(%)"

        # 5) ë¼ë²¨ â€“ ê¸€í• ai ì „ëµ: ì‚¬ëŒ/AI ë‹¨ì • ëŒ€ì‹  3ë‹¨ê³„ ìœ„í—˜ë„ë¡œë§Œ í‘œê¸°
        #    score â†‘ == AI íŒ¨í„´ ì˜ì‹¬ë„ â†‘
        if score >= 70:
            label = "ai_risk_high"
            level = "high"
            msg = f"AI íŒ¨í„´ ì˜ì‹¬ ì§€ìˆ˜ê°€ ì•½ {score}% ìˆ˜ì¤€ì…ë‹ˆë‹¤. ê¸€ ì „ì²´ì— AI ìŠ¤íƒ€ì¼ í”ì ì´ ë§ì´ í¬í•¨ëœ ì›ê³ ì…ë‹ˆë‹¤. (ë¡œì»¬ ì§€í‘œ, ì°¸ê³ ìš©)"
        elif score >= 45:
            label = "ai_risk_mid"
            level = "medium"
            msg = f"AI íŒ¨í„´ ì˜ì‹¬ ì§€ìˆ˜ê°€ ì•½ {score}% ìˆ˜ì¤€ì…ë‹ˆë‹¤. ì¼ë¶€ ë¬¸ë‹¨ì—ì„œ AI ìŠ¤íƒ€ì¼ í”ì ì´ ê´€ì°°ë©ë‹ˆë‹¤. (ë¡œì»¬ ì§€í‘œ, ì°¸ê³ ìš©)"
        else:
            label = "ai_risk_low"
            level = "low"
            msg = f"AI íŒ¨í„´ ì˜ì‹¬ ì§€ìˆ˜ê°€ ì•½ {score}% ìˆ˜ì¤€ì…ë‹ˆë‹¤. AI ìŠ¤íƒ€ì¼ í”ì  ë¹„ìœ¨ì´ ìƒëŒ€ì ìœ¼ë¡œ ë‚®ì€ ì›ê³ ì…ë‹ˆë‹¤. (ë¡œì»¬ ì§€í‘œ, ì°¸ê³ ìš©)"

        # 6) ì„¸ë¶€ AI íŒ¨í„´ êµ¬ê°„ ì¶”ì¶œ (ë¬¸ì¥ ë‹¨ìœ„)
        #    -> ì—ë””í„° ì¤‘ì•™ í•˜ì´ë¼ì´íŠ¸/ì¶”ì²œí•­ëª©ì—ì„œ ê·¸ëŒ€ë¡œ ì‚¬ìš© ê°€ëŠ¥
        spans = _sentence_spans(text)
        items = []
        for start, end, sent in spans:
            s_norm = _norm_for_dup(sent)
            toks = _ko_word_norm(sent)

            L = len(s_norm)

            # 6-1) ë¬¸ì¥ ê¸¸ì´ ê¸°ë°˜ ì ìˆ˜ (ë„ˆë¬´ ì§§ì€ ë¬¸ì¥ì€ ì œì™¸)
            len_score = 0.0
            if L >= 80:
                len_score += 0.25
            elif L >= 50:
                len_score += 0.15

            # 6-2) ì ‘ì†ì‚¬/ì „í˜•ì  ì´ì–´ì£¼ê¸° íŒ¨í„´
            s_connect_count = sum(sent.count(c) for c in ai_connect)
            connect_score = min(0.3, s_connect_count * 0.1)

            # 6-3) ê³µì†ì²´ ì–´ë¯¸
            polite_flag = sent.strip().endswith(polite_ends)
            polite_score = 0.25 if polite_flag else 0.0

            # 6-4) ë‹¨ì–´ ë°˜ë³µ íŒ¨í„´ (ë¬¸ì¥ ë‚´ë¶€)
            tf = {}
            for t in toks:
                tf[t] = tf.get(t, 0) + 1
            dup_terms = [t for t, v in tf.items() if v >= 3]
            dup_score = min(0.3, len(dup_terms) * 0.12)

            sent_raw = len_score + connect_score + polite_score + dup_score
            sent_raw = max(0.0, min(1.0, sent_raw))

            # ë¬¸ì¥ ë‹¨ìœ„ë¡œ "AI íŒ¨í„´"ìœ¼ë¡œ ë³¼ ìˆ˜ ìˆëŠ” ê¸°ì¤€
            if sent_raw >= 0.55 or (polite_flag and s_connect_count >= 1) or len(dup_terms) >= 2:
                reasons = []
                if polite_flag:
                    reasons.append("ê³µì†ì²´ ì–´ë¯¸(ìŠµë‹ˆë‹¤/í•©ë‹ˆë‹¤) ë°˜ë³µ")
                if s_connect_count >= 1:
                    reasons.append("ì ‘ì†ì‚¬/ì „í˜•ì ì¸ ì´ì–´ì£¼ê¸° í‘œí˜„ ë‹¤ìˆ˜ ì‚¬ìš©")
                if len(dup_terms) >= 2:
                    reasons.append("ë™ì¼ ë‹¨ì–´ê°€ í•œ ë¬¸ì¥ ì•ˆì—ì„œ ì—¬ëŸ¬ ë²ˆ ë°˜ë³µ")
                if L >= 120:
                    reasons.append("ê¸¸ê³  ë¹„ìŠ·í•œ ë¬¸ì¥ ê¸¸ì´ê°€ ë°˜ë³µë˜ëŠ” ê²½í–¥")

                # ë¬¸ì¥ ë‹¨ìœ„ AI íŒ¨í„´ ë ˆë²¨ (ê¸€í• ai ì „ëµìš© 3ë‹¨ê³„)
                if sent_raw >= 0.8:
                    sent_level = 3
                    sev = "high"
                elif sent_raw >= 0.6:
                    sent_level = 2
                    sev = "medium"
                else:
                    sent_level = 1
                    sev = "low"

                items.append({
                    "type": "ai_pattern",
                    "category": "AIíŒ¨í„´",
                    "subType": "sentence_pattern",
                    "score": round(sent_raw, 3),
                    # ê¸°ì¡´ severity ê·¸ëŒ€ë¡œ ë‘ë˜, ë‚´ë¶€ ê¸°ì¤€ì€ ë ˆë²¨ ê¸°ë°˜ìœ¼ë¡œ í†µì¼
                    "severity": sev,
                    # ê¸€í• ai ì „ëµìš©: 1(ì•½í•¨) ~ 3(ê°•í•¨)
                    "aiLevel": sent_level,
                    "startIndex": start,
                    "endIndex": end,
                    "text": sent,
                    "before": text[max(0, start - 30): start],
                    "after": text[end: min(len(text), end + 30)],
                    "reason": " / ".join(reasons) or "AIì—ì„œ ìì£¼ ë³´ì´ëŠ” ë¬¸ì¥ êµ¬ì¡°",
                    "source": "local-ai-v2",
                })

        signals = {
            "tokens": n_tokens,
            "sentences": len(sentences),
            "avg_sentence_tokens": avg_len,
            "type_token_ratio": ttr,
            "repeat_bigram_ratio": rep_rate,
            "polite_ratio": polite_ratio,
            "connect_ratio": connect_ratio,
        }

        # ì‚¬ìš©ëŸ‰ ë¡œê¹…(ìˆìœ¼ë©´ ì‚¬ìš©, ì—ëŸ¬ ë‚˜ë„ ë¬´ì‹œ)
        try:
            username = _username_from_req()
            log_usage(username, "ai_local_detect_v2", len(text))
        except Exception:
            pass

        return jsonify({
            "ok": True,
            "score": score,                       # 0~100, ë†’ì„ìˆ˜ë¡ AI ê°€ëŠ¥ì„± â†‘
            "label": label,                       # ai_suspected / borderline / human_like
            "level": level,                       # high / medium / low
            "message": msg,
            "signals": signals,                   # ë””ë²„ê¹…ìš© ì„¸ë¶€ ì§€í‘œ
            "items": items,                       # ë¬¸ì¥ ë‹¨ìœ„ AI íŒ¨í„´ êµ¬ê°„
            "ai_suspicious_sentences": len(items)
        })

    except Exception as e:
        import traceback
        traceback.print_exc()
        return jsonify({"ok": False, "error": str(e)}), 500

# === [NEW] ë¬¸ì„œ ìŠ¤íƒ€ì¼ í”„ë¡œíŒŒì¼ë§ ì—”ë“œí¬ì¸íŠ¸ ===
@app.post("/doc_style_profile")
@require_user
def doc_style_profile():
    """
    ì›ê³ ì˜ ë¬¸ì²´/ìš©ë„ ìœ í˜•ì„ ë‹¨ìˆœ ê·œì¹™ ê¸°ë°˜ìœ¼ë¡œ ë¶„ì„.
    - type: info | review | promo | unknown
    - scores: ê° ìœ í˜•ë³„ 0~1 ì ìˆ˜
    - features: ê·œì¹™ íŒë‹¨ì— ì‚¬ìš©í•œ ê°„ë‹¨ ì§€í‘œ
    """
    try:
        data = request.get_json(force=True, silent=True) or {}
        text = (data.get("text") or "").strip()

        if not text:
            return jsonify({"ok": False, "error": "No text provided"}), 400

        # í¬ê¸° ì œí•œ ê³µí†µ ì ìš©
        limit_resp = enforce_size_limit_or_400(text)
        if limit_resp:
            return limit_resp

        prof = analyze_doc_style(text)
        return jsonify(prof)
    except Exception as e:
        return jsonify({"ok": False, "error": str(e)}), 500

@app.post("/guide_keyword_count")
@require_user
def guide_keyword_count():
    """
    í‚¤ì›Œë“œ ë¹ˆë„/ìœ„ì¹˜ ê²€ì‚¬ (+ ìœ ì‚¬ë„ ëª¨ë“œ ì§€ì›)
    body: {
      "title": str,
      "text": str,
      "keywords": [str],
      "require": {"titleMin": int, "bodyMin": int, "totalMin": int},
      "fuzzy": bool,          # trueë©´ ìœ ì‚¬ë„ ëª¨ë“œ
      "threshold": float      # ê¸°ë³¸ 0.82 ê¶Œì¥ (0.80~0.85 ë²”ìœ„)
    }
    """
    data = request.get_json(force=True, silent=True) or {}
    title = (data.get("title") or "").strip()
    text  = (data.get("text")  or "").strip()
    fuzzy = bool(data.get("fuzzy"))
    threshold = float(data.get("threshold") or 0.5)

    # ë¹„ê´€ë¦¬ì 100KB ê°€ë“œ
    limit = enforce_size_limit_or_400(text)
    if limit:
        return limit

    keywords = data.get("keywords") or []
    require  = data.get("require") or {}
    title_need = int(require.get("titleMin") or 0)
    body_need  = int(require.get("bodyMin")  or 0)
    total_need = int(require.get("totalMin") or 0)

    # ---------- ìœ í‹¸: ìœ ì‚¬ë„/ì •í™•ì¼ì¹˜ ----------
    import re

    def _norm(s: str) -> str:
        # ê³µë°±/ë¬¸ì¥ë¶€í˜¸ ì œê±° + ì†Œë¬¸ì (ìœ ì‚¬ë„ ê³„ì‚°ìš©)
        return re.sub(r"[\s\W_]+", "", (s or "").lower())

    def _ngrams(s: str, n: int = 3):
        s = _norm(s)
        return {s[i:i+n] for i in range(max(0, len(s) - n + 1))} or ({s} if s else set())

    def _jaccard(a: set, b: set) -> float:
        if not a and not b: return 1.0
        if not a or not b:  return 0.0
        inter = len(a & b); union = len(a | b)
        return inter / (union or 1)

    def find_positions_exact(hay: str, kw: str):
        # ì •í™•ì¼ì¹˜(í˜„ì¬ ë™ì‘ê³¼ ë™ì¼)
        hits, pos = [], 0
        hay_l, kw_l = (hay or "").lower(), (kw or "").lower()
        if not kw_l: return hits
        while True:
            i = hay_l.find(kw_l, pos)
            if i == -1: break
            hits.append({"start": i, "end": i + len(kw)})
            pos = i + len(kw)
        return hits

    def find_positions_fuzzy(hay: str, kw: str, thr: float = 0.82, pad: int = 8):
        """
        3-gram Jaccard ìœ ì‚¬ë„.
        ê³µë°±/ì¡°ì‚¬/ì–´ë¯¸ ë³€í™”, 'ì•ˆì „í•˜ê²Œ ê·€ê°€í•˜ì‹œê¸¸' ê°™ì€ ë³€í˜•ì„ í¬ìš©.
        - ìœˆë„ìš°: len(kw)+pad
        """
        hits = []
        if not hay or not kw: return hits
        kw_ngr = _ngrams(kw)
        win = max(len(kw) + pad, 10)

        i, L = 0, len(hay)
        while i < L:
            seg = hay[i:i+win]
            score = _jaccard(_ngrams(seg), kw_ngr)
            if score >= thr:
                hits.append({"start": i, "end": i + len(seg), "score": round(score, 3)})
                # ê²¹ì¹¨ ê³¼ë‹¤ ë°©ì§€: í‚¤ì›Œë“œ ê¸¸ì´ì˜ ì ˆë°˜ë§Œí¼ ì í”„
                i += max(1, len(kw)//2)
            else:
                i += 1
        return hits
    # -----------------------------------------

    results = []
    for kw in keywords:
        # ë¬¸ì¥í˜•(ê³µë°± í¬í•¨Â·ë‘ ë‹¨ì–´ ì´ìƒ)ì€ ìë™ í¼ì§€ë¡œ, ì„ê³„ê°’ ì‚´ì§ ë‚®ì¶¤
        is_sentence = (" " in kw.strip())
        thr = (threshold if fuzzy else (0.75 if is_sentence else 0.82))

        tpos = find_positions_fuzzy(title, kw, thr) if (fuzzy or is_sentence) else find_positions_exact(title, kw)
        bpos = find_positions_fuzzy(text,  kw, thr) if (fuzzy or is_sentence) else find_positions_exact(text,  kw)
        tot  = len(tpos) + len(bpos)

        ok = True
        if title_need: ok = ok and (len(tpos) >= title_need)
        if body_need:  ok = ok and (len(bpos) >= body_need)
        if total_need: ok = ok and (tot >= total_need)

        results.append({
            "keyword": kw,
            "titleCount": len(tpos),
            "bodyCount":  len(bpos),
            "total":      tot,
            "titlePositions": tpos,
            "bodyPositions":  bpos,
            "ok": ok
        })

    all_ok = all(r["ok"] for r in results) if results else True
    return jsonify({"ok": True, "all_ok": all_ok, "results": results})

@app.post("/guide_verify_dedup")
@require_user
def guide_verify_dedup():
    """
    ë¬¸ì¥ í…œí”Œë¦¿ 'ì •ë°€ ëª¨ë“œ' (ìœ ì‚¬ë„ ì œê±° ë²„ì „)
    - í•˜ì´ë¸Œë¦¬ë“œ ìœ ì‚¬ë„ ëŒ€ì‹ 
      'í•µì‹¬ì–´ ì¡°í•©(2ê°œ ì´ìƒ)'ë§Œìœ¼ë¡œ í¬í•¨ ì—¬ë¶€ íŒë‹¨.
    - guide_verify_local ê³¼ ë™ì¼í•˜ê²Œ window ì•ˆì—ì„œ
      ì„œë¡œ ë‹¤ë¥¸ í•µì‹¬ì–´ê°€ coreNeed ê°œ ì´ìƒ ê°™ì´ ë“±ì¥í•˜ë©´ OKë¡œ ì²˜ë¦¬.
    body ì˜ˆì‹œ:
    {
      "text": str,
      "templates": [str],
      "threshold": float = 0.78,   # í•˜ìœ„ í˜¸í™˜ìš©(ì§€ê¸ˆì€ ì˜ë¯¸ ê±°ì˜ ì—†ìŒ)
      "coreNeed": int   = 2,       # í•µì‹¬ì–´ ìµœì†Œ ê°œìˆ˜ ( = min_core_hits )
      "window_size": int = 80      # ì˜µì…˜, ê¸°ë³¸ 80
    }
    """
    data = request.get_json(force=True, silent=True) or {}

    raw_text = data.get("text") or ""
    # ì •ê·œí™” ì•ˆ í•˜ê³  ì›ë¬¸ ê¸°ì¤€ìœ¼ë¡œë§Œ ê²€ì‚¬ (í•˜ì´ë¼ì´íŠ¸ ì¢Œí‘œ ìœ ì§€)
    text = raw_text

    # Editor.js ì—ì„œëŠ” required_guides ë¼ëŠ” ì´ë¦„ìœ¼ë¡œ ë³´ë‚´ë¯€ë¡œ
    # templates / required_guides ë‘˜ ë‹¤ ì§€ì›
    raw_templates = data.get("templates") or data.get("required_guides") or []
    templates = [
           (t or "").strip()
           for t in raw_templates
           if isinstance(t, str) and (t or "").strip()
    ]

    # ê¸°ì¡´ í•„ë“œ ì¬í™œìš©
    thr = float(data.get("threshold") or 0.78)      # ì‘ë‹µì—ë§Œ ê·¸ëŒ€ë¡œ ëŒë ¤ì¤Œ
    core_need = int(data.get("coreNeed") or 2)      # = min_core_hits
    window_size = int(data.get("window_size") or 80)

    # ë¹„ê´€ë¦¬ì 100KB ê°€ë“œ
    limit = enforce_size_limit_or_400(text)
    if limit:
        return limit

    hits = []
    for tpl in templates:
        cores = core_terms(tpl)  # í…œí”Œë¦¿ì—ì„œ í•µì‹¬ì–´ ì¶”ì¶œ

        # í•µì‹¬ì–´ ì¡°í•© ê¸°ë°˜ í›„ë³´ êµ¬ê°„ ì°¾ê¸°
        candidates = _guide_keyword_windows(
            text,
            tpl,
            window_size=window_size,
            min_core_hits=core_need,
        )

        if candidates:
            best = candidates[0]  # hit_count ê°€ì¥ í° êµ¬ê°„
            core_hit = best["hit_count"]
            start = best["start"]
            end = best["end"]
            snippet = best["text"]
            ok = core_hit >= min(core_need, len(cores))

            # scoreëŠ” 'í•µì‹¬ì–´ ì¶©ì¡± ë¹„ìœ¨'ë¡œ ì¬ì •ì˜ (0~1)
            score = core_hit / max(1, len(cores))
        else:
            core_hit = 0
            start = -1
            end = -1
            snippet = ""
            ok = False
            score = 0.0

        hits.append({
            "template": tpl,
            "score": round(score, 3),      # ìœ ì‚¬ë„ ëŒ€ì‹  'í•µì‹¬ì–´ ë¹„ìœ¨'
            "coreNeed": core_need,
            "coreHit": core_hit,
            "start": start,
            "end": end,
            "snippet": snippet,
            "ok": bool(ok),
            "cores": cores,
        })

    all_ok = all(h["ok"] for h in hits) if hits else True

    # thresholdëŠ” í•˜ìœ„ í˜¸í™˜ ë•Œë¬¸ì— ê·¸ëŒ€ë¡œ ë°˜í™˜ë§Œ í•¨
    return jsonify({
        "ok": True,
        "all_ok": all_ok,
        "hits": hits,
        "threshold": thr,
        "mode": "keyword_combo"   # ë””ë²„ê·¸ìš© í”Œë˜ê·¸(í”„ë¡ íŠ¸ì—ì„œ ë³´ê³  êµ¬ë¶„ ê°€ëŠ¥)
    })


# ==== [ADD] Hybrid Similarity Utils (KO) ====
import re
from difflib import SequenceMatcher
try:
    # ìˆìœ¼ë©´ ìë™ ì‚¬ìš©(ì„±ëŠ¥/ì •í™•ë„â†‘)
    from rapidfuzz import fuzz as _rf_fuzz
except Exception:
    _rf_fuzz = None

_KO_STOP = {"ë°","ê·¸ë¦¬ê³ ","ë˜ëŠ”","ê·¸","ì´","ì €","ê²ƒ","ì—ì„œ","ìœ¼ë¡œ","í•˜ë‹¤","í•©ë‹ˆë‹¤","ë°”ëë‹ˆë‹¤","í•´ì£¼ì„¸ìš”","í•˜ì‹œê¸¸","í•˜ì„¸ìš”","ì…ë‹ˆë‹¤","í•˜ëŠ”","í•˜ê¸°","í•˜ì—¬","í•˜ë©°","ë˜","ë°"}
_rx_token = re.compile(r"[ê°€-í£A-Za-z0-9]+")

def _ko_sim_norm(s: str) -> str:
    """
    guide_verify_local / ë¡¤ë§ ìœˆë„ìš° ê¸°ë°˜ ë¬¸ì¥ ìœ ì‚¬ë„ì—ì„œ ì‚¬ìš©í•˜ëŠ”
    í•œêµ­ì–´ ë¬¸ì¥ ì •ê·œí™” í•¨ìˆ˜.
    í˜„ì¬ëŠ” ë„ëŒì´í‘œ/ìœ ì‚¬ë¬¸ì¥ íƒì§€ìš© _norm_for_dup ê³¼ ë™ì¼ ê·œì¹™ì„ ì‚¬ìš©í•œë‹¤.
    """
    return _norm_for_dup(s or "")


# ---- [NEW] KoSimCSE ì˜ë¯¸ ìœ ì‚¬ë„ (ì˜µì…˜) ----
try:
    from sentence_transformers import SentenceTransformer
    import numpy as np

    # í•œêµ­ì–´ìš© KoSimCSE ëª¨ë¸ (ë¡œì»¬ ë‹¤ìš´ë¡œë“œ í›„ ìºì‹œ ì‚¬ìš©)
    _KOSIM_MODEL = SentenceTransformer("BM-K/KoSimCSE-roberta-multitask")
    print("[INFO] KoSimCSE model loaded for guide_verify_local")
except Exception as _e:
    _KOSIM_MODEL = None
    print("[WARN] KoSimCSE not available, fallback to 3-gram only:", _e)

def _semantic_sim_scores(candidates: list[str], template: str) -> list[float]:
    """
    KoSimCSE ê¸°ë°˜ ì˜ë¯¸ ìœ ì‚¬ë„ (0~1).
    - ëª¨ë¸ì´ ì—†ìœ¼ë©´ ì „ë¶€ 0.0 ë°˜í™˜ â†’ ê¸°ì¡´ 3-gramë§Œ ì‚¬ìš©.
    - candidates: ì‹¤ì œ ì›ê³  ìª½ ë¬¸ì¥/ì„¸ê·¸ë¨¼íŠ¸ ë¦¬ìŠ¤íŠ¸
    - template: í•„ìˆ˜ê°€ì´ë“œ í…œí”Œë¦¿ ë¬¸ì¥
    """
    if not _KOSIM_MODEL or not candidates:
        return [0.0] * len(candidates)

    # normalize_embeddings=True â†’ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ê°€ ë‹¨ìˆœ ë‚´ì ìœ¼ë¡œ ê³„ì‚°ë¨
    embs = _KOSIM_MODEL.encode([template] + candidates, normalize_embeddings=True)
    q = embs[0]
    others = embs[1:]
    # dot product = cosine similarity
    sims = (others * q).sum(axis=1).tolist()

    # ì•ˆì „í•˜ê²Œ [0, 1]ë¡œ í´ë¨í”„
    out = []
    for s in sims:
        try:
            v = float(s)
        except Exception:
            v = 0.0
        if v < 0.0: v = 0.0
        if v > 1.0: v = 1.0
        out.append(v)
    return out
# ---- [KoSimCSE block ë] ----

def kr_norm(s: str) -> str:
    # ê°„ë‹¨ ì •ê·œí™”(ê³µë°± ì •ë¦¬ + ì „ê°/íŠ¹ìˆ˜ ì œê±° ìœ ì‚¬)
    s = (s or "").strip()
    s = re.sub(r"\s+", " ", s)
    return s

def spacing_agnostic_regex(kw: str) -> str:
    # 'ì•ˆì „ ê·€ê°€' -> 'ì•ˆì „[\s\W_]*ê·€ê°€'
    toks = re.split(r"\s+", kw.strip())
    return r"[\s\W_]*".join(map(re.escape, toks))

def tokenize(s: str):
    return [t for t in _rx_token.findall(s)]

def _strip_ko_josa_ending(tok: str) -> str:
    """í•œêµ­ì–´ í† í°ì˜ ëì— ë¶™ì€ ì¡°ì‚¬/ì–´ë¯¸ë¥¼ ë‹¨ìˆœ ê·œì¹™ìœ¼ë¡œ ì œê±°í•œë‹¤.
    - ëŒ€ë¦¬ìš´ì „ì„ / ëŒ€ë¦¬ìš´ì „ì€ / ëŒ€ë¦¬ìš´ì „ì´ -> ëŒ€ë¦¬ìš´ì „
    - ìŒì£¼ê°€ / ìŒì£¼ë¥¼ -> ìŒì£¼
    ë„ˆë¬´ ê³¼í•˜ê²Œ ìë¥´ì§€ ì•Šë„ë¡, ìµœì†Œ 2ê¸€ì ì´ìƒë§Œ ë‚¨ê¸´ë‹¤.
    """
    if not tok:
        return tok
    # í•œê¸€ì´ ì—†ìœ¼ë©´ ê·¸ëŒ€ë¡œ ë‘”ë‹¤ (ì˜ë¬¸/ìˆ«ì ë“±)
    if not re.search(r"[ê°€-í£]", tok):
        return tok

    base = tok

    # 1) ì—¬ëŸ¬ ê¸€ìë¡œ ëœ ì–´ë¯¸/ì„œìˆ í˜•ì„ ë¨¼ì € ì œê±°
    multi_suffixes = [
        "ì…ë‹ˆë‹¤",
        "í•©ë‹ˆë‹¤",
        "ì˜€ìŠµë‹ˆë‹¤",
        "í–ˆìŠµë‹ˆë‹¤",
        "ë˜ì—ˆìŠµë‹ˆë‹¤",
        "ëìŠµë‹ˆë‹¤",
        "í•´ìš”",
        "ë¼ìš”",
        "ë˜ì–´ìš”",
        "í–ˆì–´ìš”",
        "ì˜€ì–´ìš”",
    ]
    for suf in multi_suffixes:
        if base.endswith(suf) and len(base) > len(suf) + 1:
            base = base[: -len(suf)]
            break

    # 2) í•œ ê¸€ì ì§œë¦¬ ì¡°ì‚¬ë“¤(ì€,ëŠ”,ì´,ê°€,ì„,ë¥¼,ë„,ë§Œ,ê¹Œ,ì™€,ê³¼,ì—,ë¡œ ë“±)ì„
    #    ë„ˆë¬´ ì¤„ì´ì§€ ì•ŠëŠ” ì„ ì—ì„œ ëì—ì„œë¶€í„° ì˜ë¼ë‚¸ë‹¤.
    while len(base) > 1 and base[-1] in "ì€ëŠ”ì´ê°€ì„ë¥¼ë„ë§Œë¿ì¡°ê¹Œì™€ê³¼ì—ë¡œ":
        base = base[:-1]

    return base


def core_terms(s: str, max_terms: int = 5):
    # 1ì°¨ í† í°í™” + ë¶ˆìš©ì–´ ì œê±°
    raw = [t for t in tokenize(s) if t not in _KO_STOP]
    normalized = []

    for t in raw:
        # ì¡°ì‚¬/ì–´ë¯¸ ì œê±°
        base = _strip_ko_josa_ending(t)
        # ë„ˆë¬´ ì§§ì€ ê±´ ë²„ë¦¼ (í•œ ê¸€ì ì¡°ì‚¬ë§Œ ë‚¨ì€ ê²½ìš° ë“±)
        if len(base) < 2:
            continue
        normalized.append(base)

    # í˜¹ì‹œ ëª¨ë‘ ì˜ë ¤ë‚˜ê°”ë‹¤ë©´, ì›ë³¸ í† í°ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    toks = normalized or raw

    # ê¸¸ì´/í¬ì†Œì„± ê¸°ì¤€ ìƒìœ„ ì¶”ì¶œ
    toks.sort(key=lambda x: (-len(x), x))
    return toks[:max_terms] or toks

def token_jaccard(a: str, b: str) -> float:
    A = set(tokenize(a)); B = set(tokenize(b))
    if not A and not B: return 1.0
    if not A or not B:  return 0.0
    return len(A & B) / len(A | B)

def char_ratio(a: str, b: str) -> float:
    if _rf_fuzz is not None:
        # ê³µë°±/ìˆœì„œ ë³€í™”ì— ê°•í•¨
        return _rf_fuzz.token_set_ratio(a, b) / 100.0
    # fallback: difflib
    return SequenceMatcher(None, a, b).ratio()

def hybrid_score(a: str, b: str) -> float:
    a1, b1 = kr_norm(a), kr_norm(b)
    tj = token_jaccard(a1, b1)
    cr = char_ratio(a1, b1)
    # ë‹¨ì–´ì™€ ë¬¸ì ìœ ì‚¬ë„ì˜ ê°€ì¤‘ í‰ê· (ì‹¤ì „ ê²€ì¦ì¹˜)
    return 0.55 * cr + 0.45 * tj

def _dedup_inter_lite_v2(
    files,
    min_len: int = 6,
    max_chars: int = 8000,
    n: int = 4,
    min_ratio: float = 0.01,
):
    """
    ë‹¤ë¬¸ì„œ ìš”ì•½ ìœ ì‚¬ë„ (ë¬¸ì„œ ë‹¨ìœ„ ì¬í™œìš© íŒì •ìš©)

    - ê° ë¬¸ì„œë¥¼ kr_norm ìœ¼ë¡œ ì •ê·œí™”í•œ ë’¤ hybrid_score ê¸°ë°˜ìœ¼ë¡œ
      ë¬¸ì„œ ì „ì²´ ìœ ì‚¬ë„(0~1)ë¥¼ ê³„ì‚°.
    - ê²°ê³¼:
      - per_file: íŒŒì¼ë³„ ìµœëŒ€/í‰ê·  ìœ ì‚¬ë„ + ìƒìœ„ ë§¤ì¹­ ë¦¬ìŠ¤íŠ¸
      - pairs: íŒŒì¼ ìŒë³„ ìœ ì‚¬ë„ + 'ì¬í™œìš© band' ë¼ë²¨
    """

    # 1) ì „ì²˜ë¦¬: ì´ë¦„/í…ìŠ¤íŠ¸ ì •ë¦¬
    docs = []
    for idx, f in enumerate(files):
        name = str(f.get("name") or f"doc_{idx+1}")
        raw = (f.get("text") or "")[:max_chars]
        norm = kr_norm(raw)
        docs.append({
            "index": idx,
            "name": name,
            "raw": raw,
            "norm": norm,
            "length": len(norm),
        })

    n_docs = len(docs)
    if n_docs < 2:
        return {
            "ok": True,
            "mode": "lite",
            "min_ratio": min_ratio,
            "per_file": [],
            "pairs": [],
        }

    # 2) ì¬í™œìš© êµ¬ê°„ ë¼ë²¨ëŸ¬
    def classify_band(score: float) -> str:
        # scoreëŠ” hybrid_score ê¸°ì¤€ (0~1)
        if score >= 0.80:
            return "ê°•í•œ ì¬í™œìš©(80%+)"
        if score >= 0.50:
            return "ë¶€ë¶„ ì¬í™œìš©(50~79%)"
        if score >= 0.30:
            return "ë¶€ë¶„ ì¤‘ë³µ(30~49%)"
        if score >= 0.10:
            return "ê²½ë¯¸í•œ ì¤‘ë³µ(10~29%)"
        return "ê±°ì˜ ì—†ìŒ(<10%)"

    pair_rows = []
    neighbors = {d["index"]: [] for d in docs}

    # 3) ë¬¸ì„œ ìŒë³„ ì „ì²´ ìœ ì‚¬ë„ ê³„ì‚°
    for i in range(n_docs):
        for j in range(i + 1, n_docs):
            di = docs[i]
            dj = docs[j]

            # hybrid_score = 0.55*ë¬¸ììœ ì‚¬ + 0.45*í† í°ìì¹´ë“œ
            s = hybrid_score(di["norm"], dj["norm"])

            # ë„ˆë¬´ ë‚®ì€ ê±´ ë²„ë¦¬ê¸°
            if s < min_ratio:
                continue

            band = classify_band(s)

            rec = {
                "i": di["index"],
                "j": dj["index"],
                "name_i": di["name"],
                "name_j": dj["name"],
                "len_i": di["length"],
                "len_j": dj["length"],
                "sim_score": round(s, 3),
                "band": band,
                "reuse_suspected": bool(s >= 0.80),
            }
            pair_rows.append(rec)

            # ì–‘ìª½ ë¬¸ì„œì— ëª¨ë‘ neighbor ë“±ë¡
            neighbors[di["index"]].append({
                "other_index": dj["index"],
                "other_name": dj["name"],
                "sim_score": round(s, 3),
                "band": band,
                "reuse_suspected": bool(s >= 0.80),
            })
            neighbors[dj["index"]].append({
                "other_index": di["index"],
                "other_name": di["name"],
                "sim_score": round(s, 3),
                "band": band,
                "reuse_suspected": bool(s >= 0.80),
            })

    # 4) íŒŒì¼ë³„ ìš”ì•½ (ìµœëŒ€/í‰ê·  ìœ ì‚¬ë„ + TOP ë§¤ì¹­)
    per_file = []
    for d in docs:
        neigh = sorted(
            neighbors[d["index"]],
            key=lambda x: x["sim_score"],
            reverse=True,
        )
        top = neigh[:10]
        max_score = top[0]["sim_score"] if top else 0.0
        avg_score = (
            sum(x["sim_score"] for x in neigh) / len(neigh)
            if neigh else 0.0
        )
        reuse_level = classify_band(max_score)

        per_file.append({
            "index": d["index"],
            "name": d["name"],
            "length": d["length"],
            "max_sim": round(max_score, 3),
            "avg_sim": round(avg_score, 3),
            "reuse_level": reuse_level,
            "top_matches": top,
        })

    # ì¬í™œìš© ì˜ì‹¬ì´ ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬í•´ì„œ ë³´ê¸° í¸í•˜ê²Œ
    per_file.sort(key=lambda x: x["max_sim"], reverse=True)
    pair_rows.sort(key=lambda x: x["sim_score"], reverse=True)

    return {
        "ok": True,
        "mode": "lite",
        "per_file": per_file,
        "pairs": pair_rows,

        # ğŸ”¹ UI / PDF ê³µí†µ ì‚¬ìš© ì•ˆë‚´ ë©”ì‹œì§€ ì¶”ê°€
        "guidance": {
            "short_ui_notice":
                "â€» ë³¸ ìœ ì‚¬ë„ ê°’ì€ ë¬¸ì¥ êµ¬ì¡°Â·íŒ¨í„´ ê¸°ë°˜ ë‚´ë¶€ ì¤‘ë³µ íƒì§€ ê²°ê³¼ì´ë©°, "
                "ê°’ì´ ë†’ì„ìˆ˜ë¡ í…œí”Œë¦¿ ì¬ì‚¬ìš© ê°€ëŠ¥ì„±ì´ í¼ì„ ì˜ë¯¸í•©ë‹ˆë‹¤. "
                "0~10%: ìì—°ìŠ¤ëŸ¬ìš´ ìœ ì‚¬ ìˆ˜ì¤€ / 11~20%: ì£¼ì˜Â·ìˆ˜ì • ê¶Œì¥ / "
                "21~30%: ì¬ì‘ì„±Â·ì§‘ì¤‘ ì ê²€ ê¶Œì¥ / 31% ì´ìƒ: ì¬í™œìš© ì›ê³  ì˜ì‹¬(ì‚¬ìš© ìì œ ê¶Œì¥)",

            "pdf_detailed_notice":
                "ë³¸ ë³´ê³ ì„œì˜ ìœ ì‚¬ë„ ê°’ì€ ë¬¸ì¥ íŒ¨í„´Â·ì„œìˆ  êµ¬ì¡°Â·ë°˜ë³µ í‘œí˜„ì„ ì¤‘ì‹¬ìœ¼ë¡œ í‰ê°€ëœ "
                "ë‚´ë¶€ ê²€ìˆ˜ ì§€í‘œì´ë©°, ì™¸ë¶€ í‘œì ˆ ì„œë¹„ìŠ¤ì˜ í‘œì ˆìœ¨ê³¼ ì§ì ‘ ë¹„êµë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. "
                "ìœ ì‚¬ë„ ê°’ì€ ë²•ì  ì±…ì„ íŒë‹¨ ê¸°ì¤€ì´ ì•„ë‹Œ ì¬í™œìš© ìœ„í—˜ë„ ì§€í‘œë¡œ í•´ì„í•´ì•¼ í•©ë‹ˆë‹¤. "
                "0~10%: ìì—°ìŠ¤ëŸ¬ìš´ ìœ ì‚¬ ìˆ˜ì¤€ / 11~20%: ì£¼ì˜ í•„ìš” / 21~30%: ì¬ì‘ì„± ê¶Œê³  / "
                "31% ì´ìƒ: ì¬í™œìš© ì›ê³  ì˜ì‹¬(ì‚¬ìš© ë¶ˆê°€ ê¶Œì¥). "
                "ìµœì¢… íŒë‹¨ì€ ë‹´ë‹¹ìì˜ ìˆ˜ë™ ê²€í† ë¥¼ í•¨ê»˜ ë°˜ì˜í•©ë‹ˆë‹¤."
        }
    }

def contains_core_terms(text: str, terms, need: int) -> int:
    hits = 0
    for t in terms:
        if re.search(spacing_agnostic_regex(t), text, flags=re.IGNORECASE):
            hits += 1
    return hits if hits >= need else hits

def slide_best(text: str, template: str, pad: int = 16):
    """
    í…œí”Œë¦¿ ê¸¸ì´Â±pad ìœˆë„ìš°ë¡œ ìŠ¬ë¼ì´ë“œí•˜ë©° ìµœê³  êµ¬ê°„ íƒìƒ‰
    """
    text = text or ""
    template = template or ""
    L = len(text)
    win = max(len(template) + pad, 20)
    best = {"score": 0.0, "start": -1, "end": -1, "snippet": ""}
    i = 0
    step = max(1, len(template)//3 or 1)
    while i < L:
        seg = text[i:i+win]
        s = hybrid_score(seg, template)
        if s > best["score"]:
            best = {"score": s, "start": i, "end": i+len(seg), "snippet": seg[:180]}
        i += step
    return best
# ==== [/ADD] ====


@app.post("/guide_verify_local")
@require_user
def guide_verify_local():
    """
    í•„ìˆ˜ê°€ì´ë“œ ê²€ì‚¬ (ìœ ì‚¬ë„ ëª¨ë¸ ì œê±°, í•µì‹¬ì–´ ì¡°í•© ê¸°ë°˜)
    - í…œí”Œë¦¿ ë¬¸ì¥ì—ì„œ í•µì‹¬ ë‹¨ì–´ë¥¼ ì¶”ì¶œí•˜ê³ 
    - ì›ë¬¸ì—ì„œ window_size ê¸€ì ì•ˆì— ì„œë¡œ ë‹¤ë¥¸ í•µì‹¬ ë‹¨ì–´ê°€
      min_core_hitsê°œ ì´ìƒ ê°™ì´ ë“±ì¥í•˜ëŠ” êµ¬ê°„ì„ ì°¾ëŠ”ë‹¤.

    body ì˜ˆì‹œ:
    {
      "text": "ì›ê³  ì „ì²´ í…ìŠ¤íŠ¸",
      "templates": ["í•„ìˆ˜ê°€ì´ë“œ ë¬¸ì¥1", "í•„ìˆ˜ê°€ì´ë“œ ë¬¸ì¥2", ...],
      "window_size": 80,      # ì˜µì…˜, ê¸°ë³¸ 80
      "min_core_hits": 2      # ì˜µì…˜, ê¸°ë³¸ 2
    }
    """
    try:
        data = request.get_json(force=True, silent=True) or {}
        text = (data.get("text") or "")
        limit = enforce_size_limit_or_400(text)
        if limit:
            return limit

        # 1) ê¸°ë³¸: templates í•„ë“œ ì‚¬ìš©
        raw_templates = data.get("templates")

        # 2) Editor.js runRequiredCheck ì—ì„œ ì“°ëŠ” required_guidesë„ ì§€ì›
        if not raw_templates:
            raw_templates = data.get("required_guides") or []

        templates = []
        for t in raw_templates:
            # í˜¹ì‹œ dict í˜•íƒœë¡œ ì˜¬ ìˆ˜ë„ ìˆìœ¼ë‹ˆ ë°©ì–´ ì½”ë“œ
            if isinstance(t, dict):
                t = t.get("text") or ""
            if not isinstance(t, str):
                t = str(t)
            t = (t or "").strip()
            if t:
                templates.append(t)


        # ê¸°ë³¸ê°’: 80ì ìœˆë„ìš°, í•µì‹¬ì–´ 2ê°œ ì´ìƒ
        window_size = int(data.get("window_size") or 80)
        min_core_hits = int(data.get("min_core_hits") or 2)

        results = []

        for tpl in templates:
            # í•µì‹¬ì–´ ì¡°í•© ê¸°ë°˜ í›„ë³´ êµ¬ê°„ ì°¾ê¸°
            candidates = _guide_keyword_windows(
                text,
                tpl,
                window_size=window_size,
                min_core_hits=min_core_hits,
            )

            if candidates:
                # ê°€ì¥ ì¢‹ì€ í›„ë³´ í•˜ë‚˜ë¥¼ ëŒ€í‘œë¡œ ì‚¼ìŒ
                best = candidates[0]
                present = True
                msg = f"í•µì‹¬ ë‹¨ì–´ê°€ {best['core_hits']}ê°œ ì´ìƒ ê°™ì€ êµ¬ê°„ì— í•¨ê»˜ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤."
            else:
                best = None
                present = False
                msg = "ì›ê³ ì—ì„œ í•´ë‹¹ í•„ìˆ˜ê°€ì´ë“œì˜ í•µì‹¬ ë‹¨ì–´ê°€ í•¨ê»˜ í¬í•¨ëœ êµ¬ê°„ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."

            # Editor.js ê°€ ê¸°ëŒ€í•˜ëŠ” í˜•ì‹ì„ ìœ ì§€í•˜ê¸° ìœ„í•´ matches ë°°ì—´ êµ¬ì„±
            matches = []
            for c in candidates:
                matches.append({
                    "start": c["start"],
                    "end": c["end"],
                    "score": c["score"],
                    "sentence": c["sentence"],
                    "core_hits": c["core_hits"],
                    "core_terms": c["core_terms"],
                    "kind": "keyword_window",
                })

            results.append({
                "template": tpl,
                "best_score": (best["score"] if best else None),
                "present": present,
                "message": msg,
                "matches": matches,
            })

        overall_present = any(r["present"] for r in results)

        try:
            log_usage(_username_from_req(), "guide_local_keywords", len(templates))
        except Exception:
            pass

        # Editor.js ì˜ runRequiredCheck ì—ì„œ ì‚¬ìš©í•˜ëŠ” í‰íƒ„í™”ëœ í›„ë³´ ë¦¬ìŠ¤íŠ¸
        paragraph_candidates = []
        for idx, r in enumerate(results, start=1):
            tpl = r.get("template", "")
            for m in r.get("matches") or []:
                paragraph_candidates.append({
                    "template": tpl,
                    "template_index": idx,          # 1ë¶€í„° ì‹œì‘ (í•„ìˆ˜ê°€ì´ë“œ ì¤„ ë²ˆí˜¸)
                    "start": m.get("start", 0),
                    "end": m.get("end", 0),
                    "sentence": m.get("sentence", ""),
                    "score": m.get("score", 0.0),
                    "core_hits": m.get("core_hits", 0),
                    "core_terms": m.get("core_terms", []),
                    "best_score": r.get("best_score"),
                    "kind": m.get("kind", "keyword_window"),
                })

        return jsonify({
            "ok": True,
            "overall_present": overall_present,
            "results": results,
            "paragraph_candidates": paragraph_candidates,
        })

    except Exception as e:
        log_error(_username_from_req(), "/guide_verify_local", 500, str(e))
        return jsonify(
            {"ok": False, "error": "SERVER_ERROR", "message": str(e)}
        ), 500


# ===== ë¬¸ì¥ ë‹¨ìœ„ ë¬¸ë§¥ì˜¤ë¥˜ ìœ í‹¸ =====

SENT_SPLIT_RE = re.compile(r'(?<=[\.!?])\s+')
END_TOKEN_RE = re.compile(r'(ë‹¤|ìš”|ë‹ˆë‹¤|í•©ë‹ˆë‹¤|í–ˆë‹¤|ì˜€ë‹¤|ë©ë‹ˆë‹¤|ë¼ìš”|\.|!|\?)\s*$')

LEADING_CONNECTIVES = (
    "í•˜ì§€ë§Œ", "ê·¸ëŸ¬ë‚˜", "ë°˜ë©´ì—", "ê·¸ëŸ°ë°", "ë‹¤ë§Œ",
    "ë˜í•œ", "ê·¸ë¦¬ê³ ", "ê²Œë‹¤ê°€", "íŠ¹íˆ", "ë˜", "í•œí¸"
)

def split_sentences_with_pos(text: str):
    spans = []
    parts = SENT_SPLIT_RE.split(text)
    cur = 0
    for p in parts:
        if not p.strip():
            cur += len(p)
            continue
        start = text.find(p, cur)
        if start == -1:
            start = text.find(p)
        end = start + len(p)
        spans.append((p, start, end))
        cur = end
    return spans

def find_fragments_by_sentence(text: str):
    items = []
    spans = split_sentences_with_pos(text)
    for s, start, end in spans:
        sent = s.strip()
        if len(sent) < 15:
            continue
        if sent.startswith(("â€œ", "\"", "â€˜", "'")) and sent.endswith(("â€", "\"", "â€™", "'")):
            continue
        if not END_TOKEN_RE.search(sent):
            items.append({
                "id": f"frag_{start}",
                "type": "ë¬¸ë§¥ì˜¤ë¥˜",
                "original": sent,
                "reason": "ë¶ˆì™„ì „ ë¬¸ì¥(ì¢…ê²°ì–´ë¯¸/ë§ˆì¹¨í‘œ ëˆ„ë½) ê°€ëŠ¥",
                "severity": "medium",
                "suggestions": ["ë¬¸ì¥ì„ ë§ˆë¬´ë¦¬í•˜ëŠ” ì¢…ê²°ì–´ë¯¸/ë§ˆì¹¨í‘œ ì¶”ê°€ ê²€í† "],
                "startIndex": start,
                "endIndex": end,
                "before": "", "after": ""
            })
    return items

def find_context_issues(text: str):
    items = []
    spans = split_sentences_with_pos(text)
    for i, (s, start, end) in enumerate(spans):
        sent = s.strip()
        if len(sent) < 10:
            continue
        for conn in LEADING_CONNECTIVES:
            if sent.startswith(conn):
                if i == 0:
                    items.append({
                        "id": f"ctx_first_{start}",
                        "type": "ë¬¸ë§¥ì˜¤ë¥˜",
                        "original": sent[:min(40, len(sent))],
                        "reason": f"ì²« ë¬¸ì¥ì—ì„œ '{conn}' ì‚¬ìš©",
                        "severity": "low",
                        "suggestions": ["ì²« ë¬¸ì¥ì€ ì „í™˜ ì—†ì´ ì£¼ì œë¥¼ ì œì‹œí•˜ëŠ” ê²ƒì´ ìì—°ìŠ¤ëŸ¬ì›€"],
                        "startIndex": start, "endIndex": end,
                        "before": "", "after": ""
                    })
                else:
                    prev = spans[i-1][0].strip()
                    if len(prev) < 10 or not END_TOKEN_RE.search(prev):
                        items.append({
                            "id": f"ctx_link_{start}",
                            "type": "ë¬¸ë§¥ì˜¤ë¥˜",
                            "original": sent[:min(40, len(sent))],
                            "reason": f"ì• ë¬¸ì¥ì´ ì•½í•œ ìƒíƒœì—ì„œ '{conn}' ì‹œì‘",
                            "severity": "low",
                            "suggestions": ["ì• ë¬¸ì¥ì„ ë³´ê°•í•˜ê±°ë‚˜ ì—°ê²°ì–´ë¥¼ ìƒëµ/ë³€ê²½"],
                            "startIndex": start, "endIndex": end,
                            "before": "", "after": ""
                        })
                break
    return items


# ============== (NEW) ë„ëŒì´í‘œ/ìœ ì‚¬ ë¼ìš°íŠ¸ ==============
@app.route("/dedup_intra", methods=["POST"])
@require_user
def dedup_intra():
    """
    ë³¸ë¬¸ í•œ ê±´ ë‚´ ë„ëŒì´í‘œ(ì •í™•íˆ ê°™ì€ ë¬¸ì¥) + ìœ ì‚¬ ë¬¸ì¥(3-gram ìì¹´ë“œ) íƒì§€
    body: { "text": str, "min_len": 6, "sim_threshold": 0.85 }
    """
    try:
        data = request.get_json(force=True, silent=True) or {}
        text = (data.get("text") or "")
        if not text.strip():
            return jsonify({"error": "No text provided"}), 400

        # [ADD] non-admin size guard (í•­ëª©ë‹¹ 100KB)
        limit_resp = enforce_size_limit_or_400(text)
        if limit_resp:
            return limit_resp

        min_len = int(data.get("min_len", 6))
        sim_th  = float(data.get("sim_threshold", 0.85))
        exact, sims = _dedup_intra(text, min_len=min_len, sim_threshold=sim_th)
        return jsonify({"exact_groups": exact, "similar_pairs": sims})
    except Exception as e:
        import traceback
        print("âœ˜ /dedup_intra ì˜¤ë¥˜:", e)
        traceback.print_exc()
        return jsonify({"error": str(e)}), 500


@app.route("/dedup_inter", methods=["POST"])
@require_user
def dedup_inter():
    """
    ì—¬ëŸ¬ íŒŒì¼ ê°„ ë„ëŒì´í‘œ/ìœ ì‚¬ ë¬¸ì¥ íƒì§€
    body:
      {
        "files": [{"name": str, "text": str}, ...],
        "min_len": 6,
        "sim_threshold": 0.85,

        # (ì˜µì…˜) ìš”ì•½/ê²½ëŸ‰ ëª¨ë“œ
        # "mode": "lite" | "summary" | "full",
        # "max_chars": 8000 (ìš”ì•½ëª¨ë“œì—ì„œ ë¬¸ì„œ ê¸¸ì´ ì»·)
      }
    """
    try:
        data = request.get_json(force=True, silent=True) or {}
        files = data.get("files") or []
        if not files:
            return jsonify({"error": "No files provided"}), 400

        # [ADD] non-admin size guard (ê° í•­ëª© text 100KB)
        limit_resp = enforce_size_limit_or_400(files)
        if limit_resp:
            return limit_resp

        mode = (data.get("mode") or "full").strip().lower()

        # --------------------------------------------------
        # ğŸ”¹ summary / lite ëª¨ë“œ â†’ n-gram ê¸°ë°˜ ë¬¸ì„œ ë‹¨ìœ„ ìœ ì‚¬ìœ¨
        #    - ëŒ€ëŸ‰(ìˆ˜ì‹­~ìˆ˜ë°±ê°œ) íŒŒì¼ì¼ ë•Œ ë¹ ë¥´ê²Œ ìš”ì•½ìš©ìœ¼ë¡œ ì‚¬ìš©
        # --------------------------------------------------
        if mode in ("lite", "summary"):
            min_len = int(data.get("min_len", 6))

            try:
                summary = _dedup_inter_lite_v2(
                    files=files,
                    min_len=min_len,
                    max_chars=int(data.get("max_chars") or 8000),
                    n=max(4, min_len),
                    min_ratio=0.01,
                )

                # (ì„ íƒ) ì‚¬ìš©ëŸ‰ ë¡œê¹…
                try:
                    log_usage(_username_from_req(), "dedup_inter_lite", len(files))
                except Exception:
                    pass

                return jsonify(summary)
            except Exception as e:
                print("[dedup_inter summary] error:", e)
                return jsonify({"ok": False, "error": str(e)}), 500

        # --------------------------------------------------
        # ğŸ”¹ Full ëª¨ë“œ â†’ ê¸°ì¡´ ìƒì„¸ ê²°ê³¼ (ë¬¸ì¥ ë‹¨ìœ„ exact/similar)
        #    - íŒŒì¼ ìˆ˜ê°€ ìƒëŒ€ì ìœ¼ë¡œ ì ì„ ë•Œ(10~30ê°œ) ì‚¬ìš©
        # --------------------------------------------------
        min_len = int(data.get("min_len", 6))
        sim_th = float(data.get("sim_threshold", 0.85))

        exact, sims = _dedup_inter(
            files,
            min_len=min_len,
            sim_threshold=sim_th,
        )

        # (ì„ íƒ) ì‚¬ìš©ëŸ‰ ë¡œê¹…
        try:
            log_usage(_username_from_req(), "dedup_inter", len(files))
        except Exception:
            pass

        return jsonify({
            "mode": "full",
            "exact_groups": exact,
            "similar_pairs": sims,
        })

    except Exception as e:
        import traceback
        print("âœ˜ /dedup_inter ì˜¤ë¥˜:", e)
        traceback.print_exc()
        try:
            log_error(_username_from_req(), "/dedup_inter", 500, str(e))
        except Exception:
            pass
        return jsonify({"error": str(e)}), 500

@app.route("/policy_verify", methods=["POST", "OPTIONS"])
@require_user
def policy_verify():
    if request.method == "OPTIONS":
        return "", 200

    try:
        data = request.get_json(force=True, silent=True) or {}
        text = (data.get("text") or "").strip()
        if not text:
            return jsonify({"error": "No text provided"}), 400

        # [ADD] non-admin size guard (í•­ëª©ë‹¹ 100KB)
        limit_resp = enforce_size_limit_or_400(text)
        if limit_resp:
            return limit_resp

        # ì‹¬ì˜ ê·œì¹™ ê²€ì‚¬
        items = rule_scan(text)
        items = attach_reasons(items)

        # ì¶œì²˜ íƒœê·¸
        for it in items:
            it["source"] = "policy"

        # (ì„ íƒ) ì‚¬ìš©ëŸ‰ ë¡œê¹…
        try:
            log_usage(_username_from_req(), "policy", 1)
        except Exception:
            pass

        return jsonify({"results": items})

    except Exception as e:
        import traceback
        print("âœ˜ /policy_verify ì˜¤ë¥˜:", e)
        traceback.print_exc()
        # (ì„ íƒ) ì—ëŸ¬ ë¡œê¹…
        try:
            log_error(_username_from_req(), "/policy_verify", 500, str(e))
        except Exception:
            pass
        return jsonify({"error": str(e), "results": []}), 500


@app.after_request
def _after(resp):
    try:
        if resp.status_code >= 400:
            log_error(_username_from_req(), request.path, resp.status_code, getattr(resp, "data", b"")[:120])
    except Exception:
        pass
    return resp

from werkzeug.exceptions import HTTPException

@app.errorhandler(HTTPException)
def _http_error(e):
    # 404/401/405 ê°™ì€ ì •ìƒ HTTP ì˜¤ë¥˜ëŠ” ì›ë˜ ìƒíƒœì½”ë“œë¡œ ê·¸ëŒ€ë¡œ ë°˜í™˜
    return e

@app.errorhandler(Exception)
def _on_error(e):
    # 404/401/405 ê°™ì€ HTTP ì˜ˆì™¸ëŠ” ì›ë˜ ìƒíƒœì½”ë“œ ê·¸ëŒ€ë¡œ ë°˜í™˜í•´ì•¼ í•¨
    if isinstance(e, HTTPException):
        return e  # â˜… ë°˜ë“œì‹œ return e !!!
    # ê·¸ ì™¸ ì§„ì§œ ì—ëŸ¬ë§Œ 500 ì²˜ë¦¬
    return jsonify({"error": "internal"}), 500



# === 4) ê´€ë¦¬ì: ê¸°ê°„ ì¡°ì • API ===
# ìœ„ì¹˜: (â‘  /admin/approve ì•„ë˜) ë˜ëŠ” (â‘¡ if __name__ == "__main__": ë°”ë¡œ ìœ„)

@app.route("/admin/adjust_days", methods=["POST"])
@require_admin
def admin_adjust_days():
    """
    body: { "username": "trial@glefit", "days": -1 }  # ìŒìˆ˜/ì–‘ìˆ˜ ëª¨ë‘ í—ˆìš©
    """
    body = request.get_json(force=True, silent=True) or {}
    username   = (body.get("username") or "").strip()
    delta_days = int(body.get("days") or 0)
    if not username or delta_days == 0:
        return jsonify({"error": "username/days required"}), 400

    conn = sqlite3.connect(DB_PATH); cur = conn.cursor()
    cur.execute("SELECT paid_until FROM users WHERE username=?", (username,))
    row = cur.fetchone()
    if not row:
        conn.close(); return jsonify({"error":"user not found"}), 404

    now = datetime.utcnow()
    try:
        base = datetime.fromisoformat(row[0]) if row[0] else now
    except Exception:
        base = now

    new_until = base + timedelta(days=delta_days)
    cur.execute(
        "UPDATE users SET paid_until=?, is_active=? WHERE username=?",
        (new_until.isoformat(), 1 if new_until > now else 0, username)
    )
    conn.commit(); conn.close()
    return jsonify({"ok": True, "paid_until": new_until.isoformat()})

@app.get("/admin/usage_summary")
@require_admin
def admin_usage_summary():
    """
    ë°˜í™˜:
    {
      "usage":[ {username, verify, policy, dedup_inter, dedup_intra, files}, ... ],
      "errors":[ {username, errors, last}, ... ],
      "agreements":[ {username, agreed_at}, ... ]
    }
    """
    conn = sqlite3.connect(DB_PATH); cur = conn.cursor()

    # usage_logs ì§‘ê³„
    cur.execute("""
      SELECT username,
             SUM(CASE WHEN action='verify' THEN 1 ELSE 0 END) AS verify,
             SUM(CASE WHEN action='policy' THEN 1 ELSE 0 END) AS policy,
             SUM(CASE WHEN action='dedup_inter' THEN 1 ELSE 0 END) AS dedup_inter,
             SUM(CASE WHEN action='dedup_intra' THEN 1 ELSE 0 END) AS dedup_intra,
             SUM(files_count) AS files
      FROM usage_logs
      GROUP BY username
      ORDER BY username
    """)
    usage = []
    for row in cur.fetchall():
        usage.append({
            "username": row[0] or "",
            "verify": row[1] or 0,
            "policy": row[2] or 0,
            "dedup_inter": row[3] or 0,
            "dedup_intra": row[4] or 0,
            "files": row[5] or 0
        })

    # error_logs ì§‘ê³„(ìœ ì €ë³„ ê°œìˆ˜ + ë§ˆì§€ë§‰ ì‹œê°)
    cur.execute("""
      SELECT username, COUNT(*),
             MAX(created_at)
      FROM error_logs
      GROUP BY username
    """)
    errors = []
    for row in cur.fetchall():
        errors.append({
            "username": row[0] or "",
            "errors": row[1] or 0,
            "last": row[2] or ""
        })

    # ë™ì˜ ëª©ë¡
    cur.execute("SELECT username, agreed_at FROM agreements ORDER BY agreed_at DESC")
    agreements = [{"username": r[0], "agreed_at": r[1]} for r in cur.fetchall()]

    conn.close()
    return jsonify({"usage": usage, "errors": errors, "agreements": agreements})

@app.get("/admin/traffic_summary")
@require_admin
def admin_traffic_summary():
    """
    ì¿¼ë¦¬:
      granularity=day|week|month (ê¸°ë³¸: day)
      start=YYYY-MM-DD ISO(UTC)  ì˜ˆ: 2025-10-01
      end=YYYY-MM-DD   (í¬í•¨)
    ë°˜í™˜:
      { "series":[ { "bucket":"2025-10-21", "visits":n, "logins":m, "active_users":k }, ... ],
        "totals": { "visits":X, "logins":Y, "unique_users":Z } }
    """
    gran = (request.args.get("granularity") or "day").lower()
    start = (request.args.get("start") or "").strip()
    end   = (request.args.get("end") or "").strip()

    # ê¸°ë³¸ ê¸°ê°„: ìµœê·¼ 30ì¼ (í•œêµ­ ì‹œê°„ KST ê¸°ì¤€)
    today = (datetime.utcnow() + timedelta(hours=9)).date()
    if not start:
        start_date = today - timedelta(days=29)
    else:
        start_date = datetime.fromisoformat(start).date()
    if not end:
        end_date = today
    else:
        end_date = datetime.fromisoformat(end).date()

    # SQLite strftime íŒ¨í„´
    if gran == "month":
        fmt = "%Y-%m"
    elif gran == "week":
        # ISO week-year-weeknum
        fmt = "%Y-W%W"
    else:
        fmt = "%Y-%m-%d"

    conn = sqlite3.connect(DB_PATH); cur = conn.cursor()

    # visits: visit_logs ê¸°ì¤€
    cur.execute(f"""
      SELECT strftime('{fmt}', created_at), COUNT(*)
      FROM visit_logs
      WHERE date(created_at) BETWEEN date(?) AND date(?)
      GROUP BY 1
      ORDER BY 1
    """, (start_date.isoformat(), end_date.isoformat()))
    visit_rows = dict(cur.fetchall() or [])

    # logins: usage_logs(action='login')
    cur.execute(f"""
      SELECT strftime('{fmt}', created_at), COUNT(*)
      FROM usage_logs
      WHERE action='login'
        AND date(created_at) BETWEEN date(?) AND date(?)
      GROUP BY 1
      ORDER BY 1
    """, (start_date.isoformat(), end_date.isoformat()))
    login_rows = dict(cur.fetchall() or [])

    # active users(í•´ë‹¹ ê¸°ê°„ ë‚´ ë¡œê·¸ì¸í–ˆë˜ ìœ ë‹ˆí¬)
    cur.execute("""
      SELECT username
      FROM usage_logs
      WHERE action='login'
        AND date(created_at) BETWEEN date(?) AND date(?)
    """, (start_date.isoformat(), end_date.isoformat()))
    uniq = set([r[0] for r in (cur.fetchall() or []) if r and r[0]])

    # ë²„í‚· ìƒì„±
    series = []
    cur_d = start_date
    while cur_d <= end_date:
        if fmt == "%Y-%m":
            bucket = cur_d.strftime("%Y-%m")
            # ì›” ë§ê¹Œì§€ ì í”„
            next_d = (cur_d.replace(day=1) + timedelta(days=32)).replace(day=1) - timedelta(days=1)
            cur_d = (cur_d.replace(day=1) + timedelta(days=32)).replace(day=1)
        elif fmt == "%Y-W%W":
            bucket = cur_d.strftime("%Y-W%W")
            next_d = cur_d + timedelta(days=6)
            cur_d = cur_d + timedelta(days=7)
        else:
            bucket = cur_d.strftime("%Y-%m-%d")
            next_d = cur_d
            cur_d = cur_d + timedelta(days=1)

        series.append({
            "bucket": bucket,
            "visits": int(visit_rows.get(bucket, 0) or 0),
            "logins": int(login_rows.get(bucket, 0) or 0),
            "active_users": len(uniq) if fmt != "%Y-%m-%d" else None  # ì¼ë³„ active_usersëŠ” ë³´í†µ ë³„ë„ ì‚°ì¶œ
        })

    totals = {
        "visits": sum(v["visits"] for v in series),
        "logins": sum(v["logins"] for v in series),
        "unique_users": len(uniq),
    }
    conn.close()
    return jsonify({"series": series, "totals": totals})


@app.route("/admin/set_days_from_now", methods=["POST"])
@require_admin
def admin_set_days_from_now():
    """
    body: { "username": "trial@glefit", "days": 7 }  # 0 í—ˆìš©(ì¦‰ì‹œ ë§Œë£Œ)
    """
    body = request.get_json(force=True, silent=True) or {}
    username = (body.get("username") or "").strip()
    days     = int(body.get("days") or 0)
    if not username:
        return jsonify({"error":"username required"}), 400

    now = datetime.utcnow()
    new_until = now + timedelta(days=max(0, days))

    conn = sqlite3.connect(DB_PATH); cur = conn.cursor()
    cur.execute(
        "UPDATE users SET paid_until=?, is_active=? WHERE username=?",
        (new_until.isoformat(), 1 if days > 0 else 0, username)
    )
    conn.commit(); conn.close()
    return jsonify({"ok": True, "paid_until": new_until.isoformat()})

# === [ADD] BOARD API (list/add/edit/delete/pin/admin_delete_all) ===
from flask import Flask

def _default_daily_limit(username):
    # board_limits í…Œì´ë¸”ì—ì„œ per-user limitì„ ì½ê³ , ì—†ìœ¼ë©´ 2 ë¦¬í„´
    conn = sqlite3.connect(DB_PATH); cur = conn.cursor()
    cur.execute("SELECT daily_limit FROM board_limits WHERE username=?", (username,))
    row = cur.fetchone()
    conn.close()
    return int(row[0]) if row and row[0] is not None else 2

def _count_today_posts(username):
    day_start = int(datetime.utcnow().replace(hour=0, minute=0, second=0, microsecond=0).timestamp() * 1000)
    day_end   = int(datetime.utcnow().replace(hour=23, minute=59, second=59, microsecond=999000).timestamp() * 1000)
    conn = sqlite3.connect(DB_PATH); cur = conn.cursor()
    cur.execute("""
        SELECT COUNT(*) FROM board_posts
        WHERE username=? AND ts BETWEEN ? AND ? AND (hidden IS NULL OR hidden=0)
    """, (username, day_start, day_end))
    n = cur.fetchone()[0]
    conn.close()
    return int(n or 0)

# === INSERT BLOCK between line 1935 and 1936 ===
from uuid import uuid4

DEFAULT_DAILY = 2

def _board_daily_limit(username):
    try:
        conn = sqlite3.connect(DB_PATH); cur = conn.cursor()
        cur.execute("SELECT daily_limit FROM board_limits WHERE username=?", (username,))
        row = cur.fetchone(); conn.close()
        if row and row[0] is not None:
            return int(row[0]) or DEFAULT_DAILY
    except Exception:
        pass
    return DEFAULT_DAILY

def _count_posts_today(username):
    try:
        start = int(datetime.utcnow().replace(hour=0,minute=0,second=0,microsecond=0).timestamp()*1000)
        end   = int(datetime.utcnow().replace(hour=23,minute=59,second=59,microsecond=999999).timestamp()*1000)
        conn = sqlite3.connect(DB_PATH); cur = conn.cursor()
        cur.execute("SELECT COUNT(1) FROM board_posts WHERE username=? AND ts BETWEEN ? AND ?",
                    (username, start, end))
        c = cur.fetchone()[0]; conn.close()
        return int(c or 0)
    except Exception:
        return 0

@app.get("/board/posts")
def board_posts_list():
    """ì½ê¸° ì „ìš©: ìµœê·¼ 100ê±´, pinned ìš°ì„ """
    conn = sqlite3.connect(DB_PATH); cur = conn.cursor()
    cur.execute("""
        SELECT id, username, text, pinned, ts
        FROM board_posts
        ORDER BY pinned DESC, ts DESC
        LIMIT 100
    """)
    rows = cur.fetchall(); conn.close()
    posts = [{"id":r[0], "user":r[1], "text":r[2], "pinned":bool(r[3]), "ts":int(r[4])} for r in rows]
    return jsonify({"posts": posts})

@app.post("/board/posts")
def board_posts_add():
    """ì‘ì„±: username/password ì¸ì¦ + ì¼ì¼ ì œí•œ"""
    payload = request.get_json(force=True, silent=True) or {}
    ok, uname, is_admin = verify_board_auth(payload)
    if not ok:
        return jsonify({"error":"auth"}), 401

    text = (payload.get("text") or "").strip()
    if not text or len(text) > 60:
        return jsonify({"error":"length"}), 400

    # ì¼ì¼ ì œí•œ(ê´€ë¦¬ìëŠ” ë¬´ì œí•œ)
    if not is_admin:
        used = _count_posts_today(uname)
        if used >= _board_daily_limit(uname):
            return jsonify({"error":"limit"}), 429

    pid = "p_" + uuid4().hex[:10]
    now = int(time.time()*1000)
    conn = sqlite3.connect(DB_PATH); cur = conn.cursor()
    cur.execute("INSERT INTO board_posts (id, username, text, pinned, ts) VALUES (?,?,?,?,?)",
                (pid, uname, text, 0, now))
    conn.commit(); conn.close()
    return jsonify({"ok":True, "id":pid, "ts":now})

@app.put("/board/posts/<pid>")
def board_posts_edit(pid):
    payload = request.get_json(force=True, silent=True) or {}
    ok, uname, is_admin = verify_board_auth(payload)
    if not ok:
        return jsonify({"error":"auth"}), 401
    text = (payload.get("text") or "").strip()
    if not text or len(text) > 60:
        return jsonify({"error":"length"}), 400

    conn = sqlite3.connect(DB_PATH); cur = conn.cursor()
    cur.execute("SELECT username FROM board_posts WHERE id=?", (pid,))
    row = cur.fetchone()
    if not row:
        conn.close(); return jsonify({"error":"not_found"}), 404
    if (row[0] != uname) and (not is_admin):
        conn.close(); return jsonify({"error":"forbidden"}), 403

    cur.execute("UPDATE board_posts SET text=? WHERE id=?", (text, pid))
    conn.commit(); conn.close()
    return jsonify({"ok":True})

@app.delete("/board/posts/<pid>")
def board_posts_delete(pid):
    payload = request.get_json(force=True, silent=True) or {}
    ok, uname, is_admin = verify_board_auth(payload)
    if not ok:
        return jsonify({"error":"auth"}), 401

    conn = sqlite3.connect(DB_PATH); cur = conn.cursor()
    cur.execute("SELECT username FROM board_posts WHERE id=?", (pid,))
    row = cur.fetchone()
    if not row:
        conn.close(); return jsonify({"error":"not_found"}), 404
    if (row[0] != uname) and (not is_admin):
        conn.close(); return jsonify({"error":"forbidden"}), 403

    cur.execute("DELETE FROM board_posts WHERE id=?", (pid,))
    conn.commit(); conn.close()
    return jsonify({"ok":True})

@app.post("/board/pin/<pid>")
def board_posts_pin(pid):
    payload = request.get_json(force=True, silent=True) or {}
    ok, uname, is_admin = verify_board_auth(payload)
    if not ok or not is_admin:
        return jsonify({"error":"admin_only"}), 403

    conn = sqlite3.connect(DB_PATH); cur = conn.cursor()
    cur.execute("SELECT pinned FROM board_posts WHERE id=?", (pid,))
    row = cur.fetchone()
    if not row:
        conn.close(); return jsonify({"error":"not_found"}), 404
    new_pin = 0 if row[0] else 1
    cur.execute("UPDATE board_posts SET pinned=? WHERE id=?", (new_pin, pid))
    conn.commit(); conn.close()
    return jsonify({"ok":True, "pinned": bool(new_pin)})

# === [ADD] ê²Œì‹œíŒ ë¯¸ë‹ˆë¡œê·¸ì¸ ê´€ë¦¬ì ì—¬ë¶€ í™•ì¸ ===
@app.post("/board/auth_check")
def board_auth_check():
    payload = request.get_json(force=True, silent=True) or {}
    ok, uname, is_admin = verify_board_auth(payload)
    if not ok:
        return jsonify({"ok": False}), 401
    return jsonify({"ok": True, "username": uname, "is_admin": bool(is_admin)})

@app.post("/admin/board/limit")
def board_limit_set():
    """ê´€ë¦¬ì: íŠ¹ì • ID ì¼ì¼ ì‘ì„± ì œí•œ ë³€ê²½"""
    payload = request.get_json(force=True, silent=True) or {}
    ok, uname, is_admin = verify_board_auth(payload)
    if not ok or not is_admin:
        return jsonify({"error":"admin_only"}), 403
    target = (payload.get("target") or "").strip().lower()
    n = int(payload.get("limit") or DEFAULT_DAILY)
    if not target:
        return jsonify({"error":"target"}), 400

    conn = sqlite3.connect(DB_PATH); cur = conn.cursor()
    cur.execute("""
        INSERT INTO board_limits (username, daily_limit)
        VALUES (?,?)
        ON CONFLICT(username) DO UPDATE SET daily_limit=excluded.daily_limit
    """, (target, n))
    conn.commit(); conn.close()
    return jsonify({"ok":True, "username": target, "limit": n})

# === Board APIs ===
@app.route("/board/list", methods=["GET"])
def board_list():
    conn = sqlite3.connect(DB_PATH); cur = conn.cursor()
    cur.execute("""
        SELECT id, username, text, pinned, ts
        FROM board_posts
        WHERE (hidden IS NULL OR hidden=0)
        ORDER BY pinned DESC, ts DESC
        LIMIT 200
    """)
    rows = cur.fetchall(); conn.close()
    items = [{"id": r[0], "user": r[1], "text": r[2], "pinned": bool(r[3]), "ts": int(r[4] or 0)} for r in rows]
    return jsonify({"ok": True, "items": items})

@app.route("/board/add", methods=["POST"])
@require_user
def board_add():
    data = (request.get_json(silent=True) or {})
    text = (data.get("text") or "").strip()
    if not text: return jsonify({"ok": False, "error":"EMPTY"}), 400
    if len(text) > 60: return jsonify({"ok": False, "error":"TOO_LONG"}), 400

    uname = _username_from_req()
    user = _get_user(uname)
    is_admin = (user or {}).get("role") == "admin"

    # ì‘ì„± ì •ì§€ ì‚¬ìš©ìë©´ ì¦‰ì‹œ ì°¨ë‹¨
    conn = sqlite3.connect(DB_PATH); cur = conn.cursor()
    r = cur.execute("SELECT IFNULL(posting_blocked,0) FROM users WHERE username=?", (uname,)).fetchone()
    if r and int(r[0]) == 1:
        conn.close()
        return jsonify({"ok": False, "error": "BLOCKED"}), 403
    conn.close()

    # === ì¼ì¼ ì œí•œ: KST ìì • ê¸°ì¤€ (ì‚­ì œê¸€ í¬í•¨ ì¹´ìš´íŠ¸) ===
    now_utc = datetime.utcnow()
    now_kst = now_utc + timedelta(hours=9)
    kst_midnight = datetime(now_kst.year, now_kst.month, now_kst.day, 0, 0, 0)
    boundary_utc = kst_midnight - timedelta(hours=9)
    boundary_ms = int(boundary_utc.timestamp() * 1000)

    # ê´€ë¦¬ìë©´ ë¬´ì œí•œ í†µê³¼
    if not is_admin:
        # per-user í•œë„: board_limits.daily_limit ì‚¬ìš©, ì—†ìœ¼ë©´ ê¸°ë³¸ 2íšŒ
        try:
            daily_limit = _default_daily_limit(uname)  # í•¨ìˆ˜ ì •ì˜: server.pyì— ì´ë¯¸ ìˆìŒ
        except Exception:
            daily_limit = 2

        conn = sqlite3.connect(DB_PATH); cur = conn.cursor()
        cur.execute(
            "SELECT COUNT(*) FROM board_posts WHERE username=? AND ts>=?",
            (uname, boundary_ms)
        )
        used = int(cur.fetchone()[0] or 0)
        conn.close()

        if used >= int(daily_limit):
            return jsonify({
                "ok": False,
                "error": "LIMIT",
                "used": used,
                "limit": int(daily_limit),
                "reset_at_kst": int(kst_midnight.timestamp() * 1000)
            }), 400
    # (ê´€ë¦¬ìëŠ” ìœ„ ì œí•œì„ ê±´ë„ˆëœ€)


    # === ì €ì¥ ===
    pid = f"p_{int(time.time()*1000)}_{uuid.uuid4().hex[:5]}"
    ts  = int(time.time()*1000)

    conn = sqlite3.connect(DB_PATH); cur = conn.cursor()
    cur.execute(
        "INSERT INTO board_posts (id, username, text, pinned, ts, hidden) VALUES (?,?,?,?,?,0)",
        (pid, uname, text, 0, ts)
    )
    conn.commit(); conn.close()

    return jsonify({
        "ok": True,
        "item": {"id": pid, "user": uname, "text": text, "pinned": False, "ts": ts}
    })


@app.route("/board/edit", methods=["POST"])
@require_user
def board_edit():
    data = (request.get_json(silent=True) or {})
    pid  = data.get("id") or ""
    text = (data.get("text") or "").strip()
    if not pid or not text: return jsonify({"ok": False, "error":"BAD_REQ"}), 400
    if len(text) > 60: return jsonify({"ok": False, "error":"TOO_LONG"}), 400

    uname = _username_from_req()
    user = _get_user(uname)
    is_admin = (user or {}).get("role") == "admin"

    conn = sqlite3.connect(DB_PATH); cur = conn.cursor()
    cur.execute("SELECT username FROM board_posts WHERE id=? AND (hidden IS NULL OR hidden=0)", (pid,))
    row = cur.fetchone()
    if not row: 
        conn.close(); return jsonify({"ok": False, "error": "NOT_FOUND"}), 404

    if (not is_admin) and (row[0] != uname):
        conn.close(); return jsonify({"ok": False, "error": "FORBIDDEN"}), 403

    cur.execute("UPDATE board_posts SET text=? WHERE id=?", (text, pid))
    conn.commit(); conn.close()
    return jsonify({"ok": True})

@app.route("/board/delete", methods=["POST"])
@require_user
def board_delete():
    data = (request.get_json(silent=True) or {})
    pid  = data.get("id") or ""
    if not pid: return jsonify({"ok": False, "error":"BAD_REQ"}), 400

    uname = _username_from_req()
    user = _get_user(uname)
    is_admin = (user or {}).get("role") == "admin"

    conn = sqlite3.connect(DB_PATH); cur = conn.cursor()
    cur.execute("SELECT username FROM board_posts WHERE id=? AND (hidden IS NULL OR hidden=0)", (pid,))
    row = cur.fetchone()
    if not row: 
        conn.close(); return jsonify({"ok": False, "error": "NOT_FOUND"}), 404

    if (not is_admin) and (row[0] != uname):
        conn.close(); return jsonify({"ok": False, "error": "FORBIDDEN"}), 403

    cur.execute("UPDATE board_posts SET hidden=1 WHERE id=?", (pid,))
    conn.commit(); conn.close()
    return jsonify({"ok": True})

@app.route("/board/toggle_pin", methods=["POST"])
@require_admin
def board_toggle_pin():
    data = (request.get_json(silent=True) or {})
    pid  = data.get("id") or ""
    if not pid: return jsonify({"ok": False, "error":"BAD_REQ"}), 400

    conn = sqlite3.connect(DB_PATH); cur = conn.cursor()
    cur.execute("SELECT pinned FROM board_posts WHERE id=? AND (hidden IS NULL OR hidden=0)", (pid,))
    row = cur.fetchone()
    if not row:
        conn.close(); return jsonify({"ok": False, "error":"NOT_FOUND"}), 404

    new_pin = 0 if (row[0] or 0) else 1
    cur.execute("UPDATE board_posts SET pinned=? WHERE id=?", (new_pin, pid))
    conn.commit(); conn.close()
    return jsonify({"ok": True, "pinned": bool(new_pin)})

@app.route("/board/admin/delete_all", methods=["POST"])
@require_admin
def board_admin_delete_all():
    conn = sqlite3.connect(DB_PATH); cur = conn.cursor()
    cur.execute("UPDATE board_posts SET hidden=1 WHERE (hidden IS NULL OR hidden=0)")
    conn.commit(); conn.close()
    return jsonify({"ok": True, "all_deleted": True})

# === END INSERT BLOCK ===

if __name__ == "__main__":
    port = int(os.getenv("PORT", "5000"))
    app.run(host="0.0.0.0", port=port, debug=False)
