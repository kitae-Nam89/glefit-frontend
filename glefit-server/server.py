# -*- coding: utf-8 -*-
import os, re, json, time, sqlite3, unicodedata
from dotenv import load_dotenv
from flask import Flask, request, jsonify
from flask_cors import CORS

# [ADD] Auth imports
from datetime import datetime, timedelta
from functools import wraps
import jwt
from passlib.hash import pbkdf2_sha256 as bcrypt


from openai import OpenAI
import yaml
from collections import defaultdict
# --- [LOCAL SPELLCHECK] begin ---
import os, re
from symspellpy import SymSpell, Verbosity
import uuid

_HANGUL_RE = re.compile(r"[Í∞Ä-Ìû£]+")
DATA_DIR = "data"
KO_WORDS_PATH = os.path.join(DATA_DIR, "ko_words.txt")   # "Îã®Ïñ¥<TAB>ÎπàÎèÑ"
WHITE_PATH    = os.path.join(DATA_DIR, "whitelist.txt")  # Ï§ÑÎãπ 1Í∞ú Îã®Ïñ¥

_sym = None
_whitelist = set()

def _init_symspell():
    import os
    os.makedirs(DATA_DIR, exist_ok=True)
    global _sym, _whitelist
    _sym = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)

    if os.path.exists(KO_WORDS_PATH):
        _sym.load_dictionary(KO_WORDS_PATH, term_index=0, count_index=1,
                             separator="\t", encoding="utf-8")

    custom_path = os.path.join(DATA_DIR, "custom_words.txt")
    if os.path.exists(custom_path):
        _sym.load_dictionary(custom_path, term_index=0, count_index=1,
                             separator="\t", encoding="utf-8")

    _whitelist = set()
    if os.path.exists(WHITE_PATH):
        for enc in ("utf-8","utf-8-sig","cp949","euc-kr"):
            try:
                with open(WHITE_PATH,"r",encoding=enc) as f:
                    _whitelist = {ln.strip() for ln in f if ln.strip()}
                break
            except UnicodeDecodeError:
                continue

    print(f"üî§ SymSpell ready: words={len(_sym.words)}, whitelist={len(_whitelist)}")

def _token_spans_ko(text: str):
    """ÌïúÍ∏Ä Ïó∞ÏÜçÍµ¨Í∞ÑÏùÑ ÌÜ†ÌÅ∞ÏúºÎ°ú ÎΩëÏïÑ (start, end, token)"""
    for m in _HANGUL_RE.finditer(text or ""):
        yield m.start(), m.end(), m.group()
# --- [LOCAL SPELLCHECK] end ---


# ================== Í∏∞Î≥∏ ÏÑ§Ï†ï ==================
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    raise RuntimeError("OPENAI_API_KEYÍ∞Ä ÏÑ§Ï†ïÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§ (.env ÌôïÏù∏)")

client = OpenAI(api_key=OPENAI_API_KEY, timeout=30)

MODEL = "gpt-4o"
DB_PATH = "rewrite.db"
MAX_CHARS_PER_CHUNK = 4000

# [ADD] JWT config
JWT_SECRET = os.getenv("JWT_SECRET", "CHANGE_ME_TO_RANDOM_SECRET")
JWT_ALG = "HS256"
# [ADD] ensure users table
def ensure_users_table():
    conn = sqlite3.connect(DB_PATH)
    cur = conn.cursor()
    cur.execute("""
    CREATE TABLE IF NOT EXISTS users (
        id INTEGER PRIMARY KEY AUTOINCREMENT,
        username TEXT UNIQUE NOT NULL,
        password_hash TEXT NOT NULL,
        is_active INTEGER DEFAULT 0,
        paid_until TEXT,
        role TEXT DEFAULT 'user',
        notes TEXT
    )
    """)
    conn.commit()
    conn.close()

ensure_users_table()

def migrate_users_table():
    conn = sqlite3.connect(DB_PATH)
    cur = conn.cursor()
    # Í≥†Í∞ù Ï†ëÏÜç Ï£ºÏÜå Î≥¥Í¥Ä(ÏÑ†ÌÉù)
    try:
        cur.execute("ALTER TABLE users ADD COLUMN site_url TEXT")
    except Exception:
        pass
    # ÏÉùÏÑ± ÏãúÍ∞Å(Í¥ÄÎ¶¨ Ìé∏Ïùò)
    try:
        cur.execute("ALTER TABLE users ADD COLUMN created_at TEXT")
    except Exception:
        pass

    # ‚òÖ‚òÖ‚òÖ ÎèôÏãúÏ†ëÏÜç Ï†úÏñ¥Ïö© Ïª¨Îüº Ï∂îÍ∞Ä ‚òÖ‚òÖ‚òÖ
    try:
        cur.execute("ALTER TABLE users ADD COLUMN token_version INTEGER DEFAULT 0")
    except Exception:
        pass
    try:
        cur.execute("ALTER TABLE users ADD COLUMN last_jti TEXT")
    except Exception:
        pass

    conn.commit()
    conn.close()

migrate_users_table()

# [ADD] user helpers & guards
def _get_user(username: str):
    conn = sqlite3.connect(DB_PATH)
    cur = conn.cursor()
    cur.execute("SELECT id, username, password_hash, is_active, paid_until, role FROM users WHERE username = ?", (username,))
    row = cur.fetchone()
    conn.close()
    if not row:
        return None
    return {
        "id": row[0], "username": row[1], "password_hash": row[2],
        "is_active": bool(row[3]),
        "paid_until": row[4],
        "role": row[5] or "user",
    }

def _remaining_days(paid_until: str) -> int:
    if not paid_until:
        return 0
    try:
        delta = datetime.fromisoformat(paid_until) - datetime.utcnow()
        return max(0, delta.days)
    except Exception:
        return 0

def _is_paid_and_active(u: dict) -> bool:
    if not u or not u.get("is_active"):
        return False
    try:
        pu = u.get("paid_until")
        if not pu:
            return False
        return datetime.utcnow() <= datetime.fromisoformat(pu)
    except Exception:
        return False

def _origin_allowed(user_site_url: str) -> bool:
    if not user_site_url:
        return True  # Ï£ºÏÜå Ï†úÌïú Ïïà Í±¥ Í≥ÑÏ†ï
    origin = (request.headers.get("Origin") or "").lower()
    referer = (request.headers.get("Referer") or "").lower()
    target = user_site_url.lower()
    return (target in origin) or (target in referer)

def require_user(fn):
    @wraps(fn)
    def wrapper(*args, **kwargs):
        if request.method == "OPTIONS":
            return "", 200

        auth = request.headers.get("Authorization","").strip()
        if not auth.startswith("Bearer "):
            return jsonify({"error":"Unauthorized"}), 401
        token = auth.split(" ",1)[1]

        try:
            data = jwt.decode(token, JWT_SECRET, algorithms=[JWT_ALG])
        except Exception:
            return jsonify({"error":"Invalid token"}), 401

        username = data.get("sub","")
        tok_ver  = int(data.get("ver", 0) or 0)
        tok_jti  = data.get("jti","") or ""

        # DBÏóêÏÑú ÏµúÏã† ÏÉÅÌÉú(ÌôúÏÑ±/Í∏∞Í∞Ñ/ÏÇ¨Ïù¥Ìä∏/Î≤ÑÏ†Ñ/JTI) ÌôïÏù∏
        conn = sqlite3.connect(DB_PATH)
        cur  = conn.cursor()
        cur.execute("SELECT is_active, paid_until, site_url, token_version, last_jti FROM users WHERE username=?", (username,))
        row = cur.fetchone()
        conn.close()

        if not row:
            return jsonify({"error":"Unauthorized"}), 401

        is_active, paid_until, site_url, db_ver, db_jti = row[0], row[1], (row[2] or ""), int(row[3] or 0), (row[4] or "")

        # Í≤∞Ï†ú/Í∏∞Í∞Ñ/ÌôúÏÑ± Ï≤¥ÌÅ¨
        try:
            if not is_active or not paid_until or datetime.utcnow() > datetime.fromisoformat(paid_until):
                return jsonify({"error":"Payment required or expired"}), 402
        except Exception:
            return jsonify({"error":"Payment required or expired"}), 402

        # ‚òÖ Îã®Ïùº Î°úÍ∑∏Ïù∏ Í∞ïÏ†ú: ÌÜ†ÌÅ∞Ïùò ver/jtiÍ∞Ä DBÏùò ÏµúÏã†Í∞íÍ≥º ÏùºÏπòÌï¥Ïïº ÌÜµÍ≥º
        if tok_ver != db_ver or (db_jti and tok_jti != db_jti):
            return jsonify({"error":"Session invalidated. Please log in again.", "code":"SESSION"}), 401

        # ÎèÑÎ©îÏù∏ Ï†úÌïú(ÏòµÏÖò)
        if site_url:
            origin  = (request.headers.get("Origin") or "").lower()
            referer = (request.headers.get("Referer") or "").lower()
            if site_url.lower() not in origin and site_url.lower() not in referer:
                return jsonify({"error":"Origin not allowed"}), 403

        return fn(*args, **kwargs)
    return wrapper

def require_admin(fn):
    @wraps(fn)
    def wrapper(*args, **kwargs):
        auth = request.headers.get("Authorization","").strip()
        if not auth.startswith("Bearer "):
            return jsonify({"error":"Unauthorized"}), 401
        token = auth.split(" ",1)[1]

        try:
            data = jwt.decode(token, JWT_SECRET, algorithms=[JWT_ALG])
        except Exception:
            return jsonify({"error":"Invalid token"}), 401

        username = data.get("sub","")
        tok_ver  = int(data.get("ver", 0) or 0)
        tok_jti  = data.get("jti","") or ""

        conn = sqlite3.connect(DB_PATH)
        cur  = conn.cursor()
        cur.execute("SELECT role, is_active, paid_until, token_version, last_jti FROM users WHERE username=?", (username,))
        row = cur.fetchone()
        conn.close()

        if not row:
            return jsonify({"error":"Unauthorized"}), 401

        role, is_active, paid_until, db_ver, db_jti = (row[0] or "user"), row[1], row[2], int(row[3] or 0), (row[4] or "")

        # Í≤∞Ï†ú/Í∏∞Í∞Ñ/ÌôúÏÑ± Ï≤¥ÌÅ¨
        try:
            if not is_active or not paid_until or datetime.utcnow() > datetime.fromisoformat(paid_until):
                return jsonify({"error":"Payment required or expired"}), 402
        except Exception:
            return jsonify({"error":"Payment required or expired"}), 402

        # Îã®Ïùº Î°úÍ∑∏Ïù∏ Í∞ïÏ†ú
        if tok_ver != db_ver or (db_jti and tok_jti != db_jti):
            return jsonify({"error":"Session invalidated. Please log in again.", "code":"SESSION"}), 401

        if role != "admin":
            return jsonify({"error":"Admin only"}), 403

        return fn(*args, **kwargs)
    return wrapper


RULE_PATH = os.getenv("RULE_PATH", "kr-medhealth.yaml")
RULES = {}   # {"rules":[{...}], ...}
RULES_INDEX = {}  # {rule_id: rule_obj}

# ================== Ïú†Ìã∏ ==================
def search_db(sentence):
    try:
        conn = sqlite3.connect(DB_PATH)
        cur = conn.cursor()
        cur.execute("SELECT alternative FROM sentences WHERE original = ?", (sentence,))
        rows = cur.fetchall()
        conn.close()
        return [r[0] for r in rows] if rows else []
    except Exception as e:
        print("DB Í≤ÄÏÉâ Ïò§Î•ò:", e)
        return []

def find_all(text, substring, start_from=0):
    out = []
    i = start_from
    L = len(substring)
    if not substring:
        return out
    while True:
        i = text.find(substring, i)
        if i == -1:
            break
        out.append(i)
        i += L
    return out

def chunk_text_with_offsets(text, max_chars=MAX_CHARS_PER_CHUNK):
    chunks = []
    n = len(text)
    i = 0
    while i < n:
        end = min(i + max_chars, n)
        if end < n:
            cut = text.rfind("\n", i, end)
            if cut == -1 or cut <= i + int(max_chars * 0.3):
                cut = end
        else:
            cut = end
        chunks.append((i, text[i:cut]))
        i = cut
    return chunks

_json_array_pattern = re.compile(r"\[\s*{.*?}\s*\]", re.DOTALL)
def extract_json_array(s):
    if not s:
        return []
    s = re.sub(r"^```json\s*|\s*```$", "", s.strip(), flags=re.IGNORECASE | re.DOTALL)
    m = _json_array_pattern.search(s)
    if not m:
        try:
            j = json.loads(s)
            return j if isinstance(j, list) else []
        except:
            return []
    try:
        return json.loads(m.group(0))
    except Exception as e:
        print("JSON ÌååÏã± Ïã§Ìå®:", e)
        return []

def basic_kr_sentence_split(text):
    parts = re.split(r"([\.?!]+|\n+)", text)
    out, buf = [], ""
    for p in parts:
        if p is None:
            continue
        buf += p
        if p.endswith((".", "?", "!", "\n")) or p.strip() == "":
            s = buf.strip()
            if s:
                out.append(s)
            buf = ""
    if buf.strip():
        out.append(buf.strip())
    return out

def add_context(text, start, length, window=8):
    before = text[max(0, start - window): start]
    after  = text[start + length: start + length + window]
    return before, after

def gpt_call(model, messages):
    return client.chat.completions.create(model=model, messages=messages)

# --- Í≥µÎ∞±/Î¨∏Ïûê Ï†ïÍ∑úÌôî & Í≥µÎ∞±Î¨¥Ïãú ÌÇ§ÏõåÎìúÏö© ---
def kr_norm(s: str) -> str:
    s = unicodedata.normalize("NFKC", s or "")
    s = s.replace("%", "ÌçºÏÑºÌä∏")
    s = re.sub(r"\s+", " ", s)
    return s

def spacing_agnostic_regex(kw: str) -> str:
    parts = list(kw)
    return r"\s*".join(map(re.escape, parts))

# ================== Ïã¨Ïùò Í∑úÏπô Î°úÎî©/Ïä§Ï∫î ==================
def load_rules(path=RULE_PATH):
    global RULES, RULES_INDEX
    try:
        with open(path, "r", encoding="utf-8") as f:
            RULES = yaml.safe_load(f) or {}
        if not isinstance(RULES, dict):
            RULES = {}
        print(f"‚úÖ Ïã¨Ïùò Í∑úÏπô Î°úÎìú ÏôÑÎ£å: {path}")
        pack = RULES.get("pack")
        ver = RULES.get("version")
        topics = RULES.get("topics")
        if pack or ver or topics:
            print(f"  - pack={pack}, version={ver}, topics={topics}")
        RULES_INDEX = {}
        for r in (RULES.get("rules") or []):
            rid = (r or {}).get("id")
            if rid:
                RULES_INDEX[str(rid)] = r
        print(f"  - rules indexed: {len(RULES_INDEX)}")
    except Exception as e:
        print("[WARN] Ïã¨Ïùò Í∑úÏπô Î°úÎìú Ïã§Ìå®:", e)
        RULES, RULES_INDEX = {}, {}


load_rules()

# === (ADD) ÏÇ¨Ïú†/Î≤ïÎ†π Îß§Ìïë Ïú†Ìã∏ (YAML meta ÏÇ¨Ïö©) ====================
def _load_reason_index_from_rules():
    meta = (RULES or {}).get("meta") or {}
    cats = meta.get("reason_categories") or {}
    mapping = []
    for ent in meta.get("rule_category_by_prefix") or []:
        try:
            rx = re.compile(ent.get("pattern", ""), re.IGNORECASE)
            mapping.append((rx, ent.get("category")))
        except re.error:
            pass
    return cats, mapping

REASON_CATS, REASON_MAP = _load_reason_index_from_rules()

def _reason_for_rule_id(rule_id: str):
    rid = rule_id or ""
    for rx, cat in (REASON_MAP or []):
        if rx.search(rid):
            info = (REASON_CATS or {}).get(cat) or {}
            reason = info.get("reason")
            legal  = info.get("legal")
            url    = info.get("legal_url")
            if not reason:
                return None, None
            small = None
            if legal and url:
                small = f"<small style='color:#777'>(Í∑ºÍ±∞: {legal} ¬∑ <a href=\"{url}\" target=\"_blank\">Î≤ïÎ†π</a>)</small>"
            elif legal:
                small = f"<small style='color:#777'>(Í∑ºÍ±∞: {legal})</small>"
            return f"ÏÇ¨Ïú†: {reason}", small
    return None, None

_rule_id_rx = re.compile(r"\(rule:([^)]+)\)")
def attach_reasons(items):
    """
    Í≤∞Í≥º Ìï≠Î™©Ïóê ÏÇ¨Ïö©Ïûê ÏπúÌôîÏ†Å ÏÇ¨Ïú†/Ï∂úÏ≤òÎ•º Î∂ÄÏ∞©ÌïòÍ≥†,
    ÎÇ¥Î∂Ä rule ID ÎÖ∏Ï∂úÏùÑ Ï†úÍ±∞ÌïúÎã§.
    Ïö∞ÏÑ†ÏàúÏúÑ:
      1) RULES_INDEX[rule_id]Ïùò rationale / legal_ref
      2) meta(reason_categories/rule_category_by_prefix) Ìè¥Î∞±
    """
    for it in items:
        rid = it.get("rule_id")
        if not rid:
            m = _rule_id_rx.search(it.get("reason","") or "")
            rid = m.group(1) if m else None

        reason_line = None
        legal_small = None

        # 1) Í∑úÏπô Î≥∏Î¨∏ÏóêÏÑú ÏßÅÏ†ë Ï∑®Îìù
        rule = RULES_INDEX.get(str(rid)) if rid else None
        if rule:
            rationale = rule.get("rationale") or rule.get("description") or ""
            legal     = rule.get("legal_ref")  or ""   # YAMLÏóê ÏÑ†ÌÉùÏ†ÅÏúºÎ°ú Ï∂îÍ∞Ä
            if rationale:
                reason_line = rationale
            if legal:
                legal_small = f"<small style='color:#777'>(Ï∂úÏ≤ò: {legal})</small>"

        # 2) Ìè¥Î∞±: meta Ïπ¥ÌÖåÍ≥†Î¶¨ Îß§Ìïë ÏÇ¨Ïö©
        if not reason_line:
            rline, small = _reason_for_rule_id(rid or "")
            if rline:
                # rline Ïòà: "ÏÇ¨Ïú†: ‚Ä¶" ÌòïÌÉú ‚Üí ÌÜµÏùº ÏúÑÌï¥ Ï†ëÎëêÏñ¥ Ï†úÍ±∞
                reason_line = rline.replace("ÏÇ¨Ïú†:", "").strip()
            if small:
                legal_small = small

        if reason_line:
            it["reason_line"] = reason_line
        if legal_small:
            it["legal_small"] = legal_small

        # ÌôîÎ©¥ ÎÖ∏Ï∂úÏö© reason Î¨∏ÏûêÏó¥ÏóêÏÑú (rule:XXX) Ï†úÍ±∞
        if it.get("reason"):
            it["reason"] = re.sub(_rule_id_rx, "", it["reason"]).strip()
    return items# =====================================================================

def _iter_rule_patterns(rule):
    patterns = ((rule or {}).get("patterns") or {}).get("any") or []
    for p in patterns:
        if not isinstance(p, dict):
            continue
        if p.get("type") in ("keyword", "regex", "phrase"):
            yield p

def _compile_regex_from_pattern(pat: dict):
    ptype = pat.get("type")
    flags = 0
    if str(pat.get("flags","")).lower().find("i") >= 0 or pat.get("casefold", True):
        flags |= re.IGNORECASE

    if ptype == "keyword":
        val = kr_norm(pat.get("value",""))
        if not val:
            return None, None
        if pat.get("spacing_agnostic", True):
            rx = spacing_agnostic_regex(val)
        else:
            rx = re.escape(val)
        return re.compile(rx, flags), f"kw:{val}"

    if ptype == "regex":
        val = pat.get("value","")
        if not val:
            return None, None
        return re.compile(val, flags), "rx"

    if ptype == "phrase":
        terms = [kr_norm(t) for t in (pat.get("terms") or []) if t]
        if len(terms) < 2:
            return None, None
        max_gap = int(pat.get("max_gap", 6))
        terms_rx = [spacing_agnostic_regex(t) for t in terms]
        rx = terms_rx[0]
        for nxt in terms_rx[1:]:
            rx += rf".{{0,{max_gap}}}" + nxt
        return re.compile(rx, flags), f"ph:{'|'.join(terms)}"
    return None, None

def _rule_excepts(rule):
    ex = ((rule or {}).get("except") or {}).get("any") or []
    out = []
    for r in ex:
        try:
            out.append(re.compile(r, re.IGNORECASE))
        except re.error:
            pass
    return out

def _scan_with_rule(text, rule):
    hits = []
    exceptors = _rule_excepts(rule)
    window = int((rule or {}).get("except_window", 14))
    for p in _iter_rule_patterns(rule):
        rx, _ = _compile_regex_from_pattern(p)
        if not rx:
            continue
        for m in rx.finditer(text):
            s, e = m.start(), m.end()
            seg = text[s:e]
            if exceptors:
                ctx = text[max(0, s - window): min(len(text), e + window)]
                if any(ex.search(ctx) for ex in exceptors):
                    continue
            hits.append((s, e, seg))
    return hits

def rule_scan(text):
    rules = (RULES or {}).get("rules") or []
    items = []
    gid = 0
    for rule in rules:
        rid   = rule.get("id", "")
        topic = rule.get("topic", "")
        sev   = (rule.get("severity") or "warn").lower()
        rationale = rule.get("rationale") or rule.get("description") or ""
        actions   = rule.get("actions") or []
        rule_type = "Ïã¨ÏùòÏúÑÎ∞ò" if sev == "block" else "Ï£ºÏùòÌëúÌòÑ"

        for (s, e, seg) in _scan_with_rule(text, rule):
            before, after = add_context(text, s, e - s)
            sugg = []
            if any(str(a).startswith("suggest:") for a in actions):
                human = "ÌëúÌòÑ ÏôÑÌôî/Í∑ºÍ±∞ Ï†úÏãú/Ï£ºÏùòÎ¨∏ Î≥ëÍ∏∞ Í≤ÄÌÜ†"
                if "MED" in " ".join(actions):
                    human = "ÏùòÎ£å ÌëúÌòÑ ÏôÑÌôî ÎòêÎäî Í∞ùÍ¥Ä Í∑ºÍ±∞ Ï†úÏãú"
                elif "HFS" in " ".join(actions):
                    human = "Í±¥Í∏∞Ïãù/ÏùòÏïΩÌíà Ïò§Ïù∏ Î∞©ÏßÄ Î¨∏Íµ¨Î°ú ÏàòÏ†ï"
                elif "GEN_NEED_EVIDENCE" in " ".join(actions):
                    human = "ÎπÑÍµê¬∑Ïö∞Ïõî ÌëúÌòÑÏùÄ Ï∂úÏ≤ò/Í∑ºÍ±∞ Î≥ëÍ∏∞"
                sugg = [human]
            else:
                sugg = ["Î¨∏Íµ¨ ÏôÑÌôî ÎòêÎäî ÏÇ≠Ï†ú Í≤ÄÌÜ†"]

            items.append({
                "id": f"r_{gid}",
                "type": rule_type,
                "original": seg,
                "suggestions": sugg[:3],
                "reason": f"[{topic}] {rationale or 'Í∑úÏπô Îß§Ïπ≠'}",
                "rule_id": rid,
                "severity": "high" if sev == "block" else "medium",
                "startIndex": s,
                "endIndex": e,
                "before": before,
                "after": after
            })
            gid += 1

    used = set()
    dedup = []
    for it in sorted(items, key=lambda x: (x["startIndex"], -(x["endIndex"]-x["startIndex"]))):
        k = (it["startIndex"], it["endIndex"], it["type"])
        if k in used:
            continue
        used.add(k)
        dedup.append(it)
    return dedup

# ============== (NEW) ÎèÑÎèåÏù¥Ìëú/Ïú†ÏÇ¨Î¨∏Ïû• ÌÉêÏßÄ Ïú†Ìã∏ ==============
_punc_rx = re.compile(r"[^\w\u3131-\u318E\uAC00-\uD7A3]+", re.UNICODE)
_ws_rx = re.compile(r"\s+")

def _norm_for_dup(s: str) -> str:
    s = unicodedata.normalize("NFKC", s or "")
    s = s.lower()
    s = _punc_rx.sub(" ", s)
    s = _ws_rx.sub(" ", s).strip()
    return s

def _char_ngrams(s: str, n=3):
    s = _norm_for_dup(s)
    return {s[i:i+n] for i in range(max(0, len(s)-n+1))} if s else set()

def _jaccard(a: set, b: set) -> float:
    if not a and not b: return 1.0
    if not a or not b:  return 0.0
    inter = len(a & b)
    union = len(a | b)
    return inter / union if union else 0.0

def _sentence_spans(text):
    """Î¨∏Ïû• Îã®ÏúÑÎ°ú (start, end, raw_sentence) Î∞òÌôò"""
    sentences = basic_kr_sentence_split(text)
    spans = []
    cursor = 0
    for s in sentences:
        idx = text.find(s, cursor)
        if idx == -1:
            idx = text.find(s)
        if idx != -1:
            spans.append((idx, idx+len(s), s))
            cursor = idx + len(s)
    return spans

def _dedup_intra(text, min_len=6, sim_threshold=0.85):
    spans = _sentence_spans(text)
    # 1) exact ÎèÑÎèåÏù¥Ìëú(Ï†ïÍ∑úÌôî ÌõÑ ÎèôÏùº)
    buckets = defaultdict(list)  # norm -> [(i, start, end, raw)]
    for i, (s, e, raw) in enumerate(spans):
        n = _norm_for_dup(raw)
        if len(n) >= min_len:
            buckets[n].append((i, s, e, raw))

    exact_groups = []
    for n, occ in buckets.items():
        if len(occ) >= 2:
            exact_groups.append({
                "norm": n,
                "occurrences": [
                    {"index": i, "start": s, "end": e, "original": raw} for (i,s,e,raw) in occ
                ]
            })

    # 2) similar(ÏûêÏπ¥Îìú: Î¨∏Ïûê 3-gram)
    sims = []
    ngrams = []
    for i, (s, e, raw) in enumerate(spans):
        n = _norm_for_dup(raw)
        if len(n) >= min_len:
            ngrams.append((i, s, e, raw, _char_ngrams(raw, 3)))
    for a in range(len(ngrams)):
        i, s1, e1, r1, g1 = ngrams[a]
        for b in range(a+1, len(ngrams)):
            j, s2, e2, r2, g2 = ngrams[b]
            score = _jaccard(g1, g2)
            if score >= sim_threshold and r1 != r2:
                sims.append({
                    "i": i, "j": j, "score": round(score, 3),
                    "a": {"start": s1, "end": e1, "original": r1},
                    "b": {"start": s2, "end": e2, "original": r2}
                })

    return exact_groups, sims

def _dedup_inter(files, min_len=6, sim_threshold=0.85):
    """
    files: [{"name": str, "text": str}, ...]
    ÍµêÏ∞® ÌååÏùº Í∞Ñ ÎèôÏùº/Ïú†ÏÇ¨ Î¨∏Ïû• ÌÉêÏßÄ
    """
    # ÏàòÏßë
    recs = []  # (file_idx, name, sent_idx, start, end, raw, norm, ngrams)
    for fi, f in enumerate(files):
        name = f.get("name") or f"file_{fi+1}"
        text = f.get("text") or ""
        spans = _sentence_spans(text)
        for si, (s, e, raw) in enumerate(spans):
            n = _norm_for_dup(raw)
            if len(n) >= min_len:
                recs.append((fi, name, si, s, e, raw, n, _char_ngrams(raw, 3)))

    # exact
    exact_map = defaultdict(list)  # norm -> [occ...]
    for fi, name, si, s, e, raw, n, grams in recs:
        exact_map[n].append({"file": name, "fileIndex": fi, "sentIndex": si,
                             "start": s, "end": e, "original": raw})

    exact = []
    for n, occ in exact_map.items():
        # ÏÑúÎ°ú Îã§Î•∏ ÌååÏùºÏóêÏÑú 2Í∞ú Ïù¥ÏÉÅÏùº ÎïåÎßå ÏùòÎØ∏
        uniq_files = {o["fileIndex"] for o in occ}
        if len(uniq_files) >= 2:
            exact.append({"norm": n, "occurrences": occ})

    # similar
    sims = []
    for a in range(len(recs)):
        fi1, n1, si1, s1, e1, r1, norm1, g1 = recs[a]
        for b in range(a+1, len(recs)):
            fi2, n2, si2, s2, e2, r2, norm2, g2 = recs[b]
            if fi1 == fi2:  # Í∞ôÏùÄ ÌååÏùºÏùÄ intraÏóêÏÑú Îã§Î£∏
                continue
            score = _jaccard(g1, g2)
            if score >= sim_threshold and r1 != r2:
                sims.append({
                    "score": round(score, 3),
                    "a": {"fileIndex": fi1, "file": n1, "sentIndex": si1, "start": s1, "end": e1, "original": r1},
                    "b": {"fileIndex": fi2, "file": n2, "sentIndex": si2, "start": s2, "end": e2, "original": r2},
                })
    return exact, sims

# ================== Flask ==================
app = Flask(__name__)
CORS(app,
     resources={r"/*": {"origins": "*"}},
     allow_headers=["Content-Type", "Authorization"],
     expose_headers=["Content-Type"],
     supports_credentials=False)

@app.route("/auth/login", methods=["POST"])
def auth_login():
    payload = request.get_json(force=True, silent=True) or {}
    username = (payload.get("username") or "").strip()
    password = payload.get("password") or ""
    u = _get_user(username)
    if not u or not bcrypt.verify(password, u["password_hash"]):
        return jsonify({"error":"Bad credentials"}), 401
    if not _is_paid_and_active(u):
        return jsonify({"error":"Payment required", "code":"PAYMENT"}), 402

    # (Îã®Ïùº Î°úÍ∑∏Ïù∏Ïö© Î≤ÑÏ†Ñ/ÌÜ†ÌÅ∞ID ÏóÖÎç∞Ïù¥Ìä∏)
    conn = sqlite3.connect(DB_PATH)
    cur = conn.cursor()
    cur.execute("SELECT token_version FROM users WHERE username=?", (username,))
    row = cur.fetchone()
    cur_ver = int((row[0] if row else 0) or 0)

    new_ver = cur_ver + 1
    new_jti = str(uuid.uuid4())
    cur.execute("UPDATE users SET token_version=?, last_jti=? WHERE username=?", (new_ver, new_jti, username))
    conn.commit()
    conn.close()

    token = jwt.encode({
        "sub": u["username"],
        "role": u["role"],
        "ver": new_ver,     # ÌÜ†ÌÅ∞ Î≤ÑÏ†Ñ
        "jti": new_jti,     # ÌÜ†ÌÅ∞ Í≥†Ïú†ID
        "exp": datetime.utcnow() + timedelta(hours=12)
    }, JWT_SECRET, algorithm=JWT_ALG)

    return jsonify({"access_token": token, "token_type": "bearer"})

@app.route("/auth/ping", methods=["GET", "OPTIONS"])
@require_user
def auth_ping():
    if request.method == "OPTIONS":
        return "", 200
    return jsonify({"ok": True})

@app.route("/auth/me", methods=["GET", "OPTIONS"])
@require_user
def auth_me():
    if request.method == "OPTIONS":
        return "", 200
    auth = request.headers.get("Authorization","")
    token = auth.split(" ",1)[1]
    data = jwt.decode(token, JWT_SECRET, algorithms=[JWT_ALG])
    u = _get_user(data.get("sub",""))
    return jsonify({
        "username": u["username"],
        "role": u["role"],
        "is_active": u["is_active"],
        "paid_until": u["paid_until"],
        "remaining_days": _remaining_days(u["paid_until"]),
    })

@app.route("/admin/create_user", methods=["POST"])
@require_admin
def admin_create_user():
    body = request.get_json(force=True, silent=True) or {}
    username = (body.get("username") or "").strip()
    password = body.get("password") or ""
    days = int(body.get("days") or 0)

    if not username or not password:
        return jsonify({"error":"username/password required"}), 400

    conn = sqlite3.connect(DB_PATH)
    cur = conn.cursor()
    try:
        paid_until = datetime.utcnow() + timedelta(days=days) if days>0 else datetime.utcnow()
        cur.execute(
            "INSERT INTO users (username, password_hash, is_active, paid_until, role) VALUES (?,?,?,?,?)",
            (username, bcrypt.hash(password), 1 if days>0 else 0, paid_until.isoformat(), "user")
        )
        conn.commit()
        return jsonify({"ok":True})
    except sqlite3.IntegrityError:
        return jsonify({"error":"username exists"}), 409
    finally:
        conn.close()

@app.route("/admin/approve", methods=["POST"])
@require_admin
def admin_approve():
    body = request.get_json(force=True, silent=True) or {}
    username = (body.get("username") or "").strip()
    extend_days = int(body.get("extend_days") or 0)
    note = (body.get("note") or "").strip()

    u = _get_user(username)
    if not u:
        return jsonify({"error":"user not found"}), 404

    base = datetime.utcnow()
    try:
        if u.get("paid_until"):
            base = max(base, datetime.fromisoformat(u["paid_until"]))
    except Exception:
        pass

    new_until = base + timedelta(days=max(1, extend_days))

    conn = sqlite3.connect(DB_PATH)
    cur = conn.cursor()
    cur.execute(
        "UPDATE users SET is_active=?, paid_until=?, notes=? WHERE username=?",
        (1, new_until.isoformat(), note, username)
    )
    conn.commit()
    conn.close()
    return jsonify({"ok":True, "paid_until": new_until.isoformat()})

@app.route("/admin/issue_user", methods=["POST"])
@require_admin
def admin_issue_user():
    """
    Í¥ÄÎ¶¨Ïûê Ï†ÑÏö©: Ïã†Í∑ú Î∞úÍ∏â ÎòêÎäî Í∏∞Ï°¥ Ïó∞Ïû•/Ï†ïÎ≥¥ ÏóÖÎç∞Ïù¥Ìä∏Î•º Ìïú Î≤àÏóê Ï≤òÎ¶¨
    body: { username, password?, days=32, site_url?, role? }
    Ïã†Í∑ú: password ÌïÑÏàò, Í∏∞Ï°¥: ÏÉùÎûµ Í∞ÄÎä•
    """
    body = request.get_json(force=True, silent=True) or {}
    username = (body.get("username") or "").strip()
    password = (body.get("password") or "").strip()
    days     = int(body.get("days") or 32)   # Í∏∞Î≥∏ 32Ïùº
    site_url = (body.get("site_url") or "").strip()
    role     = (body.get("role") or "user").strip()
    note     = (body.get("note") or "").strip()

    if not username:
        return jsonify({"error":"username required"}), 400

    conn = sqlite3.connect(DB_PATH); cur = conn.cursor()
    cur.execute("SELECT username, paid_until FROM users WHERE username=?", (username,))
    row = cur.fetchone()

    base = datetime.utcnow()
    if row and row[1]:
        try:
            prev = datetime.fromisoformat(row[1])
            if prev > base:
                base = prev
        except Exception:
            pass
    new_until = base + timedelta(days=max(1, days))

    if row is None:
        if not password:
            return jsonify({"error":"password required for new user"}), 400
        cur.execute(
            "INSERT INTO users (username, password_hash, is_active, paid_until, role, notes, site_url, created_at) VALUES (?,?,?,?,?,?,?,?)",
            (username, bcrypt.hash(password), 1, new_until.isoformat(), role, note, (site_url or None), datetime.utcnow().isoformat())
        )
    else:
        # Í∏∞Ï°¥ ÏÇ¨Ïö©Ïûê: Ïó∞Ïû• + ÏÑ†ÌÉù ÌïÑÎìú Í∞±Ïã†
        if password:
            cur.execute("UPDATE users SET password_hash=? WHERE username=?", (bcrypt.hash(password), username))
        cur.execute(
            "UPDATE users SET is_active=?, paid_until=?, role=?, notes=?, site_url=? WHERE username=?",
            (1, new_until.isoformat(), role, note, (site_url or None), username)
        )

    conn.commit(); conn.close()
    return jsonify({"ok": True, "username": username, "paid_until": new_until.isoformat(), "remaining_days": _remaining_days(new_until.isoformat())})

@app.route("/admin/set_active", methods=["POST"])
@require_admin
def admin_set_active():
    body = request.get_json(force=True, silent=True) or {}
    username = (body.get("username") or "").strip()
    is_active = 1 if body.get("is_active") else 0
    if not username:
        return jsonify({"error":"username required"}), 400
    conn = sqlite3.connect(DB_PATH); cur = conn.cursor()
    cur.execute("UPDATE users SET is_active=? WHERE username=?", (is_active, username))
    conn.commit(); conn.close()
    return jsonify({"ok": True})

@app.route("/admin/reset_password", methods=["POST"])
@require_admin
def admin_reset_password():
    body = request.get_json(force=True, silent=True) or {}
    username = (body.get("username") or "").strip()
    new_pw = (body.get("new_password") or "").strip()
    if not username or not new_pw:
        return jsonify({"error":"username/new_password required"}), 400
    conn = sqlite3.connect(DB_PATH); cur = conn.cursor()
    cur.execute("UPDATE users SET password_hash=? WHERE username=?", (bcrypt.hash(new_pw), username))
    conn.commit(); conn.close()
    return jsonify({"ok": True})

# --- Í¥ÄÎ¶¨Ïûê: ÏÇ¨Ïö©Ïûê ÏÇ≠Ï†ú(ÏûêÍ∏∞ ÏûêÏã†/ÏµúÏÉÅÏúÑ admin Î≥¥Ìò∏, Ï°¥Ïû¨Ïó¨Î∂Ä Ï≤¥ÌÅ¨) ---
@app.route("/admin/delete_user", methods=["POST"])
@require_admin
def admin_delete_user():
    body = request.get_json(silent=True) or {}
    username = (body.get("username") or "").strip()

    if not username:
        return jsonify({"error": "username required"}), 400
    # ÏµúÏÉÅÏúÑ Í¥ÄÎ¶¨Ïûê Î≥¥Ìò∏ (ÌïÑÏöî ÏóÜÎã§Î©¥ Ï£ºÏÑù Ï≤òÎ¶¨)
    if username == "admin":
        return jsonify({"error": "cannot delete admin"}), 400

    # Î≥∏Ïù∏ Í≥ÑÏ†ï ÏÇ≠Ï†ú Î∞©ÏßÄ
    try:
        auth = request.headers.get("Authorization", "")
        token = auth.split(" ", 1)[1]
        sub = jwt.decode(token, JWT_SECRET, algorithms=[JWT_ALG]).get("sub")
        if sub == username:
            return jsonify({"error": "Î≥∏Ïù∏ Í≥ÑÏ†ïÏùÄ ÏÇ≠Ï†úÌï† Ïàò ÏóÜÏäµÎãàÎã§"}), 400
    except Exception:
        # ÌÜ†ÌÅ∞ ÌååÏã± Ïã§Ìå® Ïãú ÏûêÍ∏∞ÏÇ≠Ï†ú Î∞©ÏßÄÎäî Í±¥ÎÑàÎõ∞Îêò, admin Î≥¥Ìò∏Îäî ÏúÑÏóêÏÑú Ïù¥ÎØ∏ Ï†ÅÏö©Îê®
        pass

    # Ïã§Ï†ú ÏÇ≠Ï†ú
    conn = sqlite3.connect(DB_PATH)
    cur = conn.cursor()
    cur.execute("DELETE FROM users WHERE username = ?", (username,))
    conn.commit()
    deleted = cur.rowcount
    conn.close()

    if deleted == 0:
        return jsonify({"error": "Ï°¥Ïû¨ÌïòÏßÄ ÏïäÎäî ÏÇ¨Ïö©Ïûê"}), 404

    return jsonify({"ok": True, "deleted": username})

@app.route("/admin/list_users", methods=["GET"])
@require_admin
def admin_list_users():
    q = (request.args.get("q") or "").strip()
    conn = sqlite3.connect(DB_PATH); cur = conn.cursor()
    if q:
        cur.execute("SELECT username,is_active,paid_until,role,notes,site_url,created_at FROM users WHERE username LIKE ? ORDER BY username", (f"%{q}%",))
    else:
        cur.execute("SELECT username,is_active,paid_until,role,notes,site_url,created_at FROM users ORDER BY username")
    rows = cur.fetchall()
    conn.close()
    out = []
    for u,a,pu,r,nt,site,created in rows:
        out.append({
            "username": u,
            "is_active": bool(a),
            "paid_until": pu,
            "remaining_days": _remaining_days(pu),
            "role": r,
            "note": nt,
            "site_url": site,
            "created_at": created
        })
    return jsonify({"users": out})

# Î°úÏª¨ ÎßûÏ∂§Î≤ï Ï¥àÍ∏∞Ìôî
_init_symspell()

@app.route("/verify", methods=["POST", "OPTIONS"])
@require_user
def verify():
    if request.method == "OPTIONS":
        return "", 200  # preflight OK

    try:
        data = request.get_json(force=True, silent=True) or {}
        text = (data.get("text") or "").strip()
        if not text:
            return jsonify({"error": "No text provided", "results": []}), 400

        all_items = []

        # Ïó¨Í∏∞Ïóê GPT ÎßûÏ∂§Î≤ï/Î¨∏Îß• Í≤ÄÏÇ¨ Ìò∏Ï∂ú Î°úÏßÅ ÏÇΩÏûÖ
        # (chunk_text_with_offsets + gpt_call ÌôúÏö©)

        # === GPT Í∏∞Î∞ò ÎßûÏ∂§Î≤ï/Î¨∏Îß•Ïò§Î•ò Í≤ÄÏÇ¨ ===
        chunks = chunk_text_with_offsets(text)  # (base_offset, chunk_text)
        for base, chunk in chunks:
            prompt = f"""
ÎÑàÎäî ÌïúÍµ≠Ïñ¥ Î¨∏Ïû• ÍµêÏ†ï Ï†ÑÎ¨∏Í∞ÄÎã§.
ÏïÑÎûò Í∏ÄÏóêÏÑú **ÎßûÏ∂§Î≤ï/ÎùÑÏñ¥Ïì∞Í∏∞/Ïñ¥Î≤ï Ïò§Î•ò(=type: "ÎßûÏ∂§Î≤ï")**,
**Î∂ÄÏûêÏó∞Ïä§ÎüΩÍ±∞ÎÇò ÎÅäÍ∏¥ Î¨∏Ïû•(=type: "Î¨∏Îß•Ïò§Î•ò")**Îßå Ï∞æÏïÑÎùº.
Ïã¨Ïùò/Í¥ëÍ≥†Î≤ï/ÏùòÎ£å Í∑úÏ†ï(Ìö®Í≥º¬∑Ïû¨Î∞úÎ•†¬∑Î≥¥Ïû• Îì±)ÏùÄ **ÏôÑÏ†ÑÌûà Î¨¥Ïãú**ÌïúÎã§.

Î∞òÎìúÏãú JSON Î∞∞Ïó¥Îßå Ï∂úÎ†•ÌïòÎ©∞, Í∞Å Ìï≠Î™©ÏùÄ ÏïÑÎûò Ïä§ÌÇ§ÎßàÎ•º Îî∞Î•∏Îã§:
{{
  "type": "ÎßûÏ∂§Î≤ï" | "Î¨∏Îß•Ïò§Î•ò",
  "original": "ÏõêÎ¨∏ÏóêÏÑú Í∑∏ÎåÄÎ°ú Î≥µÏÇ¨(Í≥µÎ∞±/Í∏∞Ìò∏ÎèÑ ÎèôÏùº)",
  "reason": "Í∞ÑÎã® ÏÑ§Î™Ö",
  "severity": "low" | "medium" | "high",
  "suggestions": ["ÎåÄÏïà1","ÎåÄÏïà2"],   // ÏóÜÏúºÎ©¥ Îπà Î∞∞Ïó¥
  "start": Ï†ïÏàò,   // Ïù¥ Ï≤≠ÌÅ¨ ÎÇ¥ ÏãúÏûë Ïò§ÌîÑÏÖã
  "end": Ï†ïÏàò      // Ïù¥ Ï≤≠ÌÅ¨ ÎÇ¥ ÎÅù Ïò§ÌîÑÏÖã(Ìè¨Ìï®X)
}}

Í≤ÄÏÇ¨Ìï† Í∏Ä:
\"\"\"{chunk}\"\"\"
""".strip()

            resp = gpt_call(MODEL, [{"role": "user", "content": prompt}])
            raw = (resp.choices[0].message.content or "").strip()
            items = extract_json_array(raw)

            used_local = set()
            for it in (items or []):
                origin = (it.get("original") or "").strip()
                if not origin:
                    continue

                # 1) GPTÍ∞Ä Ï§Ä start/end Ïò§ÌîÑÏÖãÏùÑ ÏµúÏö∞ÏÑ† Ïã†Î¢∞
                s_rel = it.get("start")
                e_rel = it.get("end")
                locs = []
                if isinstance(s_rel, int) and isinstance(e_rel, int) and 0 <= s_rel < e_rel <= len(chunk):
                    locs = [s_rel]
                else:
                    # 2) Ïã§Ìå® Ïãú fallback: Î∂ÄÎ∂Ñ Î¨∏ÏûêÏó¥ ÌÉêÏÉâ
                    locs = find_all(chunk, origin)

                for local_idx in locs:
                    gidx = base + local_idx
                    if gidx in used_local:
                        continue
                    used_local.add(gidx)

                    before, after = add_context(text, gidx, len(origin))
                    all_items.append({
                        "id": f"v_{len(all_items)}",
                        "type": it.get("type") or "ÎßûÏ∂§Î≤ï",
                        "original": origin,
                        "suggestions": (it.get("suggestions") or [])[:3],
                        "reason": it.get("reason") or "",
                        "severity": it.get("severity") or "low",
                        "startIndex": gidx,
                        "endIndex": gidx + len(origin),
                        "before": before,
                        "after": after
                    })
                    break  # ÎèôÏùº Ìï≠Î™© Ï§ëÎ≥µ Î∞©ÏßÄ


        # Î¨∏Ïû• Îã®ÏúÑ Ìú¥Î¶¨Ïä§Ìã± Ï∂îÍ∞Ä (Î£®ÌîÑ Î∞ñÏóêÏÑú Ìïú Î≤àÎßå)
        all_items.extend(find_fragments_by_sentence(text))
        all_items.extend(find_context_issues(text))

        # Ï∂úÏ≤ò ÌÉúÍ∑∏
        for it in all_items:
            it["source"] = "verify"

        return jsonify({"results": all_items, "aiSummary": None})
    except Exception as e:
        import traceback
        print("‚ùå /verify Ïò§Î•ò:", e)
        traceback.print_exc()
        return jsonify({"error": str(e), "results": []}), 500

@app.post("/spell/local")
def spell_local():
    """
    Î°úÏª¨ Ï≤†Ïûê(ÎßûÏ∂§Î≤ï)Îßå Í≤ÄÏÇ¨. Î¨∏Îß•/Ïñ¥Î≤ï/Ïã¨Ïùò ÌåêÎã® ÏóÜÏùå.
    body: { "text": str, "min_len": 3, "max_sug": 3 }
    """
    try:
        data = request.get_json(force=True, silent=True) or {}
        text = (data.get("text") or "")
        if not text.strip() or _sym is None:
            return jsonify({"results": []})

        min_len = int(data.get("min_len") or 3)   # ÏßßÏùÄ ÌÜ†ÌÅ∞ Ï†úÏô∏(Ïò§ÌÉê‚Üì)
        max_sug = int(data.get("max_sug") or 3)   # Ï†úÏïà ÏÉÅÏúÑ NÍ∞ú

        results = []
        for s, e, tok in _token_spans_ko(text):
            if len(tok) < min_len:
                continue
            # ÌôîÏù¥Ìä∏Î¶¨Ïä§Ìä∏/ÏÇ¨Ï†ÑÏóê ÏûàÏúºÎ©¥ ÌÜµÍ≥º
            if tok in _whitelist or _sym.words.get(tok, 0) > 0:
                continue

            suggs = _sym.lookup(tok, Verbosity.TOP, max_edit_distance=2, include_unknown=False)
            if not suggs:
                continue

            cand = [su.term for su in suggs[:max_sug]]
            results.append({
                "type": "ÎßûÏ∂§Î≤ï",
                "original": tok,
                "reason": "ÏÇ¨Ï†ÑÏóê ÏóÜÎäî Îã®Ïñ¥Î°ú Ï∂îÏ†ï",
                "severity": "low",
                "suggestions": cand,
                "startIndex": s,
                "endIndex": e,
                "before": text[max(0, s-24):s],
                "after":  text[e:min(len(text), e+24)],
                "source": "local-spell"
            })

        results.sort(key=lambda x: (x["startIndex"], x["endIndex"]))
        return jsonify({"results": results})
    except Exception as e:
        import traceback; traceback.print_exc()
        return jsonify({"results": [], "error": str(e)}), 500


# ===== Î¨∏Ïû• Îã®ÏúÑ Î¨∏Îß•Ïò§Î•ò Ïú†Ìã∏ =====

SENT_SPLIT_RE = re.compile(r'(?<=[\.!?])\s+')
END_TOKEN_RE = re.compile(r'(Îã§|Ïöî|ÎãàÎã§|Ìï©ÎãàÎã§|ÌñàÎã§|ÏòÄÎã§|Îê©ÎãàÎã§|ÎèºÏöî|\.|!|\?)\s*$')

LEADING_CONNECTIVES = (
    "ÌïòÏßÄÎßå", "Í∑∏Îü¨ÎÇò", "Î∞òÎ©¥Ïóê", "Í∑∏Îü∞Îç∞", "Îã§Îßå",
    "ÎòêÌïú", "Í∑∏Î¶¨Í≥†", "Í≤åÎã§Í∞Ä", "ÌäπÌûà", "Îòê", "ÌïúÌé∏"
)

def split_sentences_with_pos(text: str):
    spans = []
    parts = SENT_SPLIT_RE.split(text)
    cur = 0
    for p in parts:
        if not p.strip():
            cur += len(p)
            continue
        start = text.find(p, cur)
        if start == -1:
            start = text.find(p)
        end = start + len(p)
        spans.append((p, start, end))
        cur = end
    return spans

def find_fragments_by_sentence(text: str):
    items = []
    spans = split_sentences_with_pos(text)
    for s, start, end in spans:
        sent = s.strip()
        if len(sent) < 15:
            continue
        if sent.startswith(("‚Äú", "\"", "‚Äò", "'")) and sent.endswith(("‚Äù", "\"", "‚Äô", "'")):
            continue
        if not END_TOKEN_RE.search(sent):
            items.append({
                "id": f"frag_{start}",
                "type": "Î¨∏Îß•Ïò§Î•ò",
                "original": sent,
                "reason": "Î∂àÏôÑÏ†Ñ Î¨∏Ïû•(Ï¢ÖÍ≤∞Ïñ¥ÎØ∏/ÎßàÏπ®Ìëú ÎàÑÎùΩ) Í∞ÄÎä•",
                "severity": "medium",
                "suggestions": ["Î¨∏Ïû•ÏùÑ ÎßàÎ¨¥Î¶¨ÌïòÎäî Ï¢ÖÍ≤∞Ïñ¥ÎØ∏/ÎßàÏπ®Ìëú Ï∂îÍ∞Ä Í≤ÄÌÜ†"],
                "startIndex": start,
                "endIndex": end,
                "before": "", "after": ""
            })
    return items

def find_context_issues(text: str):
    items = []
    spans = split_sentences_with_pos(text)
    for i, (s, start, end) in enumerate(spans):
        sent = s.strip()
        if len(sent) < 10:
            continue
        for conn in LEADING_CONNECTIVES:
            if sent.startswith(conn):
                if i == 0:
                    items.append({
                        "id": f"ctx_first_{start}",
                        "type": "Î¨∏Îß•Ïò§Î•ò",
                        "original": sent[:min(40, len(sent))],
                        "reason": f"Ï≤´ Î¨∏Ïû•ÏóêÏÑú '{conn}' ÏÇ¨Ïö©",
                        "severity": "low",
                        "suggestions": ["Ï≤´ Î¨∏Ïû•ÏùÄ Ï†ÑÌôò ÏóÜÏù¥ Ï£ºÏ†úÎ•º Ï†úÏãúÌïòÎäî Í≤ÉÏù¥ ÏûêÏó∞Ïä§Îü¨ÏõÄ"],
                        "startIndex": start, "endIndex": end,
                        "before": "", "after": ""
                    })
                else:
                    prev = spans[i-1][0].strip()
                    if len(prev) < 10 or not END_TOKEN_RE.search(prev):
                        items.append({
                            "id": f"ctx_link_{start}",
                            "type": "Î¨∏Îß•Ïò§Î•ò",
                            "original": sent[:min(40, len(sent))],
                            "reason": f"Ïïû Î¨∏Ïû•Ïù¥ ÏïΩÌïú ÏÉÅÌÉúÏóêÏÑú '{conn}' ÏãúÏûë",
                            "severity": "low",
                            "suggestions": ["Ïïû Î¨∏Ïû•ÏùÑ Î≥¥Í∞ïÌïòÍ±∞ÎÇò Ïó∞Í≤∞Ïñ¥Î•º ÏÉùÎûµ/Î≥ÄÍ≤Ω"],
                            "startIndex": start, "endIndex": end,
                            "before": "", "after": ""
                        })
                break
    return items


# ============== (NEW) ÎèÑÎèåÏù¥Ìëú/Ïú†ÏÇ¨ ÎùºÏö∞Ìä∏ ==============
@app.route("/dedup_intra", methods=["POST"])
@require_user
def dedup_intra():
    """
    Î≥∏Î¨∏ Ìïú Í±¥ ÎÇ¥ ÎèÑÎèåÏù¥Ìëú(Ï†ïÌôïÌûà Í∞ôÏùÄ Î¨∏Ïû•) + Ïú†ÏÇ¨ Î¨∏Ïû•(3-gram ÏûêÏπ¥Îìú) ÌÉêÏßÄ
    body: { "text": str, "min_len": 6, "sim_threshold": 0.85 }
    """
    try:
        data = request.get_json(force=True, silent=True) or {}
        text = (data.get("text") or "")
        if not text.strip():
            return jsonify({"error": "No text provided"}), 400
        min_len = int(data.get("min_len", 6))
        sim_th  = float(data.get("sim_threshold", 0.85))
        exact, sims = _dedup_intra(text, min_len=min_len, sim_threshold=sim_th)
        return jsonify({"exact_groups": exact, "similar_pairs": sims})
    except Exception as e:
        import traceback
        print("‚ùå /dedup_intra Ïò§Î•ò:", e)
        traceback.print_exc()
        return jsonify({"error": str(e)}), 500

@app.route("/dedup_inter", methods=["POST"])
@require_user
def dedup_inter():
    """
    Ïó¨Îü¨ ÌååÏùº Í∞Ñ ÎèÑÎèåÏù¥Ìëú/Ïú†ÏÇ¨ Î¨∏Ïû• ÌÉêÏßÄ
    body: { "files": [{"name": str, "text": str}, ...], "min_len": 6, "sim_threshold": 0.85 }
    """
    try:
        data = request.get_json(force=True, silent=True) or {}
        files = data.get("files") or []
        if not files:
            return jsonify({"error": "No files provided"}), 400
        min_len = int(data.get("min_len", 6))
        sim_th  = float(data.get("sim_threshold", 0.85))
        exact, sims = _dedup_inter(files, min_len=min_len, sim_threshold=sim_th)
        return jsonify({"exact_groups": exact, "similar_pairs": sims})
    except Exception as e:
        import traceback
        print("‚ùå /dedup_inter Ïò§Î•ò:", e)
        traceback.print_exc()
        return jsonify({"error": str(e)}), 500

@app.route("/policy_verify", methods=["POST", "OPTIONS"])
@require_user
def policy_verify():
    if request.method == "OPTIONS":
        return "", 200

    try:
        data = request.get_json(force=True, silent=True) or {}
        text = (data.get("text") or "").strip()
        if not text:
            return jsonify({"error": "No text provided", "results": []}), 400

        # Ïã¨Ïùò Í∑úÏπô Í≤ÄÏÇ¨
        items = rule_scan(text)
        items = attach_reasons(items)

        # Ï∂úÏ≤ò ÌÉúÍ∑∏
        for it in items:
            it["source"] = "policy"

        return jsonify({"results": items})
    except Exception as e:
        import traceback
        print("‚ùå /policy_verify Ïò§Î•ò:", e)
        traceback.print_exc()
        return jsonify({"error": str(e), "results": []}), 500

@app.route("/health", methods=["GET"])
def health():
    return jsonify({
        "ok": True,
        "routes": [
            "/auth/login","/auth/ping","/auth/me",
            "/admin/issue_user","/admin/set_active","/admin/reset_password","/admin/list_users","/admin/delete_user",
            "/verify","/policy_verify","/dedup_intra","/dedup_inter","/spell/local","/health"
        ]
    })

if __name__ == "__main__":
    port = int(os.getenv("PORT", "5000"))
    app.run(host="0.0.0.0", port=port, debug=False)
